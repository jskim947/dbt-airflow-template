# DBT-Airflow Template ì‚¬ìš© íŠœí† ë¦¬ì–¼

ì´ íŠœí† ë¦¬ì–¼ì€ DBT-Airflow Templateì„ ì‚¬ìš©í•˜ì—¬ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ê°„ ë°ì´í„° ë³µì‚¬ ë° ë³€í™˜ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)
2. [ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •](#2-ë°ì´í„°ë² ì´ìŠ¤-ì—°ê²°-ì„¤ì •)
3. [ì²« ë²ˆì§¸ ë°ì´í„° ë³µì‚¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰](#3-ì²«-ë²ˆì§¸-ë°ì´í„°-ë³µì‚¬-íŒŒì´í”„ë¼ì¸-ì‹¤í–‰)
4. [DBT í†µí•© ë° ìŠ¤ëƒ…ìƒ· ìƒì„±](#4-dbt-í†µí•©-ë°-ìŠ¤ëƒ…ìƒ·-ìƒì„±)
5. [ì¦ë¶„ ë™ê¸°í™” ì„¤ì •](#5-ì¦ë¶„-ë™ê¸°í™”-ì„¤ì •)
6. [ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì„¤ì •](#6-ëª¨ë‹ˆí„°ë§-ë°-ì•Œë¦¼-ì„¤ì •)
7. [ë¬¸ì œ í•´ê²°](#7-ë¬¸ì œ-í•´ê²°)
8. [ê³ ê¸‰ ê¸°ëŠ¥](#8-ê³ ê¸‰-ê¸°ëŠ¥)

## 1. í™˜ê²½ ì„¤ì •

### 1.1 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **Python**: 3.12 ì´ìƒ
- **PostgreSQL**: 12 ì´ìƒ
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 8GB RAM (ê¶Œì¥ 16GB)
- **ë””ìŠ¤í¬**: ìµœì†Œ 10GB ì—¬ìœ  ê³µê°„

### 1.2 í”„ë¡œì íŠ¸ í´ë¡  ë° ì„¤ì •

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd dbt-airflow-template

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ë˜ëŠ”
.venv\Scripts\activate     # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 1.3 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# .env íŒŒì¼ í¸ì§‘
nano .env
```

`.env` íŒŒì¼ ë‚´ìš©:
```env
# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´
SOURCE_DB_HOST=localhost
SOURCE_DB_PORT=5432
SOURCE_DB_NAME=source_db
SOURCE_DB_USER=source_user
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
SOURCE_DB_PASSWORD=your_source_password_here

TARGET_DB_HOST=localhost
TARGET_DB_PORT=5432
TARGET_DB_NAME=target_db
TARGET_DB_USER=target_user
# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜ˆì‹œ
TARGET_DB_PASSWORD=your_target_password_here

# Airflow ì„¤ì •
AIRFLOW_HOME=./airflow
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@localhost:5432/airflow

# DBT ì„¤ì •
DBT_PROJECT_PATH=./airflow/dbt
DBT_PROFILES_DIR=./airflow/dbt
```

## 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •

### 2.1 Airflow ì—°ê²° ì„¤ì •

Airflow ì›¹ UIì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤:

1. **Airflow ì›¹ UI ì ‘ì†**: http://localhost:8080
2. **Admin â†’ Connections** ë©”ë‰´ë¡œ ì´ë™
3. **+ ë²„íŠ¼** í´ë¦­í•˜ì—¬ ìƒˆ ì—°ê²° ì¶”ê°€

#### ì†ŒìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
```
Connection Id: source_postgres
Connection Type: Postgres
Host: localhost
Schema: source_db
Login: source_user
Password: your_source_password_here
Port: 5432
```

#### íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
```
Connection Id: target_postgres
Connection Type: Postgres
Host: localhost
Schema: target_db
Login: target_user
Password: your_target_password_here
Port: 5432
```

### 2.2 ì—°ê²° í…ŒìŠ¤íŠ¸

```bash
# Airflow CLIë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸
airflow connections test source_postgres
airflow connections test target_postgres
```

## 3. ì²« ë²ˆì§¸ ë°ì´í„° ë³µì‚¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

### 3.1 í…Œì´ë¸” ì„¤ì • êµ¬ì„±

`airflow/dags/postgres_data_copy_dag.py` íŒŒì¼ì—ì„œ í…Œì´ë¸” ì„¤ì •ì„ ìˆ˜ì •í•©ë‹ˆë‹¤:

```python
# í…Œì´ë¸” ì„¤ì • ì˜ˆì‹œ
TABLE_CONFIGS = [
    {
        "source": "public.users",           # ì†ŒìŠ¤ í…Œì´ë¸”
        "target": "raw_data.users",         # íƒ€ê²Ÿ í…Œì´ë¸”
        "primary_key": ["id"],              # ê¸°ë³¸í‚¤
        "sync_mode": "full_sync",           # ë™ê¸°í™” ëª¨ë“œ
        "batch_size": 10000,                # ë°°ì¹˜ í¬ê¸°
        "enabled": True                     # í™œì„±í™” ì—¬ë¶€
    }
]
```

### 3.2 DAG í™œì„±í™” ë° ì‹¤í–‰

1. **Airflow ì›¹ UIì—ì„œ DAG í™œì„±í™”**
   - `postgres_multi_table_copy` DAG ì°¾ê¸°
   - **Toggle** ë²„íŠ¼ í´ë¦­í•˜ì—¬ í™œì„±í™”

2. **ìˆ˜ë™ ì‹¤í–‰**
   - **Trigger DAG** ë²„íŠ¼ í´ë¦­
   - ì‹¤í–‰ ë‚ ì§œ ì„ íƒ í›„ **Trigger** í´ë¦­

3. **ì‹¤í–‰ ìƒíƒœ ëª¨ë‹ˆí„°ë§**
   - **Graph View**ì—ì„œ íƒœìŠ¤í¬ë³„ ì‹¤í–‰ ìƒíƒœ í™•ì¸
   - **Log** ë²„íŠ¼ìœ¼ë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸

### 3.3 ì‹¤í–‰ ê²°ê³¼ í™•ì¸

```bash
# Airflow ë¡œê·¸ í™•ì¸
airflow tasks logs postgres_multi_table_copy copy_data_with_dynamic_sql 2024-01-01T00:00:00

# ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²°ê³¼ í™•ì¸
psql -h localhost -U target_user -d target_db -c "SELECT COUNT(*) FROM raw_data.users;"
```

## 4. DBT í†µí•© ë° ìŠ¤ëƒ…ìƒ· ìƒì„±

### 4.1 DBT í”„ë¡œì íŠ¸ ì´ˆê¸°í™”

```bash
# DBT í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd airflow/dbt

# DBT í”„ë¡œì íŠ¸ ì´ˆê¸°í™” (ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŒ)
# dbt init dbt_airflow_template

# DBT í”„ë¡œí•„ ì„¤ì •
dbt debug
```

### 4.2 DBT ìŠ¤ëƒ…ìƒ· ì„¤ì •

`airflow/dbt/snapshots/users_snapshot.sql` íŒŒì¼ ìƒì„±:

```sql
{% snapshot users_snapshot %}

{{
    config(
      target_schema='snapshots',
      strategy='timestamp',
      unique_key='id',
      updated_at='updated_at',
    )
}}

select * from {{ source('raw_data', 'users') }}

{% endsnapshot %}
```

### 4.3 DBT ìŠ¤ëƒ…ìƒ· ì‹¤í–‰

```bash
# DBT ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
dbt snapshot

# ë˜ëŠ” Airflow DAGë¥¼ í†µí•œ ì‹¤í–‰
airflow dags trigger dbt_processing
```

### 4.4 ìŠ¤ëƒ…ìƒ· ê²°ê³¼ í™•ì¸

```sql
-- ìŠ¤ëƒ…ìƒ· í…Œì´ë¸” í™•ì¸
SELECT 
    id,
    name,
    email,
    dbt_updated_at,
    dbt_valid_from,
    dbt_valid_to
FROM snapshots.users_snapshot
ORDER BY id, dbt_valid_from;
```

## 5. ì¦ë¶„ ë™ê¸°í™” ì„¤ì •

### 5.1 ì¦ë¶„ ë™ê¸°í™” í…Œì´ë¸” ì„¤ì •

```python
TABLE_CONFIGS = [
    {
        "source": "public.orders",
        "target": "raw_data.orders",
        "primary_key": ["order_id"],
        "sync_mode": "incremental_sync",           # ì¦ë¶„ ë™ê¸°í™”
        "incremental_field": "updated_at",         # ì¦ë¶„ í•„ë“œ
        "incremental_field_type": "timestamp",     # í•„ë“œ íƒ€ì…
        "batch_size": 5000,
        "enabled": True
    }
]
```

### 5.2 ì¦ë¶„ ë™ê¸°í™” ë¡œì§

```python
def build_incremental_filter(incremental_field: str, incremental_field_type: str) -> str:
    """
    ì¦ë¶„ ë™ê¸°í™”ë¥¼ ìœ„í•œ WHERE ì¡°ê±´ ìƒì„±
    """
    if incremental_field_type == "timestamp":
        return f"{incremental_field} > (SELECT MAX({incremental_field}) FROM target_table)"
    elif incremental_field_type == "yyyymmdd":
        return f"{incremental_field} >= '20240101'"
    else:
        return f"{incremental_field} > '2024-01-01'"
```

### 5.3 ì¦ë¶„ ë™ê¸°í™” í…ŒìŠ¤íŠ¸

```bash
# ì²« ë²ˆì§¸ ì‹¤í–‰ (ì „ì²´ ë°ì´í„°)
airflow dags trigger postgres_multi_table_copy

# ë°ì´í„° ìˆ˜ì • í›„ ë‘ ë²ˆì§¸ ì‹¤í–‰ (ì¦ë¶„ ë°ì´í„°ë§Œ)
# ì†ŒìŠ¤ í…Œì´ë¸”ì—ì„œ ì¼ë¶€ ë°ì´í„° ìˆ˜ì •
psql -h localhost -U source_user -d source_db -c "
UPDATE public.orders 
SET updated_at = CURRENT_TIMESTAMP 
WHERE order_id = 1;
"

# ì¦ë¶„ ë™ê¸°í™” ì‹¤í–‰
airflow dags trigger postgres_multi_table_copy
```

## 6. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì„¤ì •

### 6.1 ë¡œê·¸ ë ˆë²¨ ì„¤ì •

`airflow.cfg` íŒŒì¼ì—ì„œ ë¡œê·¸ ë ˆë²¨ ì„¤ì •:

```ini
[logging]
logging_level = INFO
fab_logging_level = WARN
logging_config_class = 
colored_console_log = True
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(message)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
```

### 6.2 Slack ì•Œë¦¼ ì„¤ì •

```python
# Slack ì›¹í›… URL ì„¤ì •
SLACK_WEBHOOK_URL = Variable.get("SLACK_WEBHOOK_URL", default_var="")

def send_slack_notification(message: str, channel: str = "#data-pipeline"):
    """
    Slackìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
    """
    if not SLACK_WEBHOOK_URL:
        logger.warning("Slack ì›¹í›… URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return
    
    payload = {
        "text": message,
        "channel": channel,
        "username": "Data Pipeline Bot"
    }
    
    response = requests.post(SLACK_WEBHOOK_URL, json=payload)
    response.raise_for_status()
```

### 6.3 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
import time
import psutil

def collect_performance_metrics():
    """
    ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    """
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    
    # ì‘ì—… ìˆ˜í–‰
    # ...
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    
    execution_time = end_time - start_time
    memory_usage = end_memory - start_memory
    
    logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    logger.info(f"  - ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    logger.info(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
    
    return {
        "execution_time": execution_time,
        "memory_usage": memory_usage
    }
```

## 7. ë¬¸ì œ í•´ê²°

### 7.1 ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨

```bash
# ì—°ê²° í…ŒìŠ¤íŠ¸
psql -h localhost -U username -d database -c "SELECT 1;"

# ë°©í™”ë²½ ì„¤ì • í™•ì¸
sudo ufw status
sudo ufw allow 5432

# PostgreSQL ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
sudo systemctl status postgresql
```

#### ê¶Œí•œ ë¬¸ì œ

```sql
-- ì‚¬ìš©ì ê¶Œí•œ í™•ì¸
SELECT usename, usecreatedb, usesuper, usebypassrls 
FROM pg_user 
WHERE usename = 'your_username';

-- í…Œì´ë¸” ê¶Œí•œ í™•ì¸
SELECT grantee, privilege_type 
FROM information_schema.role_table_grants 
WHERE table_name = 'your_table';
```

#### ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
table_config["batch_size"] = 1000  # ê¸°ë³¸ê°’ 10000ì—ì„œ ì¤„ì„

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import psutil
memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
logger.info(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
```

### 7.2 ë””ë²„ê¹… ë°©ë²•

#### ìƒì„¸ ë¡œê·¸ í™•ì¸

```bash
# Airflow íƒœìŠ¤í¬ ë¡œê·¸
airflow tasks logs <dag_id> <task_id> <execution_date>

# DBT ë¡œê·¸
dbt run --log-level debug

# Python ìŠ¤í¬ë¦½íŠ¸ ë””ë²„ê¹…
python -m pdb your_script.py
```

#### ë°ì´í„° ê²€ì¦

```python
def validate_data_integrity(source_table: str, target_table: str):
    """
    ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
    """
    # í–‰ ìˆ˜ ë¹„êµ
    source_count = get_table_row_count(source_table)
    target_count = get_table_row_count(target_table)
    
    logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” í–‰ ìˆ˜: {source_count}")
    logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” í–‰ ìˆ˜: {target_count}")
    
    if source_count != target_count:
        logger.error(f"í–‰ ìˆ˜ ë¶ˆì¼ì¹˜: ì†ŒìŠ¤ {source_count}, íƒ€ê²Ÿ {target_count}")
        return False
    
    # ìƒ˜í”Œ ë°ì´í„° ë¹„êµ
    sample_size = min(1000, source_count)
    source_sample = get_table_sample(source_table, sample_size)
    target_sample = get_table_sample(target_table, sample_size)
    
    # ë°ì´í„° ë¹„êµ ë¡œì§
    # ...
    
    return True
```

## 8. ê³ ê¸‰ ê¸°ëŠ¥

### 8.1 ì‚¬ìš©ì ì •ì˜ SQL

ë³µì¡í•œ ë°ì´í„° ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
table_config = {
    "source": "public.complex_table",
    "target": "raw_data.complex_table",
    "primary_key": ["id"],
    "sync_mode": "full_sync",
    "custom_select": """
        SELECT 
            id,
            name,
            CASE 
                WHEN priority ~ '^[0-9]+\\.[0-9]+$' 
                THEN CAST(CAST(priority AS NUMERIC) AS BIGINT)::TEXT
                ELSE priority 
            END AS priority,
            updated_at
        FROM public.complex_table
        WHERE status = 'active'
    """,
    "custom_count": "SELECT COUNT(*) FROM public.complex_table WHERE status = 'active'"
}
```

### 8.2 ë³‘ë ¬ ì²˜ë¦¬

ì—¬ëŸ¬ í…Œì´ë¸”ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from multiprocessing import Pool, cpu_count

def parallel_table_processing(table_configs: list[dict], max_workers: int = None):
    """
    ì—¬ëŸ¬ í…Œì´ë¸”ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    """
    if max_workers is None:
        max_workers = min(cpu_count(), len(table_configs))
    
    with Pool(processes=max_workers) as pool:
        results = pool.map(process_single_table, table_configs)
    
    return results

def process_single_table(table_config: dict):
    """
    ë‹¨ì¼ í…Œì´ë¸” ì²˜ë¦¬
    """
    try:
        # ë°ì´í„° ë³µì‚¬ ì‹¤í–‰
        result = copy_table_data(table_config)
        return {"table": table_config["source"], "status": "success", "result": result}
    except Exception as e:
        return {"table": table_config["source"], "status": "error", "error": str(e)}
```

### 8.3 ìë™ ì¬ì‹œë„

ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì •ì´ë‚˜ ì¼ì‹œì  ì˜¤ë¥˜ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ìë™ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜:

```python
import time
from functools import wraps

def retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):
    """
    ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)
                        logger.warning(f"{func.__name__} ì‹¤í–‰ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                        logger.info(f"{delay}ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(delay)
                    else:
                        logger.error(f"{func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
            
            raise last_exception
        
        return wrapper
    return decorator

@retry_with_backoff(max_retries=3, base_delay=1.0)
def copy_data_with_retry(table_config: dict):
    """
    ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë°ì´í„° ë³µì‚¬
    """
    return copy_data_with_dynamic_sql(table_config)
```

### 8.4 ë°ì´í„° í’ˆì§ˆ ê²€ì¦

ë°ì´í„° í’ˆì§ˆì„ ìë™ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ê¸°ëŠ¥:

```python
def validate_data_quality(table_name: str, schema: dict):
    """
    ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    """
    quality_checks = []
    
    for column in schema["columns"]:
        col_name = column["name"]
        col_type = column["type"]
        
        # NULL ê°’ ê²€ì¦
        null_check = f"SELECT COUNT(*) FROM {table_name} WHERE {col_name} IS NULL"
        null_count = execute_query(null_check)
        
        if null_count > 0:
            quality_checks.append({
                "column": col_name,
                "check": "null_values",
                "status": "warning",
                "message": f"{null_count}ê°œì˜ NULL ê°’ ë°œê²¬"
            })
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        if col_type in ["BIGINT", "INTEGER"]:
            decimal_check = f"""
                SELECT COUNT(*) FROM {table_name} 
                WHERE {col_name}::TEXT ~ '\\.'
            """
            decimal_count = execute_query(decimal_check)
            
            if decimal_count > 0:
                quality_checks.append({
                    "column": col_name,
                    "check": "decimal_in_integer",
                    "status": "error",
                    "message": f"{decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ ë°œê²¬"
                })
    
    return quality_checks
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ìœ ìš©í•œ ëª…ë ¹ì–´

```bash
# Airflow DAG ìƒíƒœ í™•ì¸
airflow dags list

# íŠ¹ì • DAGì˜ íƒœìŠ¤í¬ ìƒíƒœ í™•ì¸
airflow tasks list postgres_multi_table_copy

# DAG ì‹¤í–‰ ì´ë ¥ í™•ì¸
airflow dags state postgres_multi_table_copy 2024-01-01T00:00:00

# DBT ëª¨ë¸ ìƒíƒœ í™•ì¸
dbt ls

# DBT ì˜ì¡´ì„± ê·¸ë˜í”„ ìƒì„±
dbt deps
dbt list --select +model_name
```

### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

- **Airflow UI**: http://localhost:8080
- **PostgreSQL ê´€ë¦¬ ë„êµ¬**: pgAdmin, DBeaver
- **ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§**: htop, iotop, nethogs

### ë¡œê·¸ ë¶„ì„

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f airflow/logs/dag_id/task_id/execution_date/attempt_number.log

# ì—ëŸ¬ ë¡œê·¸ ê²€ìƒ‰
grep -r "ERROR" airflow/logs/

# ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„
grep -r "execution_time" airflow/logs/ | awk '{print $NF}' | sort -n
```

---

ì´ íŠœí† ë¦¬ì–¼ì„ í†µí•´ DBT-Airflow Templateì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ìµí˜”ìŠµë‹ˆë‹¤. 
ë” ìì„¸í•œ ì •ë³´ëŠ” [ì•„í‚¤í…ì²˜ ë¬¸ì„œ](ARCHITECTURE.md)ì™€ [í”„ë¡œì íŠ¸ README](../../README.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

ë¬¸ì œê°€ ìˆê±°ë‚˜ ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ í”„ë¡œì íŠ¸ ì´ìŠˆ í˜ì´ì§€ì— ë¬¸ì˜í•´ ì£¼ì„¸ìš”. 