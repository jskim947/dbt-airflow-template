# DBT-Airflow Template ì•„í‚¤í…ì²˜

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ê°„ ë°ì´í„° ë³µì‚¬ ë° ë³€í™˜ì„ ìœ„í•œ ì¢…í•©ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source DB     â”‚    â”‚     Airflow     â”‚    â”‚   Target DB     â”‚
â”‚  (PostgreSQL)   â”‚â”€â”€â”€â–¶â”‚      DAGs       â”‚â”€â”€â”€â–¶â”‚  (PostgreSQL)   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ EDI_690       â”‚    â”‚ â€¢ Data Copy     â”‚    â”‚ â€¢ Raw Data      â”‚
â”‚ â€¢ Stock Master  â”‚    â”‚ â€¢ DBT Process   â”‚    â”‚ â€¢ Transformed   â”‚
â”‚ â€¢ Other Tables  â”‚    â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Analytics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚      DBT        â”‚
                       â”‚                 â”‚
                       â”‚ â€¢ Snapshots     â”‚
                       â”‚ â€¢ Models        â”‚
                       â”‚ â€¢ Tests         â”‚
                       â”‚ â€¢ Documentation â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

### 1. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„
```
Source DB â†’ Airflow DAG â†’ CSV Export â†’ Target DB
```

### 2. ë°ì´í„° ë³€í™˜ ë‹¨ê³„
```
Target DB â†’ DBT Snapshots â†’ DBT Models â†’ Analytics Layer
```

### 3. ê²€ì¦ ë° ëª¨ë‹ˆí„°ë§ ë‹¨ê³„
```
Data Validation â†’ Quality Checks â†’ Monitoring â†’ Alerts
```

## ğŸ¯ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. ë°ì´í„° ë³µì‚¬ ì—”ì§„ (`DataCopyEngine`)

#### ì£¼ìš” ì±…ì„
- ì†ŒìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ
- CSV íŒŒì¼ì„ í†µí•œ ì¤‘ê°„ ì €ì¥
- íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë°ì´í„° ë¡œë“œ
- ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ê²€ì¦

#### í•µì‹¬ ë©”ì„œë“œ

```python
class DataCopyEngine:
    def copy_data_with_custom_sql(
        self, 
        source_table: str, 
        target_table: str, 
        primary_keys: list[str],
        sync_mode: str = "full_sync",
        custom_where: str = None,
        custom_select: str = None,
        custom_count: str = None,
        batch_size: int = 10000
    ) -> dict[str, Any]:
        """
        ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë³µì‚¬ ìˆ˜í–‰
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            sync_mode: ë™ê¸°í™” ëª¨ë“œ (full_sync/incremental_sync)
            custom_where: ì‚¬ìš©ì ì •ì˜ WHERE ì¡°ê±´
            custom_select: ì‚¬ìš©ì ì •ì˜ SELECT ì¿¼ë¦¬
            custom_count: ì‚¬ìš©ì ì •ì˜ COUNT ì¿¼ë¦¬
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
```

#### ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

1. **ìŠ¤í‚¤ë§ˆ ê²€ì¦**
   ```python
   def _validate_and_convert_data_types(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
       """
       ë°ì´í„°í”„ë ˆì„ì˜ ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ìˆ˜í–‰
       """
   ```

2. **ë°ì´í„° íƒ€ì… ë³€í™˜**
   ```python
   def _convert_column_by_type(self, column_series: pd.Series, col_type: str, col_name: str) -> pd.Series:
       """
       ì»¬ëŸ¼ íƒ€ì…ì— ë”°ë¼ ë°ì´í„° ë³€í™˜ ìˆ˜í–‰
       """
   ```

3. **ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ë¡œë“œ**
   ```python
   def create_temp_table_and_import_csv(
       self, 
       target_table: str, 
       source_schema: dict[str, Any], 
       csv_path: str, 
       batch_size: int = 1000
   ) -> tuple[str, int]:
       """
       ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
       """
   ```

4. **MERGE ì‘ì—… ìˆ˜í–‰**
   ```python
   def _execute_merge_operation(
       self, 
       source_table: str, 
       target_table: str, 
       primary_keys: list[str], 
       sync_mode: str
   ) -> dict[str, Any]:
       """
       MERGE ì‘ì—…ì„ í†µí•œ ë°ì´í„° ë™ê¸°í™”
       """
   ```

### 2. ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ëª¨ë“ˆ (`DatabaseOperations`)

#### ì£¼ìš” ì±…ì„
- ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬
- í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ë° ìƒì„±
- ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

#### í•µì‹¬ ë©”ì„œë“œ

```python
class DatabaseOperations:
    def get_table_schema(self, table_name: str) -> dict[str, Any]:
        """
        í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
        """
    
    def create_table_if_not_exists(self, target_table: str, source_schema: dict[str, Any]) -> None:
        """
        íƒ€ê²Ÿ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
        """
    
    def verify_data_integrity(
        self, 
        source_table: str, 
        target_table: str, 
        primary_keys: list[str],
        sample_size: int = 1000
    ) -> dict[str, Any]:
        """
        ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ í…Œì´ë¸” ê°„ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        """
```

### 3. ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (`Monitoring`)

#### ì£¼ìš” ì±…ì„
- íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ ì¶”ì 
- ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì—ëŸ¬ ë¡œê¹… ë° ì•Œë¦¼

#### í•µì‹¬ ë©”ì„œë“œ

```python
class Monitoring:
    def __init__(self, dag_id: str, task_id: str):
        self.start_time = time.time()
        self.checkpoints = []
        self.status = "running"
    
    def add_checkpoint(self, step: str, message: str) -> None:
        """
        ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
        """
    
    def stop_monitoring(self, final_status: str) -> dict[str, Any]:
        """
        ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ ë° ê²°ê³¼ ë°˜í™˜
        """
```

## ğŸš€ Airflow DAG êµ¬ì¡°

### 1. ë©”ì¸ ë°ì´í„° ë³µì‚¬ DAG (`postgres_data_copy_dag.py`)

#### DAG êµ¬ì¡°
```
start â†’ connection_test â†’ schema_validation â†’ data_copy â†’ 
merge_operation â†’ data_validation â†’ dbt_snapshot â†’ cleanup â†’ end
```

#### ì£¼ìš” íƒœìŠ¤í¬

1. **`test_source_connection`**
   - ì†ŒìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
   - ì—°ê²° ì •ë³´ ìœ íš¨ì„± ê²€ì¦

2. **`test_target_connection`**
   - íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
   - ê¶Œí•œ ë° ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸

3. **`get_source_data_count`**
   - ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ ì¡°íšŒ
   - ì¦ë¶„ ë™ê¸°í™” ì¡°ê±´ í™•ì¸

4. **`get_source_schema`**
   - ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
   - ì»¬ëŸ¼ íƒ€ì… ë° ì œì•½ì¡°ê±´ ë¶„ì„

5. **`create_target_table`**
   - íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„± ë˜ëŠ” ê²€ì¦
   - ìŠ¤í‚¤ë§ˆ ì¼ì¹˜ì„± í™•ì¸

6. **`copy_data_with_dynamic_sql`**
   - ë™ì  SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬
   - ë°°ì¹˜ ì²˜ë¦¬ ë° ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§

7. **`execute_merge_operation`**
   - MERGE ì‘ì—…ì„ í†µí•œ ë°ì´í„° ë™ê¸°í™”
   - ì¶©ëŒ í•´ê²° ë° ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥

8. **`validate_data_integrity`**
   - ë³µì‚¬ëœ ë°ì´í„°ì˜ ë¬´ê²°ì„± ê²€ì¦
   - ìƒ˜í”Œ ë°ì´í„° ë¹„êµ ë° í†µê³„ ìˆ˜ì§‘

9. **`run_dbt_snapshot`**
   - DBT ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
   - ë°ì´í„° ë³€ê²½ì‚¬í•­ ì¶”ì 

10. **`cleanup_temp_table`**
    - ì„ì‹œ í…Œì´ë¸” ì •ë¦¬
    - ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë° ì •ë¦¬

### 2. ë¦¬íŒ©í† ë§ëœ ë°ì´í„° ë³µì‚¬ DAG (`postgres_data_copy_dag_refactored.py`)

#### ê°œì„ ëœ êµ¬ì¡°
- ëª¨ë“ˆí™”ëœ íƒœìŠ¤í¬ í•¨ìˆ˜
- í–¥ìƒëœ ì—ëŸ¬ í•¸ë“¤ë§
- ìƒì„¸í•œ ëª¨ë‹ˆí„°ë§ ë° ì²´í¬í¬ì¸íŠ¸
- DBT íŒŒì´í”„ë¼ì¸ í†µí•©

#### ì£¼ìš” íƒœìŠ¤í¬

1. **`copy_table_data`**
   ```python
   def copy_table_data(table_config: dict, **context) -> str:
       """
       ê°œë³„ í…Œì´ë¸” ë°ì´í„° ë³µì‚¬
       """
       try:
           # ëª¨ë‹ˆí„°ë§ ì‹œì‘
           monitoring = Monitoring(context["dag"].dag_id, context["task"].task_id)
           
           # 1. ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
           source_schema = get_source_schema(table_config, **context)
           monitoring.add_checkpoint("ìŠ¤í‚¤ë§ˆ ì¡°íšŒ", f"ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì™„ë£Œ: {len(source_schema['columns'])}ê°œ ì»¬ëŸ¼")
           
           # 2. íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„±/ê²€ì¦
           create_target_table(table_config, source_schema, **context)
           monitoring.add_checkpoint("í…Œì´ë¸” ìƒì„±", "íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„±/ê²€ì¦ ì™„ë£Œ")
           
           # 3. ë°ì´í„° ë³µì‚¬
           copy_result = copy_data_with_dynamic_sql(table_config, **context)
           monitoring.add_checkpoint("ë°ì´í„° ë³µì‚¬", f"ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {copy_result.get('exported_rows', 0)}í–‰ ì²˜ë¦¬")
           
           # 4. MERGE ì‘ì—…
           merge_result = execute_merge_operation(table_config, **context)
           monitoring.add_checkpoint("MERGE ì‘ì—…", f"MERGE ì‘ì—… ì™„ë£Œ: {merge_result.get('message', '')}")
           
           # 5. DBT íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
           pipeline_result = run_dbt_pipeline(table_config, **context)
           monitoring.add_checkpoint("DBT ì‹¤í–‰", f"DBT íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {pipeline_result.get('message', '')}")
           
           # ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
           monitoring.stop_monitoring("success")
           return f"í…Œì´ë¸” {table_config['source']} ì²˜ë¦¬ ì™„ë£Œ"
           
       except Exception as e:
           error_msg = f"í…Œì´ë¸” {table_config['source']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
           logger.error(error_msg)
           monitoring.stop_monitoring("failed")
           raise Exception(error_msg)
   ```

2. **`run_dbt_pipeline`**
   ```python
   def run_dbt_pipeline(table_config: dict, **context) -> dict[str, Any]:
       """
       DBT íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
       """
       try:
           # DBT ì„¤ì •
           pipeline_config = {
               "run_snapshot": True,      # ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
               "run_models": False,       # ëª¨ë¸ ì‹¤í–‰ (í–¥í›„ í™•ì¥)
               "run_tests": False,        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í–¥í›„ í™•ì¥)
               "cleanup": True,           # ì •ë¦¬ ì‘ì—…
           }
           
           # DBT ëª…ë ¹ì–´ ì‹¤í–‰
           dbt_result = execute_dbt_commands(pipeline_config, **context)
           
           return {
               "status": "success",
               "message": "DBT íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ",
               "results": dbt_result
           }
           
       except Exception as e:
           return {
               "status": "error",
               "message": f"DBT íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}",
               "results": None
           }
   ```

### 3. DBT ì²˜ë¦¬ DAG (`dbt_processing_dag.py`)

#### DAG êµ¬ì¡°
```
start â†’ dbt_snapshot â†’ dbt_run â†’ dbt_test â†’ dbt_docs_generate â†’ end
```

#### ì£¼ìš” íƒœìŠ¤í¬

1. **`run_dbt_snapshot`**
   - DBT ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
   - ë°ì´í„° ë³€ê²½ì‚¬í•­ ì¶”ì 

2. **`run_dbt_models`**
   - DBT ëª¨ë¸ ì‹¤í–‰
   - ë°ì´í„° ë³€í™˜ ë° ì§‘ê³„

3. **`run_dbt_tests`**
   - DBT í…ŒìŠ¤íŠ¸ ì‹¤í–‰
   - ë°ì´í„° í’ˆì§ˆ ê²€ì¦

4. **`generate_dbt_docs`**
   - DBT ë¬¸ì„œ ìƒì„±
   - ë°ì´í„° ê³„ë³´ ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬

## ğŸ”§ DBT í†µí•©

### 1. ìŠ¤ëƒ…ìƒ· ê´€ë¦¬

#### ìŠ¤ëƒ…ìƒ· ì„¤ì •
```yaml
# dbt_project.yml
snapshots:
  +target_schema: snapshots
  +strategy: timestamp
  +updated_at: updated_at
  +unique_key: id
```

#### ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
```python
def execute_dbt_snapshot(table_name: str, **context) -> dict[str, Any]:
    """
    DBT ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
    """
    try:
        # DBT í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
        dbt_project_path = Variable.get("DBT_PROJECT_PATH", default_var="airflow/dbt")
        
        # ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ëª…ë ¹ì–´
        snapshot_cmd = f"cd {dbt_project_path} && dbt snapshot --select {table_name}"
        
        # ëª…ë ¹ì–´ ì‹¤í–‰
        result = subprocess.run(
            snapshot_cmd,
            shell=True,
            capture_output=True,
            text=True,
            cwd=dbt_project_path
        )
        
        if result.returncode == 0:
            return {
                "status": "success",
                "message": f"ìŠ¤ëƒ…ìƒ· {table_name} ìƒì„± ì™„ë£Œ",
                "output": result.stdout
            }
        else:
            return {
                "status": "error",
                "message": f"ìŠ¤ëƒ…ìƒ· {table_name} ìƒì„± ì‹¤íŒ¨",
                "error": result.stderr
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}",
            "error": str(e)
        }
```

### 2. ëª¨ë¸ ì‹¤í–‰

#### ëª¨ë¸ êµ¬ì¡°
```
models/
â”œâ”€â”€ staging/           # ìŠ¤í…Œì´ì§• ë ˆì´ì–´
â”‚   â”œâ”€â”€ stg_edi_690.sql
â”‚   â””â”€â”€ stg_stock_master.sql
â”œâ”€â”€ marts/            # ë§ˆíŠ¸ ë ˆì´ì–´
â”‚   â”œâ”€â”€ dim_securities.sql
â”‚   â””â”€â”€ fact_events.sql
â””â”€â”€ intermediate/     # ì¤‘ê°„ ì²˜ë¦¬ ë ˆì´ì–´
    â””â”€â”€ int_event_summary.sql
```

#### ëª¨ë¸ ì‹¤í–‰
```python
def execute_dbt_models(model_selection: str = "all", **context) -> dict[str, Any]:
    """
    DBT ëª¨ë¸ ì‹¤í–‰
    """
    try:
        # DBT í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
        dbt_project_path = Variable.get("DBT_PROJECT_PATH", default_var="airflow/dbt")
        
        # ëª¨ë¸ ì„ íƒ
        if model_selection == "all":
            run_cmd = f"cd {dbt_project_path} && dbt run"
        else:
            run_cmd = f"cd {dbt_project_path} && dbt run --select {model_selection}"
        
        # ëª…ë ¹ì–´ ì‹¤í–‰
        result = subprocess.run(
            run_cmd,
            shell=True,
            capture_output=True,
            text=True,
            cwd=dbt_project_path
        )
        
        if result.returncode == 0:
            return {
                "status": "success",
                "message": f"ëª¨ë¸ ì‹¤í–‰ ì™„ë£Œ: {model_selection}",
                "output": result.stdout
            }
        else:
            return {
                "status": "error",
                "message": f"ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {model_selection}",
                "error": result.stderr
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}",
            "error": str(e)
        }
```

## ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ìµœì í™”

### 1. ë°°ì¹˜ ì²˜ë¦¬

#### ë°°ì¹˜ í¬ê¸° ìµœì í™”
```python
def optimize_batch_size(table_size: int, memory_limit: int = 8) -> int:
    """
    í…Œì´ë¸” í¬ê¸°ì™€ ë©”ëª¨ë¦¬ ì œí•œì„ ê³ ë ¤í•œ ë°°ì¹˜ í¬ê¸° ìµœì í™”
    """
    # ë©”ëª¨ë¦¬ ì œí•œ (GB)
    memory_limit_gb = memory_limit
    
    # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
    base_batch_size = 10000
    
    # í…Œì´ë¸” í¬ê¸°ì— ë”°ë¥¸ ì¡°ì •
    if table_size > 1000000:  # 100ë§Œ í–‰ ì´ìƒ
        batch_size = base_batch_size // 2
    elif table_size > 100000:  # 10ë§Œ í–‰ ì´ìƒ
        batch_size = base_batch_size
    else:
        batch_size = base_batch_size * 2
    
    # ë©”ëª¨ë¦¬ ì œí•œì— ë”°ë¥¸ ì¡°ì •
    if memory_limit_gb < 4:
        batch_size = batch_size // 2
    elif memory_limit_gb > 16:
        batch_size = batch_size * 2
    
    return max(batch_size, 1000)  # ìµœì†Œ 1000í–‰ ë³´ì¥
```

### 2. ë³‘ë ¬ ì²˜ë¦¬

#### ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©
```python
from multiprocessing import Pool, cpu_count

def parallel_data_processing(table_configs: list[dict], max_workers: int = None) -> list[dict]:
    """
    ì—¬ëŸ¬ í…Œì´ë¸”ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    """
    if max_workers is None:
        max_workers = min(cpu_count(), len(table_configs))
    
    with Pool(processes=max_workers) as pool:
        results = pool.map(process_single_table, table_configs)
    
    return results

def process_single_table(table_config: dict) -> dict:
    """
    ë‹¨ì¼ í…Œì´ë¸” ì²˜ë¦¬ (ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)
    """
    try:
        # ë°ì´í„° ë³µì‚¬ ì‹¤í–‰
        copy_result = copy_table_data(table_config)
        return {
            "table": table_config["source"],
            "status": "success",
            "result": copy_result
        }
    except Exception as e:
        return {
            "table": table_config["source"],
            "status": "error",
            "error": str(e)
        }
```

### 3. ë©”ëª¨ë¦¬ ìµœì í™”

#### ë°ì´í„°í”„ë ˆì„ ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:
    """
    ë°ì´í„°í”„ë ˆì„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
    """
    # ìˆ«ì ì»¬ëŸ¼ ìµœì í™”
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    
    # ë¶€ë™ì†Œìˆ˜ì  ì»¬ëŸ¼ ìµœì í™”
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')
    
    # ë¬¸ìì—´ ì»¬ëŸ¼ ìµœì í™”
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype('category')
    
    return df
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. ì„±ëŠ¥ ë©”íŠ¸ë¦­

#### ì‹¤í–‰ ì‹œê°„ ì¶”ì 
```python
import time
from contextlib import contextmanager

@contextmanager
def performance_monitoring(operation_name: str):
    """
    ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    """
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    
    try:
        yield
    finally:
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        logger.info(f"{operation_name} ì™„ë£Œ:")
        logger.info(f"  - ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        logger.info(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
```

#### ë°ì´í„° ì²˜ë¦¬ í†µê³„
```python
def collect_processing_stats(
    source_count: int,
    target_count: int,
    execution_time: float,
    memory_usage: float
) -> dict[str, Any]:
    """
    ë°ì´í„° ì²˜ë¦¬ í†µê³„ ìˆ˜ì§‘
    """
    return {
        "source_records": source_count,
        "target_records": target_count,
        "processing_rate": target_count / execution_time if execution_time > 0 else 0,
        "memory_efficiency": target_count / memory_usage if memory_usage > 0 else 0,
        "execution_time": execution_time,
        "memory_usage": memory_usage
    }
```

### 2. ì•Œë¦¼ ì‹œìŠ¤í…œ

#### Slack ì•Œë¦¼
```python
def send_slack_notification(
    message: str,
    channel: str = "#data-pipeline",
    webhook_url: str = None
) -> None:
    """
    Slackìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
    """
    if webhook_url is None:
        webhook_url = Variable.get("SLACK_WEBHOOK_URL", default_var="")
    
    if not webhook_url:
        logger.warning("Slack ì›¹í›… URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return
    
    try:
        payload = {
            "text": message,
            "channel": channel,
            "username": "Data Pipeline Bot",
            "icon_emoji": ":robot_face:"
        }
        
        response = requests.post(webhook_url, json=payload)
        response.raise_for_status()
        
        logger.info(f"Slack ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {channel}")
        
    except Exception as e:
        logger.error(f"Slack ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
```

#### ì´ë©”ì¼ ì•Œë¦¼
```python
def send_email_notification(
    subject: str,
    message: str,
    recipients: list[str],
    smtp_config: dict = None
) -> None:
    """
    ì´ë©”ì¼ë¡œ ì•Œë¦¼ ì „ì†¡
    """
    if smtp_config is None:
        smtp_config = {
            "host": Variable.get("SMTP_HOST", default_var="localhost"),
            "port": Variable.get("SMTP_PORT", default_var=587),
            "username": Variable.get("SMTP_USERNAME", default_var=""),
            "password": Variable.get("SMTP_PASSWORD", default_var=""),
            "use_tls": Variable.get("SMTP_USE_TLS", default_var=True)
        }
    
    try:
        import smtplib
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        
        # ì´ë©”ì¼ ë©”ì‹œì§€ ìƒì„±
        msg = MIMEMultipart()
        msg['From'] = smtp_config["username"]
        msg['To'] = ", ".join(recipients)
        msg['Subject'] = subject
        
        msg.attach(MIMEText(message, 'plain'))
        
        # SMTP ì„œë²„ ì—°ê²° ë° ì „ì†¡
        with smtplib.SMTP(smtp_config["host"], smtp_config["port"]) as server:
            if smtp_config["use_tls"]:
                server.starttls()
            
            if smtp_config["username"] and smtp_config["password"]:
                server.login(smtp_config["username"], smtp_config["password"])
            
            server.send_message(msg)
        
        logger.info(f"ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {', '.join(recipients)}")
        
    except Exception as e:
        logger.error(f"ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
```

## ğŸš¨ ì—ëŸ¬ í•¸ë“¤ë§ ë° ë³µêµ¬

### 1. ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

#### ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
```python
import time
from functools import wraps

def retry_with_backoff(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0
):
    """
    ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt < max_retries - 1:
                        # ì§€ìˆ˜ ë°±ì˜¤í”„ ê³„ì‚°
                        delay = min(base_delay * (exponential_base ** attempt), max_delay)
                        
                        logger.warning(
                            f"{func.__name__} ì‹¤í–‰ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}"
                        )
                        logger.info(f"{delay:.2f}ì´ˆ í›„ ì¬ì‹œë„...")
                        
                        time.sleep(delay)
                    else:
                        logger.error(f"{func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
            
            raise last_exception
        
        return wrapper
    return decorator
```

### 2. íšŒë³µ ì „ëµ

#### ë°ì´í„° ë³µêµ¬
```python
def recover_failed_pipeline(
    table_config: dict,
    failure_point: str,
    **context
) -> dict[str, Any]:
    """
    ì‹¤íŒ¨í•œ íŒŒì´í”„ë¼ì¸ ë³µêµ¬
    """
    try:
        if failure_point == "data_copy":
            # ë°ì´í„° ë³µì‚¬ ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
            logger.info("ë°ì´í„° ë³µì‚¬ ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨, ë³µêµ¬ ì‹œë„...")
            
            # ì„ì‹œ í…Œì´ë¸” ì •ë¦¬
            cleanup_temp_tables(table_config, **context)
            
            # ë°ì´í„° ë³µì‚¬ ì¬ì‹œë„
            copy_result = copy_data_with_dynamic_sql(table_config, **context)
            
            return {
                "status": "recovered",
                "message": "ë°ì´í„° ë³µì‚¬ ë³µêµ¬ ì™„ë£Œ",
                "result": copy_result
            }
            
        elif failure_point == "merge_operation":
            # MERGE ì‘ì—…ì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
            logger.info("MERGE ì‘ì—…ì—ì„œ ì‹¤íŒ¨, ë³µêµ¬ ì‹œë„...")
            
            # MERGE ì‘ì—… ì¬ì‹œë„
            merge_result = execute_merge_operation(table_config, **context)
            
            return {
                "status": "recovered",
                "message": "MERGE ì‘ì—… ë³µêµ¬ ì™„ë£Œ",
                "result": merge_result
            }
            
        else:
            return {
                "status": "error",
                "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ì‹¤íŒ¨ ì§€ì : {failure_point}"
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"ë³µêµ¬ ì‹¤íŒ¨: {e}"
        }
```

## ğŸ“ˆ í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜

### 1. í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜

#### ë°ì´í„° ì†ŒìŠ¤ í”ŒëŸ¬ê·¸ì¸
```python
from abc import ABC, abstractmethod

class DataSourcePlugin(ABC):
    """ë°ì´í„° ì†ŒìŠ¤ í”ŒëŸ¬ê·¸ì¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def connect(self, config: dict) -> bool:
        """ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°"""
        pass
    
    @abstractmethod
    def extract_data(self, query: str, **kwargs) -> pd.DataFrame:
        """ë°ì´í„° ì¶”ì¶œ"""
        pass
    
    @abstractmethod
    def get_schema(self, table_name: str) -> dict:
        """ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ"""
        pass

class PostgreSQLPlugin(DataSourcePlugin):
    """PostgreSQL ë°ì´í„° ì†ŒìŠ¤ í”ŒëŸ¬ê·¸ì¸"""
    
    def connect(self, config: dict) -> bool:
        # PostgreSQL ì—°ê²° ë¡œì§
        pass
    
    def extract_data(self, query: str, **kwargs) -> pd.DataFrame:
        # PostgreSQL ë°ì´í„° ì¶”ì¶œ ë¡œì§
        pass
    
    def get_schema(self, table_name: str) -> dict:
        # PostgreSQL ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ë¡œì§
        pass
```

### 2. ì„¤ì • ê¸°ë°˜ ë™ì‘

#### ë™ì  ì„¤ì • ë¡œë”©
```python
def load_table_configs(environment: str = "production") -> list[dict]:
    """
    í™˜ê²½ë³„ í…Œì´ë¸” ì„¤ì • ë¡œë”©
    """
    config_path = f"configs/{environment}/table_configs.json"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            configs = json.load(f)
        
        logger.info(f"{environment} í™˜ê²½ ì„¤ì • ë¡œë”© ì™„ë£Œ: {len(configs)}ê°œ í…Œì´ë¸”")
        return configs
        
    except FileNotFoundError:
        logger.warning(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {config_path}")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"ì„¤ì • íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
```

## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥

### 1. ì‹¤ì‹œê°„ ì²˜ë¦¬
- Apache Kafkaë¥¼ í†µí•œ ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
- Change Data Capture (CDC) êµ¬í˜„
- ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ ë° ì§‘ê³„

### 2. ë¨¸ì‹ ëŸ¬ë‹ í†µí•©
- ë°ì´í„° í’ˆì§ˆ ìë™ ê°ì§€
- ì´ìƒ íŒ¨í„´ ìë™ íƒì§€
- ì˜ˆì¸¡ ëª¨ë¸ ìë™ í•™ìŠµ ë° ë°°í¬

### 3. í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ
- Kubernetes ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- ì„œë²„ë¦¬ìŠ¤ í•¨ìˆ˜ í†µí•©
- ë©€í‹° í´ë¼ìš°ë“œ ì§€ì›

---

ì´ ì•„í‚¤í…ì²˜ ë¬¸ì„œëŠ” DBT-Airflow Templateì˜ í•µì‹¬ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì„ ì„¤ëª…í•©ë‹ˆë‹¤. 
í”„ë¡œì íŠ¸ì˜ ë°œì „ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. 