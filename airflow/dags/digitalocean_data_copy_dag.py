"""
DigitalOcean ë°ì´í„° ë³µì‚¬ DAG
DigitalOcean PostgreSQLì—ì„œ ë¡œì»¬ PostgreSQLë¡œ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ëŠ” DAG

ì§€ì›í•˜ëŠ” ë™ê¸°í™” ëª¨ë“œ:
- full_sync: ì „ì²´ í…Œì´ë¸” ë™ê¸°í™”
- incremental_sync: ì¦ë¶„ ë™ê¸°í™” (íƒ€ìž„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
"""

import logging
import os
import subprocess
import time
from datetime import datetime, timedelta
from typing import Any

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# ê³µí†µ ëª¨ë“ˆ import
from common.data_copy_engine import DataCopyEngine
from common.database_operations import DatabaseOperations
from common.monitoring import MonitoringManager, ProgressTracker

from common.connection_manager import ConnectionManager
from common.dag_config_manager import DAGConfigManager
from common.settings import DAGSettings, BatchSettings

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "data_team",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email": ["admin@example.com"],
}

# DAG ì •ì˜
dag = DAG(
    "digitalocean_postgres_data_copy",
    default_args=default_args,
    description="Copy data from DigitalOcean PostgreSQL to local PostgreSQL with dbt snapshots",
    schedule_interval="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["postgres", "data-copy", "etl", "digitalocean", "dbt-snapshot"],
    max_active_runs=1,
)

# ì—°ê²° ID ì„¤ì • (DAGConfigManagerì—ì„œ ê°€ì ¸ì˜¤ê¸°)
dag_config = DAGConfigManager.get_dag_config("digitalocean_data_copy_dag")
SOURCE_CONN_ID = dag_config.get("source_connection", "digitalocean_postgres")
TARGET_CONN_ID = dag_config.get("target_connection", "postgres_default")

# dbt í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
DBT_PROJECT_PATH = "/opt/airflow/dbt"

# DigitalOcean PostgreSQL í…Œì´ë¸” ì„¤ì • (DAGConfigManagerì—ì„œ ê°€ì ¸ì˜¤ê¸°)
DIGITALOCEAN_TABLES_CONFIG = DAGConfigManager.get_table_configs("digitalocean_data_copy_dag")

# ì„¤ì •ì´ ë¹„ì–´ìžˆê±°ë‚˜ ìž˜ëª»ëœ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
if not DIGITALOCEAN_TABLES_CONFIG or not isinstance(DIGITALOCEAN_TABLES_CONFIG, list):
    logger.warning("DAGConfigManagerì—ì„œ í…Œì´ë¸” ì„¤ì •ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    DIGITALOCEAN_TABLES_CONFIG = [
        {
            "source": "public.users",
            "target": "public.users",
            "primary_key": "id",
            "sync_mode": "full_sync",
            "batch_size": 10000,
            "chunk_mode": True,
            "enable_checkpoint": True,
            "max_retries": 3,
            "description": "ì‚¬ìš©ìž í…Œì´ë¸”"
        }
    ]

# ë°ì´í„°ë² ì´ìŠ¤ ìž‘ì—… ê°ì²´ ì´ˆê¸°í™”
db_operations = DatabaseOperations(SOURCE_CONN_ID, TARGET_CONN_ID)

# ë°ì´í„° ë³µì‚¬ ì—”ì§„ ì´ˆê¸°í™”
data_copy_engine = DataCopyEngine(db_operations)
monitoring_manager = MonitoringManager()


def validate_connections(**context) -> bool:
    """ì—°ê²° ìœ íš¨ì„± ê²€ì‚¬"""
    try:
        # ConnectionManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê²° ê²€ì‚¬
        source_valid = ConnectionManager.test_connection(SOURCE_CONN_ID)
        target_valid = ConnectionManager.test_connection(TARGET_CONN_ID)
        
        if source_valid and target_valid:
            logger.info(f"âœ… ëª¨ë“  ì—°ê²° ê²€ì¦ ì„±ê³µ")
            return True
        else:
            logger.error(f"âŒ ì—°ê²° ê²€ì¦ ì‹¤íŒ¨: ì†ŒìŠ¤={source_valid}, íƒ€ê²Ÿ={target_valid}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ì—°ê²° ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

def copy_table_data(table_config: dict, **context) -> bool:
    """ê°œë³„ í…Œì´ë¸” ë°ì´í„° ë³µì‚¬"""
    try:
        logger.info(f"ðŸ”„ í…Œì´ë¸” ë³µì‚¬ ì‹œìž‘: {table_config['source']} -> {table_config['target']}")
        
        # ë°ì´í„° ë³µì‚¬ ì‹¤í–‰
        result = data_copy_engine.copy_data_with_custom_sql(
            source_conn_id=SOURCE_CONN_ID,
            target_conn_id=TARGET_CONN_ID,
            source_table=table_config['source'],
            target_table=table_config['target'],
            primary_key=table_config['primary_key'],
            sync_mode=table_config['sync_mode'],
            batch_size=table_config.get('batch_size', 10000),
            chunk_mode=table_config.get('chunk_mode', True),
            enable_checkpoint=table_config.get('enable_checkpoint', True),
            max_retries=table_config.get('max_retries', 3),
            incremental_field=table_config.get('incremental_field'),
            incremental_field_type=table_config.get('incremental_field_type'),
            custom_where=table_config.get('custom_where'),
            description=table_config.get('description', '')
        )
        
        if result:
            logger.info(f"âœ… í…Œì´ë¸” ë³µì‚¬ ì™„ë£Œ: {table_config['source']}")
            return True
        else:
            logger.error(f"âŒ í…Œì´ë¸” ë³µì‚¬ ì‹¤íŒ¨: {table_config['source']}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ í…Œì´ë¸” ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {table_config['source']} - {str(e)}")
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def run_dbt_snapshot(**context) -> bool:
    """dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰"""
    try:
        logger.info("ðŸ”„ dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ì‹œìž‘")
        
        # dbt ìŠ¤ëƒ…ìƒ· ëª…ë ¹ì–´ ì‹¤í–‰
        cmd = f"cd {DBT_PROJECT_PATH} && dbt snapshot"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            logger.info("âœ… dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ì™„ë£Œ")
            return True
        else:
            logger.error(f"âŒ dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        error_handler.handle_error(e, context)
        return False

def verify_data_integrity(**context) -> bool:
    """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
    try:
        logger.info("ðŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹œìž‘")
        
        # ê° í…Œì´ë¸”ë³„ ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
        for table_config in DIGITALOCEAN_TABLES_CONFIG:
            source_count = db_operations.get_table_row_count(
                conn_id=SOURCE_CONN_ID,
                table_name=table_config['source']
            )
            target_count = db_operations.get_table_row_count(
                conn_id=TARGET_CONN_ID,
                table_name=table_config['target']
            )
            
            if source_count == target_count:
                logger.info(f"âœ… {table_config['source']}: ì†ŒìŠ¤ {source_count}í–‰, íƒ€ê²Ÿ {target_count}í–‰ ì¼ì¹˜")
            else:
                logger.warning(f"âš ï¸ {table_config['source']}: ì†ŒìŠ¤ {source_count}í–‰, íƒ€ê²Ÿ {target_count}í–‰ ë¶ˆì¼ì¹˜")
        
        logger.info("âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        error_handler.handle_error(e, context)
        return False

# íƒœìŠ¤í¬ ì •ì˜
validate_connections_task = PythonOperator(
    task_id="validate_connections",
    python_callable=validate_connections,
    dag=dag,
)

# í…Œì´ë¸”ë³„ ë³µì‚¬ íƒœìŠ¤í¬ ìƒì„±
copy_tasks = []
for i, table_config in enumerate(DIGITALOCEAN_TABLES_CONFIG):
    task = PythonOperator(
        task_id=f"copy_table_{i}_{table_config['source'].replace('.', '_').replace('-', '_')}",
        python_callable=copy_table_data,
        op_kwargs={"table_config": table_config},
        dag=dag,
    )
    copy_tasks.append(task)

run_dbt_snapshot_task = PythonOperator(
    task_id="run_dbt_snapshot",
    python_callable=run_dbt_snapshot,
    dag=dag,
)

verify_integrity_task = PythonOperator(
    task_id="verify_data_integrity",
    python_callable=verify_data_integrity,
    dag=dag,
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
validate_connections_task >> copy_tasks
copy_tasks >> run_dbt_snapshot_task
run_dbt_snapshot_task >> verify_integrity_task 