"""
DAG for setting up Airflow Variables for table configurations and copy methods
- ëª¨ë“  ì„¤ì •ê°’ì„ Airflow Variablesë¡œ ê´€ë¦¬
- settings.pyëŠ” ì´ ì„¤ì •ì„ ê°€ì ¸ì˜¤ëŠ” ê¸°ëŠ¥ë§Œ ì œê³µ
"""

import json
from datetime import datetime
import os

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator

# DAG configuration
dag_config = {
    "dag_id": "variable_setup_dag",
    "schedule_interval": "@once",  # Run once manually
    "start_date": datetime(2023, 1, 1),
    "catchup": False,
    "default_args": {"retries": 1},
    "tags": ["setup", "variables", "configuration"],
}

# DAG configurations (DBTë³„ ì„¤ì •)
DAG_CONFIGS = {
    "postgres_multi_table_copy_refactored": {
        "dag_id": "postgres_multi_table_copy_refactored",
        "description": "PostgreSQL Multi-Table Data Copy DAG (Refactored)",
        "schedule_interval": "@am1am7",
        "enabled": True,
        "tags": ["postgres", "data-copy", "etl", "refactored"],
        "source_connection": "fs2_postgres",
        "target_connection": "postgres_default",
        "tables": ["ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°", "ff_v3_ff_sec_entity", "sym_v1_sym_ticker_exchange", "sym_v1_sym_coverage"],
        "execution_status": "active",
        "last_successful_run": "2025-08-21T01:08:45.430516+00:00",
        "avg_execution_time_minutes": 45,
        "success_rate_percent": 95.0
    },
    "postgres_multi_table_copy_refactored2": {
        "dag_id": "postgres_multi_table_copy_refactored2",
        "description": "PostgreSQL Multi-Table Data Copy DAG (Refactored)",
        "schedule_interval": "@am9",
        "enabled": True,
        "tags": ["postgres", "data-copy", "etl", "refactored"],
        "source_connection": "digitalocean_postgres",
        "target_connection": "postgres_default",
        "tables": ["scrap.stock_info","scrap.etf_info"],
        "execution_status": "active",
        "last_successful_run": "2025-08-21T01:08:45.430516+00:00",
        "avg_execution_time_minutes": 45,
        "success_rate_percent": 95.0
    },
    "digitalocean_data_copy_dag": {
        "dag_id": "digitalocean_data_copy_dag",
        "description": "DigitalOcean PostgreSQL Data Copy DAG",
        "schedule_interval": "@daily",
        "enabled": True,
        "tags": ["postgres", "data-copy", "etl", "digitalocean", "dbt-snapshot"],
        "source_connection": "digitalocean_postgres",
        "target_connection": "postgres_default",
        "tables": ["public.users", "public.orders"],
        "execution_status": "pending",
        "last_successful_run": None,
        "avg_execution_time_minutes": None,
        "success_rate_percent": None
    },
    "hybrid_data_copy_dag": {
        "dag_id": "hybrid_data_copy_dag",
        "description": "Hybrid Data Copy DAG (DigitalOcean to Multiple Targets)",
        "schedule_interval": "@daily",
        "enabled": True,
        "tags": ["postgres", "data-copy", "etl", "hybrid", "multi-target", "digitalocean"],
        "source_connection": "digitalocean_postgres",
        "target_connections": ["postgres_default", "fs2_postgres"],
        "tables": ["public.users", "public.orders"],
        "execution_status": "pending",
        "last_successful_run": None,
        "avg_execution_time_minutes": None,
        "success_rate_percent": None
    },
    "postgres_data_copy_dag": {
        "dag_id": "postgres_data_copy_dag",
        "description": "PostgreSQL Multi-Table Data Copy DAG (Original)",
        "schedule_interval": "@daily",
        "enabled": False,  # ë¦¬íŒ©í† ë§ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
        "tags": ["postgres", "data-copy", "etl", "multi-table", "dbt-snapshot"],
        "source_connection": "fs2_postgres",
        "target_connection": "postgres_default",
        "tables": ["fds_íŒ©ì…‹.ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°"],
        "execution_status": "deprecated",
        "last_successful_run": None,
        "avg_execution_time_minutes": None,
        "success_rate_percent": None
    },
    "dbt_processing_dag": {
        "dag_id": "dbt_processing_dag",
        "description": "DBT Processing DAG",
        "schedule_interval": "@daily",
        "enabled": True,
        "tags": ["dbt", "data-transformation", "etl"],
        "source_connection": "postgres_default",
        "target_connection": "postgres_default",
        "tables": ["staging", "marts"],
        "execution_status": "active",
        "last_successful_run": None,
        "avg_execution_time_minutes": None,
        "success_rate_percent": None
    }
}



# Table configurations (í…Œì´ë¸”ë³„ ì„¤ì • - copy_methodì™€ sync_mode í†µí•©)
TABLE_CONFIGS = {
    "ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°": {
        "dag_id": "postgres_multi_table_copy_refactored",
        "source_connection": "fs2_postgres",
        "source_schema": "fds_íŒ©ì…‹",
        "source_table": "fds_íŒ©ì…‹.ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°",
        "primary_key": ["ì¸í¬ë§¥ìŠ¤ì½”ë“œ", "íŒ©ì…‹ê±°ëž˜ì†Œ", "gts_exnm", "í‹°ì»¤"],
        "sync_mode": "full_sync",
        "batch_size": 10000,
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 5,
        "description": "ì¸í¬ë§¥ìŠ¤ ì¢…ëª© ë§ˆìŠ¤í„° - ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
    },
    "ff_v3_ff_sec_entity": {
        "dag_id": "postgres_multi_table_copy_refactored",
        "source_connection": "fs2_postgres",
        "source_schema": "fds_copy",
        "source_table": "fds_copy.ff_v3_ff_sec_entity",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.ff_v3_ff_sec_entity",
        "primary_key": ["fsym_id"],
        "sync_mode": "full_sync",
        "batch_size": 20000,
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 5,
        "description": "FF v3 ë³´ì•ˆ ì—”í‹°í‹° - ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
    },
    "sym_v1_sym_ticker_exchange": {
        "dag_id": "postgres_multi_table_copy_refactored",
        "source_connection": "fs2_postgres",
        "source_schema": "fds_copy",
        "source_table": "fds_copy.sym_v1_sym_ticker_exchange",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.sym_v1_sym_ticker_exchange",
        "primary_key": ["fsym_id"],
        "sync_mode": "full_sync",
        "batch_size": 10000,
        "custom_where": "SPLIT_PART(ticker_exchange, '-', 2) IN ('BRU','TSE','SEC','SHE','SHG','SSC','BER','DUS','ETR','FRA','HAM','HAN','MUN','STU','TGAT','XEX','PAR','LIS','LON','HKG','HSC','SZSC','JKT','MIL','FKA','JAS','NGO','OSE','SAP','TKS','KRX','AMS','SES','ROCO','TAI','ADF','ASE','BATS','BATY','BOS','CBO','CHI','CIS','EDGA','EDGX','EPRL','IEXG','ISE','LTSE','MEMX','NAS','NYS','PHL','PSE','HSTC','STC')",
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "ì‹¬ë³¼ í‹°ì»¤ ê±°ëž˜ì†Œ - ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
    },
    "sym_v1_sym_coverage": {
        "dag_id": "postgres_multi_table_copy_refactored",
        "source_connection": "fs2_postgres",
        "source_schema": "fds_copy",
        "source_table": "fds_copy.sym_v1_sym_coverage",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.sym_v1_sym_coverage",
        "primary_key": ["fsym_id"],
        "sync_mode": "full_sync",
        "batch_size": 5000,
        "custom_where": "fsym_id like '%-L' AND fref_security_type NOT IN ('ETF_UVI','ETF_NAV','NVDR','ALIEN','RIGHT','WARRANT') AND universe_type = 'EQ'",
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "ì‹¬ë³¼ ì»¤ë²„ë¦¬ì§€ - ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
    },
    "edi_690": {
        "dag_id": "postgres_multi_table_copy_refactored",
        "source_connection": "fs2_postgres",
        "source_schema": "m23",
        "source_table": "m23.edi_690",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.edi_690",
        "primary_key": ["eventcd", "eventid", "optionid", "serialid", "scexhid", "sedolid"],
        "sync_mode": "incremental_sync",
        "batch_size": 10000,
        "incremental_field": "changed",
        "incremental_field_type": "yyyymmdd",
        "custom_where": "changed >= '20250812'",
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "EDI 690 ì´ë²¤íŠ¸ ë°ì´í„° - ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
    },
    "digitalocean_users": {
        "dag_id": "digitalocean_data_copy_dag",
        "source_connection": "digitalocean_postgres",
        "source_schema": "public",
        "source_table": "public.users",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.digitalocean_users",
        "primary_key": ["id"],
        "sync_mode": "full_sync",
        "batch_size": 10000,
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "DigitalOcean ì‚¬ìš©ìž í…Œì´ë¸” - ì „ì²´ ë™ê¸°í™”"
    },
    "digitalocean_orders": {
        "dag_id": "digitalocean_data_copy_dag",
        "source_connection": "digitalocean_postgres",
        "source_schema": "public",
        "source_table": "public.orders",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.digitalocean_orders",
        "primary_key": ["order_id"],
        "sync_mode": "incremental_sync",
        "batch_size": 5000,
        "incremental_field": "created_at",
        "incremental_field_type": "timestamp",
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "DigitalOcean ì£¼ë¬¸ í…Œì´ë¸” - ì¦ë¶„ ë™ê¸°í™”"
    },
    "edi_690_edi": {
        "dag_id": "postgres_multi_table_copy_refactored_edi",
        "source_connection": "fs2_postgres",
        "source_schema": "m23",
        "source_table": "m23.edi_690",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.edi_690",
        "primary_key": ["eventcd", "eventid", "optionid", "serialid", "scexhid", "sedolid"],
        "sync_mode": "incremental_sync",
        "batch_size": 10000,
        "incremental_field": "changed",
        "incremental_field_type": "yyyymmdd",
        "custom_where": "changed >= '20250812'",
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "EDI 690 ì´ë²¤íŠ¸ ë°ì´í„° - EDI DAGìš©"
    },
    "edi_691_edi": {
        "dag_id": "postgres_multi_table_copy_refactored_edi",
        "source_connection": "fs2_postgres",
        "source_schema": "m23",
        "source_table": "m23.edi_691",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.edi_691",
        "primary_key": ["eventcd", "eventid", "optionid", "serialid", "scexhid", "sedolid"],
        "sync_mode": "incremental_sync",
        "batch_size": 10000,
        "incremental_field": "changed",
        "incremental_field_type": "yyyymmdd",
        "custom_where": "changed >= '20250812'",
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "EDI 691 ì´ë²¤íŠ¸ ë°ì´í„° - EDI DAGìš©"
    },
    "scrap.stock_info": {
        "dag_id": "postgres_multi_table_copy_refactored2",
        "source_connection": "digitalocean_postgres",
        "source_schema": "scrap",
        "source_table": "scrap.stock_info",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.scrap_stock_info",
        "primary_key": ["id"],
        "sync_mode": "full_sync",
        "batch_size": 10000,
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "Scrap ì¢…ëª© ì •ë³´ - ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
    },
    "scrap.etf_info": {
        "dag_id": "postgres_multi_table_copy_refactored2",
        "source_connection": "digitalocean_postgres",
        "source_schema": "scrap",
        "source_table": "scrap.etf_info",
        "target_connection": "postgres_default",
        "target_schema": "raw_data",
        "target_table": "raw_data.scrap_etf_info",
        "primary_key": ["id"],
        "sync_mode": "full_sync",
        "batch_size": 10000,
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "Scrap ETF ì •ë³´ - ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
    }
}



# Schedule configurations (ìŠ¤ì¼€ì¤„ ì„¤ì •)
SCHEDULE_CONFIGS = {
    "am1am7": "0 16,22 * * *",  # ë§¤ì¼ ì˜¤ì „ 1ì‹œ, ì˜¤ì „ 7ì‹œ (KST)
    "pm18": "0 9 * * *",          # ë§¤ì¼ ì˜¤í›„ 18ì‹œ (KST)
    "am9": "0 0 * * *",          # ë§¤ì¼ ì˜¤ì „ 9ì‹œ (KST)
    "daily": "0 2 * * *",          # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
    "hourly": "0 * * * *",         # ë§¤ì‹œê°„
    "manual": "@once",             # ìˆ˜ë™ ì‹¤í–‰
}

# Environment configurations (í™˜ê²½ë³„ ì„¤ì •)
ENVIRONMENT_CONFIGS = {
    "development": {
        "debug": True,
        "log_level": "DEBUG",
        "email_notifications": False,
        "retries": 1,
        "batch_size": 1000,
    },
    "staging": {
        "debug": False,
        "log_level": "INFO",
        "email_notifications": True,
        "retries": 2,
        "batch_size": 5000,
    },
    "production": {
        "debug": False,
        "log_level": "WARNING",
        "email_notifications": True,
        "retries": 3,
        "batch_size": 10000,
    }
}

# Sync mode configurations (ë™ê¸°í™” ëª¨ë“œë³„ ì„¤ì • - copy_methodì™€ sync_mode í†µí•©)
SYNC_MODE_CONFIGS = {
    "full_sync": {
        "description": "Full table sync - truncate and insert all data",
        "truncate_before_copy": True,
        "parallel_workers": 4,
        "timeout_minutes": 30,
        "sync_mode": "full_sync",
    },
    "incremental_sync": {
        "description": "Incremental sync - only new/modified records",
        "truncate_before_copy": False,
        "parallel_workers": 2,
        "timeout_minutes": 15,
        "sync_mode": "incremental_sync",
        "upsert_strategy": "insert_on_conflict",
    }
}

# Execution monitoring configurations (ì‹¤í–‰ ëª¨ë‹ˆí„°ë§ ì„¤ì •)
EXECUTION_MONITORING_CONFIGS = {
    "monitoring_enabled": True,
    "alert_thresholds": {
        "execution_time_minutes": 60,
        "success_rate_percent": 90.0,
        "error_count": 3,
        "memory_usage_mb": 2048
    },
    "notification_channels": {
        "email": ["admin@yourcompany.com", "data-team@yourcompany.com"],
        "slack": "#data-pipeline-alerts",
        "webhook": "https://hooks.slack.com/services/..."
    },
    "retry_policies": {
        "max_retries": 3,
        "retry_delay_minutes": 5,
        "exponential_backoff": True
    },
    "checkpoint_settings": {
        "enable_checkpoint": True,
        "checkpoint_interval_rows": 10000,
        "checkpoint_timeout_minutes": 30
    }
}

# dbt configurations
DBT_CONFIGS = {
    "snapshot_tables": ["ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°", "ff_v3_ff_sec_entity", "edi_690"],
    "snapshot_strategy": "timestamp",
    "snapshot_updated_at": "changed",
    "models_to_run": ["staging", "marts"],
    "test_after_run": True,
    "full_refresh": False,
    "project_path": "/opt/airflow/dbt",
    "profile_name": "postgres_data_copy",
    "target_name": "dev",
    "snapshot_select": "tag:infomax",
    "run_select": "tag:infomax",
    "test_select": "tag:infomax",
}

# ì²­í¬ ë°©ì‹ ë°ì´í„° ë³µì‚¬ ì„¤ì •
CHUNK_MODE_CONFIGS = {
    "default_settings": {
        "chunk_mode": True,
        "enable_checkpoint": True,
        "max_retries": 3,
        "description": "ê¸°ë³¸ ì²­í¬ ë°©ì‹ ì„¤ì • - ëª¨ë“  í…Œì´ë¸”ì— ì ìš©"
    },
    "size_based_recommendations": {
        "small": {  # 10ë§Œ í–‰ ì´í•˜
            "chunk_mode": False,
            "enable_checkpoint": False,
            "batch_size": 1000,
            "max_retries": 2,
            "reason": "ì†Œìš©ëŸ‰ í…Œì´ë¸”ì€ ê¸°ì¡´ ë°©ì‹ì´ ë” ë¹ ë¦„"
        },
        "medium": {  # 10ë§Œ ~ 100ë§Œ í–‰
            "chunk_mode": True,
            "enable_checkpoint": True,
            "batch_size": 5000,
            "max_retries": 3,
            "reason": "ì¤‘ê°„ í¬ê¸° í…Œì´ë¸”ì€ ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"
        },
        "large": {  # 100ë§Œ ~ 1000ë§Œ í–‰
            "chunk_mode": True,
            "enable_checkpoint": True,
            "batch_size": 10000,
            "max_retries": 5,
            "reason": "ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì€ ì²­í¬ ë°©ì‹ìœ¼ë¡œë§Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ ê°€ëŠ¥"
        },
        "xlarge": {  # 1000ë§Œ í–‰ ì´ìƒ
            "chunk_mode": True,
            "enable_checkpoint": True,
            "batch_size": 15000,
            "max_retries": 7,
            "reason": "ì´ˆëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì€ ì²­í¬ ë°©ì‹ê³¼ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìˆ˜"
        }
    },
    "environment_settings": {
        "development": {
            "default_chunk_mode": True,
            "default_enable_checkpoint": True,
            "default_max_retries": 3,
            "default_batch_size": 5000,
            "description": "ê°œë°œ í™˜ê²½ - ì•ˆì „í•œ ì²­í¬ ë°©ì‹ ê¸°ë³¸ê°’"
        },
        "staging": {
            "default_chunk_mode": True,
            "default_enable_checkpoint": True,
            "default_max_retries": 4,
            "default_batch_size": 8000,
            "description": "ìŠ¤í…Œì´ì§• í™˜ê²½ - ì•ˆì „í•œ ì²­í¬ ë°©ì‹, ì¤‘ê°„ ë°°ì¹˜ í¬ê¸°"
        },
        "production": {
            "default_chunk_mode": True,
            "default_enable_checkpoint": True,
            "default_max_retries": 5,
            "default_batch_size": 10000,
            "description": "í”„ë¡œë•ì…˜ í™˜ê²½ - ì•ˆì „í•œ ì²­í¬ ë°©ì‹, ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°"
        }
    }
}

# Worker configurations (ì›Œì»¤ ì„¤ì •)
WORKER_CONFIGS = {
    "default_workers": 4,
    "max_workers": 4,
    "min_workers": 2,
    "worker_timeout_seconds": 600,
    "worker_memory_limit_mb": 1024,
    "description": "ì›Œì»¤ ì„¤ì • - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ìµœì ê°’"
}

# Performance optimization configurations (ì„±ëŠ¥ ìµœì í™” ì„¤ì •)
PERFORMANCE_CONFIGS = {
    "enable_session_optimization": True,
    "enable_unlogged_staging": True,
    "enable_auto_analyze": True,
    "enable_auto_index": True,
    "enable_streaming_pipe": True,
    "session_parameters": {
        "synchronous_commit": "off",
        "statement_timeout": "0",
        "work_mem": "128MB",
        "lock_timeout": "300s",
    },
    "batch_size_optimization": {
        "large_table_threshold": 1000000,  # 100ë§Œ í–‰ ì´ìƒ
        "large_table_batch_size": 50000,   # 5ë§Œ í–‰
        "medium_table_threshold": 100000,  # 10ë§Œ í–‰ ì´ìƒ
        "medium_table_batch_size": 20000,  # 2ë§Œ í–‰
        "small_table_batch_size": 10000,   # 1ë§Œ í–‰
    },
    "parallel_processing": {
        "max_concurrent_tables": 2,        # ë™ì‹œ ì‹¤í–‰ í…Œì´ë¸” ìˆ˜
        "max_concurrent_chunks": 4,        # ë™ì‹œ ì‹¤í–‰ ì²­í¬ ìˆ˜
        "pool_name": "postgres_copy_pool",
    },
    "description": "ì„±ëŠ¥ ìµœì í™” ì„¤ì • - í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥"
}

# Table batch size configurations (í…Œì´ë¸”ë³„ ë°°ì¹˜ í¬ê¸° ì„¤ì •)
TABLE_BATCH_CONFIGS = {
    "ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°": 10000,
    "ff_v3_ff_sec_entity": 20000,
    "edi_690": 10000,
    "sym_v1_sym_ticker_exchange": 10000,
    "sym_v1_sym_coverage": 5000,
    "digitalocean_users": 10000,
    "digitalocean_orders": 5000,
    "description": "í…Œì´ë¸”ë³„ ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸° ì„¤ì •"
}

# Default DAG settings (ê¸°ë³¸ DAG ì„¤ì •)
DEFAULT_DAG_CONFIG = {
    "owner": "data_team",
    "depends_on_past": False,
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay_minutes": 5,
    "email": ["admin@yourcompany.com"],
    "start_date": "2024-01-01",
    "catchup": False,
    "max_active_runs": 1,
    "description": "ê¸°ë³¸ DAG ì„¤ì • - ëª¨ë“  DAGì— ì ìš©"
}

# Default tags (ê¸°ë³¸ íƒœê·¸)
DEFAULT_TAGS = ["postgres", "data-copy", "etl", "refactored"]





def setup_table_variables(**context):
    """Set up table configuration variables"""
    Variable.set("table_configs", json.dumps(TABLE_CONFIGS, indent=2))
    print(f"Set table_configs variable with {len(TABLE_CONFIGS)} tables")
    return f"Successfully set {len(TABLE_CONFIGS)} table configurations"





def setup_schedule_variables(**context):
    """Set up schedule configuration variables"""
    Variable.set("schedule_configs", json.dumps(SCHEDULE_CONFIGS, indent=2))
    print(f"Set schedule_configs variable with {len(SCHEDULE_CONFIGS)} schedules")
    return f"Successfully set {len(SCHEDULE_CONFIGS)} schedule configurations"


def setup_environment_variables(**context):
    """Set up environment configuration variables"""
    Variable.set("environment_configs", json.dumps(ENVIRONMENT_CONFIGS, indent=2))
    print(f"Set environment_configs variable with {len(ENVIRONMENT_CONFIGS)} environments")
    return f"Successfully set {len(ENVIRONMENT_CONFIGS)} environment configurations"


def setup_sync_mode_variables(**context):
    """Set up sync mode configuration variables"""
    Variable.set("sync_mode_configs", json.dumps(SYNC_MODE_CONFIGS, indent=2))
    print(f"Set sync_mode_configs variable with {len(SYNC_MODE_CONFIGS)} sync modes")
    return f"Successfully set {len(SYNC_MODE_CONFIGS)} sync mode configurations"


def setup_dbt_variables(**context):
    """Set up dbt configuration variables"""
    Variable.set("dbt_configs", json.dumps(DBT_CONFIGS, indent=2))
    print(f"Set dbt_configs variable with {len(DBT_CONFIGS)} dbt configurations")
    return f"Successfully set {len(DBT_CONFIGS)} dbt configurations"


def setup_chunk_mode_variables(**context):
    """Set up chunk mode configuration variables"""
    # ì„¤ì • ë²„ì „ ê´€ë¦¬ ì¶”ê°€
    from datetime import datetime
    
    config_with_version = {
        "version": "1.0.0",
        "created_at": datetime.now().isoformat(),
        "environment": os.getenv("AIRFLOW_ENV", "development"),
        "configs": CHUNK_MODE_CONFIGS
    }
    
    Variable.set("chunk_mode_configs", json.dumps(config_with_version, indent=2))
    print(f"Set chunk_mode_configs variable with chunk mode configurations")
    print(f"  - Version: {config_with_version['version']}")
    print(f"  - Created at: {config_with_version['created_at']}")
    print(f"  - Environment: {config_with_version['environment']}")
    print("  - Default settings configured")
    print("  - Size-based recommendations configured")
    print("  - Environment-specific settings configured")
    return "Successfully set chunk mode configurations"


def setup_worker_variables(**context):
    """Set up worker configuration variables"""
    Variable.set("worker_configs", json.dumps(WORKER_CONFIGS, indent=2))
    print(f"Set worker_configs variable with worker configurations")
    return "Successfully set worker configurations"


def setup_performance_variables(**context):
    """Set up performance optimization configuration variables"""
    Variable.set("performance_configs", json.dumps(PERFORMANCE_CONFIGS, indent=2))
    print(f"Set performance_configs variable with performance configurations")
    return "Successfully set performance configurations"


def setup_table_batch_variables(**context):
    """Set up table batch size configuration variables"""
    Variable.set("table_batch_configs", json.dumps(TABLE_BATCH_CONFIGS, indent=2))
    print(f"Set table_batch_configs variable with table batch configurations")
    return "Successfully set table batch configurations"


def setup_default_dag_variables(**context):
    """Set up default DAG configuration variables"""
    Variable.set("default_dag_config", json.dumps(DEFAULT_DAG_CONFIG, indent=2))
    Variable.set("default_tags", json.dumps(DEFAULT_TAGS, indent=2))
    print("Set default DAG configuration variables")
    return "Successfully set default DAG configuration variables"


def setup_dag_variables(**context):
    """Set up DAG configuration variables"""
    Variable.set("dag_configs", json.dumps(DAG_CONFIGS, indent=2))
    print(f"Set dag_configs variable with {len(DAG_CONFIGS)} DAG configurations")
    return f"Successfully set {len(DAG_CONFIGS)} DAG configurations"


def setup_monitoring_variables(**context):
    """Set up execution monitoring configuration variables"""
    Variable.set("execution_monitoring_configs", json.dumps(EXECUTION_MONITORING_CONFIGS, indent=2))
    print(f"Set execution_monitoring_configs variable with {len(EXECUTION_MONITORING_CONFIGS)} monitoring configurations")
    return f"Successfully set {len(EXECUTION_MONITORING_CONFIGS)} monitoring configurations"


def verify_variables(**context):
    """Verify all variables are set correctly"""
    try:
        # ëª¨ë“  ì„¤ì • ë³€ìˆ˜ ê²€ì¦
        variables_to_check = [
            "table_configs", 
            "schedule_configs",
            "environment_configs",
            "sync_mode_configs",
            "dbt_configs",
            "chunk_mode_configs",
            "worker_configs",
            "performance_configs",
            "table_batch_configs",
            "default_dag_config",
            "default_tags",
            "dag_configs",
            "execution_monitoring_configs"
        ]
        
        verification_results = {}
        for var_name in variables_to_check:
            try:
                var_value = Variable.get(var_name, deserialize_json=True, default_var=None)
                if var_value:
                    verification_results[var_name] = f"âœ… ì„¤ì •ë¨ ({len(var_value) if isinstance(var_value, (dict, list)) else 'N/A'})"
                else:
                    verification_results[var_name] = "âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ"
            except Exception as e:
                verification_results[var_name] = f"âŒ ì˜¤ë¥˜: {str(e)}"

        print("ðŸ” ëª¨ë“  ì„¤ì • ë³€ìˆ˜ ê²€ì¦ ê²°ê³¼:")
        for var_name, result in verification_results.items():
            print(f"  - {var_name}: {result}")
        
        # ì²­í¬ ë°©ì‹ ì„¤ì • ê²€ì¦
        print("\nðŸ” ì²­í¬ ë°©ì‹ ì„¤ì • ê²€ì¦:")
        try:
            table_configs = json.loads(Variable.get("table_configs"))
            for table_name, config in table_configs.items():
                chunk_mode = config.get("chunk_mode", True)
                enable_checkpoint = config.get("enable_checkpoint", True)
                max_retries = config.get("max_retries", 3)
                sync_mode = config.get("sync_mode", "unknown")
                
                print(f"  - {table_name}:")
                print(f"    * Chunk mode: {'âœ… í™œì„±í™”' if chunk_mode else 'âŒ ë¹„í™œì„±í™”'}")
                print(f"    * Checkpoint: {'âœ… í™œì„±í™”' if enable_checkpoint else 'âŒ ë¹„í™œì„±í™”'}")
                print(f"    * Max retries: {max_retries}")
                print(f"    * Sync mode: {sync_mode}")
                
                # ì„¤ì • ìœ íš¨ì„± ê²€ì¦
                if enable_checkpoint and not chunk_mode:
                    print(f"    âš ï¸  ê²½ê³ : ì²´í¬í¬ì¸íŠ¸ëŠ” ì²­í¬ ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì‚¬ìš© ê°€ëŠ¥")
        except Exception as e:
            print(f"  âŒ í…Œì´ë¸” ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: {e}")

        return "All variables verified successfully"
    except Exception as e:
        print(f"âŒ Error verifying variables: {e}")
        raise


# Create DAG
dag = DAG(**dag_config)

# Create tasks
setup_tables_task = PythonOperator(
    task_id="setup_table_variables",
    python_callable=setup_table_variables,
    dag=dag,
)

setup_schedules_task = PythonOperator(
    task_id="setup_schedule_variables",
    python_callable=setup_schedule_variables,
    dag=dag,
)

setup_environments_task = PythonOperator(
    task_id="setup_environment_variables",
    python_callable=setup_environment_variables,
    dag=dag,
)

setup_sync_methods_task = PythonOperator(
    task_id="setup_sync_mode_variables",
    python_callable=setup_sync_mode_variables,
    dag=dag,
)

setup_dbt_task = PythonOperator(
    task_id="setup_dbt_variables",
    python_callable=setup_dbt_variables,
    dag=dag,
)

setup_chunk_mode_task = PythonOperator(
    task_id="setup_chunk_mode_variables",
    python_callable=setup_chunk_mode_variables,
    dag=dag,
)

setup_worker_task = PythonOperator(
    task_id="setup_worker_variables",
    python_callable=setup_worker_variables,
    dag=dag,
)

setup_performance_task = PythonOperator(
    task_id="setup_performance_variables",
    python_callable=setup_performance_variables,
    dag=dag,
)

setup_table_batch_task = PythonOperator(
    task_id="setup_table_batch_variables",
    python_callable=setup_table_batch_variables,
    dag=dag,
)

setup_default_dag_task = PythonOperator(
    task_id="setup_default_dag_variables",
    python_callable=setup_default_dag_variables,
    dag=dag,
)

setup_dag_task = PythonOperator(
    task_id="setup_dag_variables",
    python_callable=setup_dag_variables,
    dag=dag,
)

setup_monitoring_task = PythonOperator(
    task_id="setup_monitoring_variables",
    python_callable=setup_monitoring_variables,
    dag=dag,
)

verify_task = PythonOperator(
    task_id="verify_variables",
    python_callable=verify_variables,
    dag=dag,
)

# Set task dependencies (ìˆœì°¨ì  ì‹¤í–‰)
(
    setup_tables_task
    >> setup_schedules_task
    >> setup_environments_task
    >> setup_sync_methods_task
    >> setup_dbt_task
    >> setup_chunk_mode_task
    >> setup_worker_task
    >> setup_performance_task
    >> setup_table_batch_task
    >> setup_default_dag_task
    >> setup_dag_task
    >> setup_monitoring_task
    >> verify_task
)
