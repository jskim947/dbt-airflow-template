"""
Data Copy Engine Module
ë°ì´í„° ë³µì‚¬ í•µì‹¬ ë¡œì§, CSV ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°, ì„ì‹œ í…Œì´ë¸” ê´€ë¦¬, MERGE ì‘ì—… ë“±ì„ ë‹´ë‹¹
"""

import logging
import os
import time
import csv
import json
import gc
import psutil
from typing import Any
from contextlib import contextmanager

import pandas as pd
import psycopg2

from common.database_operations import DatabaseOperations

logger = logging.getLogger(__name__)


class DataCopyEngine:
    """ë°ì´í„° ë³µì‚¬ ì—”ì§„ í´ë˜ìŠ¤"""
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ê°’ ìƒìˆ˜
    class MemoryThresholds:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ê°’ ì •ì˜"""
        WARNING_MB = 500      # ê²½ê³  ì„ê³„ê°’ (500MB)
        CRITICAL_MB = 1000    # ìœ„í—˜ ì„ê³„ê°’ (1GB)
        EMERGENCY_MB = 1500   # ê¸´ê¸‰ ì„ê³„ê°’ (1.5GB)
    
    # ì²­í¬ ë°©ì‹ ê¸°ë³¸ ì„¤ì • ìƒìˆ˜
    class ChunkModeDefaults:
        """ì²­í¬ ë°©ì‹ ê¸°ë³¸ ì„¤ì •"""
        DEFAULT_CHUNK_MODE = True
        DEFAULT_ENABLE_CHECKPOINT = True
        DEFAULT_MAX_RETRIES = 3
        DEFAULT_BATCH_SIZE = 10000

    def __init__(self, db_ops: DatabaseOperations, temp_dir: str = "/tmp"):
        """
        ì´ˆê¸°í™”

        Args:
            db_ops: DatabaseOperations ì¸ìŠ¤í„´ìŠ¤
            temp_dir: ì„ì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.db_ops = db_ops
        self.temp_dir = temp_dir
        self.source_hook = db_ops.get_source_hook()
        self.target_hook = db_ops.get_target_hook()
        
        # ì›Œì»¤ ì„¤ì • ë° ì„±ëŠ¥ ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        try:
            from common.settings import BatchSettings
            self.worker_config = BatchSettings.get_worker_config()
            self.performance_config = BatchSettings.get_performance_optimization_config()
            logger.info(f"ì›Œì»¤ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.worker_config}")
            logger.info(f"ì„±ëŠ¥ ìµœì í™” ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.performance_config}")
        except ImportError:
            # ì„¤ì • ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
            self.worker_config = {
                "default_workers": 4,
                "max_workers": 8,
                "min_workers": 2,
                "worker_timeout_seconds": 600,
                "worker_memory_limit_mb": 1024,
            }
            self.performance_config = {
                "enable_session_optimization": True,
                "enable_unlogged_staging": True,
                "enable_auto_analyze": True,
                "enable_auto_index": True,
                "enable_streaming_pipe": False,
                "session_parameters": {
                    "synchronous_commit": "off",
                    "statement_timeout": "0",
                    "work_mem": "128MB",
                    "lock_timeout": "300s",
                },
                "batch_size_optimization": {
                    "large_table_threshold": 1000000,
                    "large_table_batch_size": 50000,
                    "medium_table_threshold": 100000,
                    "medium_table_batch_size": 20000,
                    "small_table_batch_size": 10000,
                },
                "parallel_processing": {
                    "max_concurrent_tables": 2,
                    "max_concurrent_chunks": 4,
                    "pool_name": "postgres_copy_pool",
                }
            }
            logger.warning("ì„¤ì • ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")

    def export_to_csv(
        self,
        table_name: str,
        csv_path: str,
        where_clause: str | None = None,
        batch_size: int = 10000,
        order_by_field: str | None = None,
        chunk_mode: bool = True,  # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°
        enable_checkpoint: bool = True,  # ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”
        max_retries: int = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    ) -> int:
        """
        í…Œì´ë¸”ì„ CSVë¡œ ë‚´ë³´ë‚´ê¸° (ì²­í¬ ë°©ì‹ ì§€ì›)

        Args:
            table_name: í…Œì´ë¸”ëª…
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            where_clause: WHERE ì ˆ ì¡°ê±´
            batch_size: ë°°ì¹˜ í¬ê¸°
            order_by_field: ì •ë ¬ ê¸°ì¤€ í•„ë“œ
            chunk_mode: ì²­í¬ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ (True: ë©”ëª¨ë¦¬ ëˆ„ì  ì—†ìŒ, False: ê¸°ì¡´ ë°©ì‹)
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ í™œì„±í™” ì—¬ë¶€
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            ë‚´ë³´ë‚¸ í–‰ ìˆ˜
        """
        try:
            # ì²­í¬ ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš° ìƒˆë¡œìš´ ë°©ì‹ ì‚¬ìš©
            if chunk_mode:
                logger.info(f"ì²­í¬ ë°©ì‹ìœ¼ë¡œ CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘: {table_name}")
                return self._export_to_csv_chunked(
                    table_name, csv_path, where_clause, batch_size, 
                    order_by_field, enable_checkpoint, max_retries
                )
            else:
                logger.info(f"ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘: {table_name}")
                return self._export_to_csv_legacy(
                    table_name, csv_path, where_clause, batch_size, order_by_field
                )

        except Exception as e:
            logger.error(f"CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {table_name} -> {csv_path}, ì˜¤ë¥˜: {e!s}")
            raise

    def _optimize_target_session(self, hook) -> None:
        """
        íƒ€ê²Ÿ ì„¸ì…˜ì—ì„œ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¸ì…˜ íŒŒë¼ë¯¸í„° ì„¤ì •
        
        Args:
            hook: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ í›…
        """
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ìŠ¤í‚µ
        if not self.performance_config.get("enable_session_optimization", True):
            logger.info("ì„¸ì…˜ ìµœì í™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
            
        try:
            with hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # ì„¤ì •ì—ì„œ ì„¸ì…˜ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
                    session_params = self.performance_config.get("session_parameters", {})
                    
                    # WAL/ë™ê¸°í™” ë¹„ìš© ì ˆê°
                    if "synchronous_commit" in session_params:
                        cursor.execute(f"SET synchronous_commit = {session_params['synchronous_commit']};")
                    
                    # íƒ€ì„ì•„ì›ƒ ì„¤ì • (íƒœìŠ¤í¬ ë ˆë²¨ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë³´í˜¸)
                    if "statement_timeout" in session_params:
                        cursor.execute(f"SET statement_timeout = '{session_params['statement_timeout']}';")
                    
                    # ì¡°ì¸/ì •ë ¬ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì„¤ì •
                    if "work_mem" in session_params:
                        cursor.execute(f"SET work_mem = '{session_params['work_mem']}';")
                    
                    # ì ê¸ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    if "lock_timeout" in session_params:
                        cursor.execute(f"SET lock_timeout = '{session_params['lock_timeout']}';")
                    
                    logger.info(f"íƒ€ê²Ÿ ì„¸ì…˜ ìµœì í™” ì™„ë£Œ: {session_params}")
                    
        except Exception as e:
            logger.warning(f"íƒ€ê²Ÿ ì„¸ì…˜ ìµœì í™” ì‹¤íŒ¨ (ê¸°ë³¸ ì„¤ì • ì‚¬ìš©): {e}")

    def _analyze_target_table(self, target_table: str) -> None:
        """
        íƒ€ê²Ÿ í…Œì´ë¸”ì— ANALYZE ì‹¤í–‰í•˜ì—¬ í†µê³„ ìµœì‹ í™”
        
        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
        """
        # ìë™ ANALYZEê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ìŠ¤í‚µ
        if not self.performance_config.get("enable_auto_analyze", True):
            logger.info("ìë™ ANALYZEê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
            
        try:
            with self.target_hook.get_conn() as conn:
                # CREATE INDEX CONCURRENTLYëŠ” íŠ¸ëœì­ì…˜ ë¸”ë¡ ë°–ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨ â†’ autocommit í™œì„±í™”
                original_autocommit = getattr(conn, "autocommit", False)
                try:
                    conn.autocommit = True
                except Exception:
                    pass
                with conn.cursor() as cursor:
                    cursor.execute(f"ANALYZE {target_table};")
                    logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” {target_table} ANALYZE ì™„ë£Œ")
                    
        except Exception as e:
            logger.warning(f"íƒ€ê²Ÿ í…Œì´ë¸” {target_table} ANALYZE ì‹¤íŒ¨: {e}")

    def _check_and_create_indexes(self, table_name: str, primary_keys: list[str]) -> None:
        """
        MERGE í‚¤ì— ëŒ€í•œ ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ ì ê²€ ë° í•„ìš” ì‹œ ìë™ ìƒì„±
        
        Args:
            table_name: í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ëª©ë¡
        """
        # ìë™ ì¸ë±ìŠ¤ ìƒì„±ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ìŠ¤í‚µ
        if not self.performance_config.get("enable_auto_index", True):
            logger.info("ìë™ ì¸ë±ìŠ¤ ìƒì„±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
            
        try:
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
                    for pk in primary_keys:
                        index_name = f"idx_{table_name.replace('.', '_')}_{pk}"
                        
                        # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                        cursor.execute(f"""
                            SELECT EXISTS (
                                SELECT 1 FROM pg_indexes 
                                WHERE tablename = '{table_name.split('.')[-1]}' 
                                AND indexname = '{index_name}'
                            )
                        """)
                        
                        exists = cursor.fetchone()[0]
                        
                        if not exists:
                            # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ CONCURRENTLYë¡œ ìƒì„±
                            try:
                                create_index_sql = f"CREATE INDEX CONCURRENTLY {index_name} ON {table_name} ({pk})"
                                cursor.execute(create_index_sql)
                                logger.info(f"ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index_name} on {table_name}({pk})")
                            except Exception as e:
                                logger.warning(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {index_name}, ì˜¤ë¥˜: {e}")
                        else:
                            logger.info(f"ì¸ë±ìŠ¤ ì´ë¯¸ ì¡´ì¬: {index_name}")
                # autocommit ë³µì›
                try:
                    conn.autocommit = original_autocommit
                except Exception:
                    pass
                
        except Exception as e:
            logger.warning(f"ì¸ë±ìŠ¤ ì ê²€ ë° ìƒì„± ì‹¤íŒ¨: {e}")

    def copy_data_with_streaming_pipe(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        custom_where: str | None = None,
        batch_size: int = 10000,
        verified_target_schema: dict[str, Any] | None = None,
        chunk_mode: bool = True,
        enable_checkpoint: bool = True,
        max_retries: int = 3
    ) -> dict[str, Any]:
        """
        ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ (ì¤‘ê°„ CSV íŒŒì¼ ì—†ìŒ)
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ëª©ë¡
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            custom_where: ì‚¬ìš©ì ì •ì˜ WHERE ì¡°ê±´
            batch_size: ë°°ì¹˜ í¬ê¸°
            verified_target_schema: ê²€ì¦ëœ íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
            chunk_mode: ì²­í¬ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ í™œì„±í™” ì—¬ë¶€
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            ë³µì‚¬ ê²°ê³¼
        """
        try:
            start_time = time.time()
            
            # 1ë‹¨ê³„: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            source_schema = self.db_ops.get_table_schema(source_table)
            
            # 2ë‹¨ê³„: ì‚¬ìš©ì ì •ì˜ SQL ìƒì„±
            count_sql, select_sql = self._build_custom_sql_queries(
                source_table, source_schema, incremental_field,
                incremental_field_type, custom_where
            )
            
            # 3ë‹¨ê³„: ë°ì´í„° ê°œìˆ˜ í™•ì¸
            row_count = self._get_source_row_count(count_sql)
            
            if row_count == 0:
                logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì— ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": time.time() - start_time,
                    "message": "ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŒ"
                }
            
            # 4ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¡œ ì§ì ‘ ë³µì‚¬
            imported_rows = self._stream_data_directly(
                select_sql, target_table, primary_keys, sync_mode, 
                verified_target_schema, source_schema
            )
            
            total_time = time.time() - start_time
            
            result = {
                "status": "success",
                "exported_rows": row_count,
                "imported_rows": imported_rows,
                "total_execution_time": total_time,
                "message": f"ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¡œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {imported_rows}í–‰"
            }
            
            logger.info(f"ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {result}")
            return result
            
        except Exception as e:
            total_time = time.time() - start_time
            error_msg = f"ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            
            result = {
                "status": "error",
                "exported_rows": 0,
                "imported_rows": 0,
                "total_execution_time": total_time,
                "error": error_msg
            }
            
            return result

    def _stream_data_directly(
        self,
        select_sql: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        verified_target_schema: dict[str, Any] | None,
        source_schema: dict[str, Any]
    ) -> int:
        """
        ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ì—ì„œ íƒ€ê²Ÿìœ¼ë¡œ ì§ì ‘ ë°ì´í„° ë³µì‚¬
        
        Args:
            select_sql: SELECT SQL
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ëª©ë¡
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            verified_target_schema: ê²€ì¦ëœ íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ
            source_schema: ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ

        Returns:
            ë³µì‚¬ëœ í–‰ ìˆ˜
        """
        try:
            # 1ë‹¨ê³„: ì„ì‹œ í…Œì´ë¸” ìƒì„± (UNLOGGED)
            temp_table = self.create_temp_table(target_table, verified_target_schema or source_schema)
            
            # 2ë‹¨ê³„: psql íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë³µì‚¬
            imported_rows = self._copy_with_psql_pipe(select_sql, temp_table, source_schema)
            
            if imported_rows == 0:
                logger.warning("ìŠ¤íŠ¸ë¦¬ë° ë³µì‚¬ ì‹¤íŒ¨")
                return 0
            
            # 3ë‹¨ê³„: MERGE ì‘ì—… ìˆ˜í–‰
            merge_result = self.execute_merge_operation(
                temp_table, target_table, primary_keys, sync_mode
            )
            
            if merge_result["status"] != "success":
                logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {merge_result['message']}")
                # ì‹¤íŒ¨ ì‹œ ìŠ¤í…Œì´ì§• í…Œì´ë¸” ì •ë¦¬ ì‹œë„
                try:
                    with self.target_hook.get_conn() as conn:
                        with conn.cursor() as cursor:
                            cursor.execute(f"DROP TABLE IF EXISTS {temp_table}")
                            conn.commit()
                except Exception:
                    pass
                return 0
            
            # 4ë‹¨ê³„: ì„±ëŠ¥ ìµœì í™”
            try:
                self._analyze_target_table(target_table)
                self._check_and_create_indexes(target_table, primary_keys)
            except Exception as e:
                logger.warning(f"ì„±ëŠ¥ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            return imported_rows
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}")
            return 0

    def _copy_with_psql_pipe(
        self,
        select_sql: str,
        temp_table: str,
        source_schema: dict[str, Any]
    ) -> int:
        """
        psql íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë³µì‚¬
        
        Args:
            select_sql: SELECT SQL
            temp_table: ì„ì‹œ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ

        Returns:
            ë³µì‚¬ëœ í–‰ ìˆ˜
        """
        try:
            import subprocess
            import os
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            source_env = os.environ.copy()
            target_env = os.environ.copy()
            
            # ì†ŒìŠ¤ DB ì—°ê²° ì •ë³´ ì„¤ì •
            source_conn = self.source_hook.get_connection(self.db_ops.source_conn_id)
            source_env.update({
                'PGPASSWORD': source_conn.password,
                'PGHOST': source_conn.host,
                'PGPORT': str(source_conn.port),
                'PGUSER': source_conn.login,
                'PGDATABASE': source_conn.schema
            })
            
            # íƒ€ê²Ÿ DB ì—°ê²° ì •ë³´ ì„¤ì •
            target_conn = self.target_hook.get_connection(self.db_ops.target_conn_id)
            target_env.update({
                'PGPASSWORD': target_conn.password,
                'PGHOST': target_conn.host,
                'PGPORT': str(target_conn.port),
                'PGUSER': target_conn.login,
                'PGDATABASE': target_conn.schema
            })
            
            # ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ (STDOUTìœ¼ë¡œ)
            source_cmd = [
                'psql',
                '-h', source_conn.host,
                '-p', str(source_conn.port),
                '-U', source_conn.login,
                '-d', source_conn.schema,
                '-c', f"\\copy ({select_sql}) TO STDOUT WITH CSV HEADER"
            ]
            
            # íƒ€ê²Ÿìœ¼ë¡œ ë°ì´í„° ì ì¬ (STDINì—ì„œ)
            target_cmd = [
                'psql',
                '-h', target_conn.host,
                '-p', str(target_conn.port),
                '-U', target_conn.login,
                '-d', target_conn.schema,
                '-c', f"\\copy {temp_table} FROM STDIN WITH CSV HEADER"
            ]
            
            logger.info(f"ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ ì‹œì‘: {temp_table}")
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°, ëŒ€ìš©ëŸ‰ ë²„í¼ë§ íšŒí”¼)
            source_process = subprocess.Popen(
                source_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=source_env
            )

            target_process = subprocess.Popen(
                target_cmd,
                stdin=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=target_env
            )

            # ì†ŒìŠ¤ stdoutì„ íƒ€ê²Ÿ stdinìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì „ë‹¬
            assert source_process.stdout is not None
            assert target_process.stdin is not None
            
            try:
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
                for chunk in iter(lambda: source_process.stdout.read(1024 * 1024), b""):
                    if chunk:  # ë¹ˆ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                        target_process.stdin.write(chunk)
                        target_process.stdin.flush()  # ì¦‰ì‹œ í”ŒëŸ¬ì‹œ
                
                # ì†ŒìŠ¤ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
                source_process.wait()
                
                # íƒ€ê²Ÿ stdin ë‹«ê¸° (ë°ì´í„° ì „ì†¡ ì™„ë£Œ í›„)
                target_process.stdin.close()
                
                # íƒ€ê²Ÿ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
                target_process.wait()
                
            except Exception as stream_error:
                logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {stream_error}")
                # í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
                if source_process.poll() is None:
                    source_process.terminate()
                if target_process.poll() is None:
                    target_process.terminate()
                raise stream_error

            # ì—ëŸ¬ ìˆ˜ì§‘
            source_stderr = source_process.stderr.read() if source_process.stderr else b""
            target_stderr = target_process.stderr.read() if target_process.stderr else b""
            
            # ê²°ê³¼ í™•ì¸
            if source_process.returncode != 0:
                logger.error(f"ì†ŒìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨: {source_stderr.decode()}")
                return 0
                
            if target_process.returncode != 0:
                logger.error(f"íƒ€ê²Ÿ ì ì¬ ì‹¤íŒ¨: {target_stderr.decode()}")
                return 0
            
            # ë³µì‚¬ëœ í–‰ ìˆ˜ í™•ì¸
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"SELECT COUNT(*) FROM {temp_table}")
                    row_count = cursor.fetchone()[0]
            
            logger.info(f"ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ ì™„ë£Œ: {temp_table}, {row_count}í–‰")
            return row_count
            
        except Exception as e:
            logger.error(f"psql íŒŒì´í”„ë¼ì¸ ë³µì‚¬ ì‹¤íŒ¨: {e}")
            return 0

    def copy_data_with_binary_format(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        custom_where: str | None = None,
        batch_size: int = 10000,
        verified_target_schema: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """
        ë°”ì´ë„ˆë¦¬ í¬ë§·ì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ (CSV ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ)
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ëª©ë¡
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            custom_where: ì‚¬ìš©ì ì •ì˜ WHERE ì¡°ê±´
            batch_size: ë°°ì¹˜ í¬ê¸°
            verified_target_schema: ê²€ì¦ëœ íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ

        Returns:
            ë³µì‚¬ ê²°ê³¼
        """
        try:
            start_time = time.time()
            
            # 1ë‹¨ê³„: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            source_schema = self.db_ops.get_table_schema(source_table)
            
            # 2ë‹¨ê³„: ì‚¬ìš©ì ì •ì˜ SQL ìƒì„±
            count_sql, select_sql = self._build_custom_sql_queries(
                source_table, source_schema, incremental_field,
                incremental_field_type, custom_where
            )
            
            # 3ë‹¨ê³„: ë°ì´í„° ê°œìˆ˜ í™•ì¸
            row_count = self._get_source_row_count(count_sql)
            
            if row_count == 0:
                logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì— ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": time.time() - start_time,
                    "message": "ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŒ"
                }
            
            # 4ë‹¨ê³„: ë°”ì´ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ì§ì ‘ ë³µì‚¬
            imported_rows = self._copy_with_binary_format(
                select_sql, target_table, primary_keys, sync_mode, 
                verified_target_schema, source_schema
            )
            
            total_time = time.time() - start_time
            
            result = {
                "status": "success",
                "exported_rows": row_count,
                "imported_rows": imported_rows,
                "total_execution_time": total_time,
                "message": f"ë°”ì´ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {imported_rows}í–‰"
            }
            
            logger.info(f"ë°”ì´ë„ˆë¦¬ í¬ë§·ì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {result}")
            return result
            
        except Exception as e:
            total_time = time.time() - start_time
            error_msg = f"ë°”ì´ë„ˆë¦¬ í¬ë§·ì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            
            result = {
                "status": "error",
                "exported_rows": 0,
                "imported_rows": 0,
                "total_execution_time": total_time,
                "error": error_msg
            }
            
            return result

    def _copy_with_binary_format(
        self,
        select_sql: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        verified_target_schema: dict[str, Any] | None,
        source_schema: dict[str, Any]
    ) -> int:
        """
        ë°”ì´ë„ˆë¦¬ í¬ë§·ì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ì—ì„œ íƒ€ê²Ÿìœ¼ë¡œ ì§ì ‘ ë°ì´í„° ë³µì‚¬
        
        Args:
            select_sql: SELECT SQL
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ëª©ë¡
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            verified_target_schema: ê²€ì¦ëœ íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ
            source_schema: ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ

        Returns:
            ë³µì‚¬ëœ í–‰ ìˆ˜
        """
        try:
            # 1ë‹¨ê³„: ì„ì‹œ í…Œì´ë¸” ìƒì„± (UNLOGGED)
            temp_table = self.create_temp_table(target_table, verified_target_schema or source_schema)
            
            # 2ë‹¨ê³„: psql ë°”ì´ë„ˆë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë³µì‚¬
            imported_rows = self._copy_with_psql_binary_pipe(select_sql, temp_table, source_schema)
            
            if imported_rows == 0:
                logger.warning("ë°”ì´ë„ˆë¦¬ ë³µì‚¬ ì‹¤íŒ¨")
                return 0
            
            # 3ë‹¨ê³„: MERGE ì‘ì—… ìˆ˜í–‰
            merge_result = self.execute_merge_operation(
                temp_table, target_table, primary_keys, sync_mode
            )
            
            if merge_result["status"] != "success":
                logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {merge_result['message']}")
                return 0
            
            # 4ë‹¨ê³„: ì„±ëŠ¥ ìµœì í™”
            try:
                self._analyze_target_table(target_table)
                self._check_and_create_indexes(target_table, primary_keys)
            except Exception as e:
                logger.warning(f"ì„±ëŠ¥ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            return imported_rows
            
        except Exception as e:
            logger.error(f"ë°”ì´ë„ˆë¦¬ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}")
            return 0

    def _copy_with_psql_binary_pipe(
        self,
        select_sql: str,
        temp_table: str,
        source_schema: dict[str, Any]
    ) -> int:
        """
        psql ë°”ì´ë„ˆë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë³µì‚¬
        
        Args:
            select_sql: SELECT SQL
            temp_table: ì„ì‹œ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ

        Returns:
            ë³µì‚¬ëœ í–‰ ìˆ˜
        """
        try:
            import subprocess
            import os
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            source_env = os.environ.copy()
            target_env = os.environ.copy()
            
            # ì†ŒìŠ¤ DB ì—°ê²° ì •ë³´ ì„¤ì •
            source_conn = self.source_hook.get_connection(self.db_ops.source_conn_id)
            source_env.update({
                'PGPASSWORD': source_conn.password,
                'PGHOST': source_conn.host,
                'PGPORT': str(source_conn.port),
                'PGUSER': source_conn.login,
                'PGDATABASE': source_conn.schema
            })
            
            # íƒ€ê²Ÿ DB ì—°ê²° ì •ë³´ ì„¤ì •
            target_conn = self.target_hook.get_connection(self.db_ops.target_conn_id)
            target_env.update({
                'PGPASSWORD': target_conn.password,
                'PGHOST': target_conn.host,
                'PGPORT': str(target_conn.port),
                'PGUSER': target_conn.login,
                'PGDATABASE': target_conn.schema
            })
            
            # ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ (ë°”ì´ë„ˆë¦¬ í¬ë§·, STDOUTìœ¼ë¡œ)
            source_cmd = [
                'psql',
                '-h', source_conn.host,
                '-p', str(source_conn.port),
                '-U', source_conn.login,
                '-d', source_conn.schema,
                '-c', f"\\copy ({select_sql}) TO STDOUT WITH (FORMAT binary)"
            ]
            
            # íƒ€ê²Ÿìœ¼ë¡œ ë°ì´í„° ì ì¬ (ë°”ì´ë„ˆë¦¬ í¬ë§·, STDINì—ì„œ)
            target_cmd = [
                'psql',
                '-h', target_conn.host,
                '-p', str(target_conn.port),
                '-U', target_conn.login,
                '-d', target_conn.schema,
                '-c', f"\\copy {temp_table} FROM STDIN WITH (FORMAT binary)"
            ]
            
            logger.info(f"ë°”ì´ë„ˆë¦¬ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ ì‹œì‘: {temp_table}")
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            source_process = subprocess.Popen(
                source_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=source_env
            )
            
            target_process = subprocess.Popen(
                target_cmd,
                stdin=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=target_env
            )
            
            # ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° (ëŒ€ìš©ëŸ‰ ë²„í¼ë§ íšŒí”¼)
            assert source_process.stdout is not None
            assert target_process.stdin is not None
            
            try:
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
                for chunk in iter(lambda: source_process.stdout.read(1024 * 1024), b""):
                    if chunk:  # ë¹ˆ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                        target_process.stdin.write(chunk)
                        target_process.stdin.flush()  # ì¦‰ì‹œ í”ŒëŸ¬ì‹œ
                
                # ì†ŒìŠ¤ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
                source_process.wait()
                
                # íƒ€ê²Ÿ stdin ë‹«ê¸° (ë°ì´í„° ì „ì†¡ ì™„ë£Œ í›„)
                target_process.stdin.close()
                
                # íƒ€ê²Ÿ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
                target_process.wait()
                
            except Exception as stream_error:
                logger.error(f"ë°”ì´ë„ˆë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {stream_error}")
                # í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
                if source_process.poll() is None:
                    source_process.terminate()
                if target_process.poll() is None:
                    target_process.terminate()
                raise stream_error

            # ì—ëŸ¬ ìˆ˜ì§‘
            source_stderr = source_process.stderr.read() if source_process.stderr else b""
            target_stderr = target_process.stderr.read() if target_process.stderr else b""
            
            # ê²°ê³¼ í™•ì¸
            if source_process.returncode != 0:
                logger.error(f"ì†ŒìŠ¤ ë°”ì´ë„ˆë¦¬ ì¶”ì¶œ ì‹¤íŒ¨: {source_stderr.decode()}")
                return 0
                
            if target_process.returncode != 0:
                logger.error(f"íƒ€ê²Ÿ ë°”ì´ë„ˆë¦¬ ì ì¬ ì‹¤íŒ¨: {target_stderr.decode()}")
                return 0
            
            # ë³µì‚¬ëœ í–‰ ìˆ˜ í™•ì¸
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"SELECT COUNT(*) FROM {temp_table}")
                    row_count = cursor.fetchone()[0]
            
            logger.info(f"ë°”ì´ë„ˆë¦¬ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ ì™„ë£Œ: {temp_table}, {row_count}í–‰")
            return row_count
            
        except Exception as e:
            logger.error(f"psql ë°”ì´ë„ˆë¦¬ íŒŒì´í”„ë¼ì¸ ë³µì‚¬ ì‹¤íŒ¨: {e}")
            return 0

    def _export_to_csv_chunked(
        self, 
        table_name: str, 
        csv_path: str, 
        where_clause: str | None, 
        batch_size: int, 
        order_by_field: str | None, 
        enable_checkpoint: bool = True, 
        max_retries: int = 3
    ) -> int:
        """
        ì„¸ì…˜ ê´€ë¦¬ì™€ íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ì  ì—†ìŒ)
        """
        try:
            # ì „ì²´ í–‰ ìˆ˜ ì¡°íšŒ
            total_count = self.db_ops.get_table_row_count(table_name, where_clause=where_clause)
            
            if total_count == 0:
                logger.warning(f"í…Œì´ë¸” {table_name}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬ ì‹œë„
            start_offset, total_exported = 0, 0
            if enable_checkpoint:
                start_offset, total_exported = self._resume_from_checkpoint(table_name, csv_path)
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜
            session_refresh_interval = 200  # 200ê°œ ì²­í¬ë§ˆë‹¤ ì„¸ì…˜ ê°±ì‹  (ë¹ˆë„ ê°ì†Œ)
            chunk_count = 0
            current_batch_size = batch_size
            performance_metrics = {
                'start_time': time.time(),
                'chunk_processing_times': [],
                'memory_usage_history': [],
                'session_refresh_count': 0
            }
            
            # í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            columns = self._get_table_columns(table_name)
            
            with open(csv_path, 'a' if start_offset > 0 else 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                
                # í—¤ë”ëŠ” ì²˜ìŒì—ë§Œ ì“°ê¸°
                if start_offset == 0:
                    writer.writerow(columns)
                
                offset = start_offset
                
                while offset < total_count:
                    chunk_start_time = time.time()
                    try:
                        # ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ê°±ì‹  (ë” íš¨ìœ¨ì ìœ¼ë¡œ)
                        if chunk_count % session_refresh_interval == 0:
                            # ì„¸ì…˜ ìƒíƒœê°€ ì‹¤ì œë¡œ ë¶ˆëŸ‰í•œ ê²½ìš°ì—ë§Œ ê°±ì‹ 
                            if not self._check_session_health():
                                self._refresh_database_session()
                                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ê°±ì‹  (ì²­í¬ {chunk_count})")
                                performance_metrics['session_refresh_count'] += 1
                            else:
                                logger.debug(f"ì„¸ì…˜ ìƒíƒœ ì •ìƒ, ê°±ì‹  ìƒëµ (ì²­í¬ {chunk_count})")
                        
                        # íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬
                        batch_result = self._process_single_chunk_with_transaction(
                            table_name, where_clause, current_batch_size, offset, order_by_field, writer, max_retries
                        )
                        
                        if batch_result['success']:
                            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                            chunk_time = time.time() - chunk_start_time
                            performance_metrics['chunk_processing_times'].append(chunk_time)
                            
                            total_exported += batch_result['rows_processed']
                            offset += current_batch_size
                            chunk_count += 1
                            
                            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                            if enable_checkpoint:
                                self._save_checkpoint(table_name, offset, total_exported, csv_path)
                            
                            # ì§„í–‰ë¥  ë¡œê¹…
                            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ : {total_exported}/{total_count}")
                            
                            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                            memory_usage = self._check_memory_usage()
                            performance_metrics['memory_usage_history'].append(memory_usage)
                            
                            # ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
                            if chunk_count % 50 == 0:  # 50ê°œ ì²­í¬ë§ˆë‹¤ ì¡°ì • (ë¹ˆë„ ê°ì†Œ)
                                new_batch_size = self._optimize_batch_size_dynamically(
                                    current_batch_size, memory_usage, chunk_time, self._check_session_health()
                                )
                                if new_batch_size != current_batch_size:
                                    logger.info(f"ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •: {current_batch_size} â†’ {new_batch_size}")
                                    current_batch_size = new_batch_size
                            
                            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¡œê¹…
                            if chunk_count % 500 == 0:  # 500ê°œ ì²­í¬ë§ˆë‹¤ ë¡œê¹… (ë¹ˆë„ ê°ì†Œ)
                                self._log_performance_metrics(performance_metrics, chunk_count, total_exported, total_count)
                        else:
                                                        # ì²­í¬ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬
                            error_action = self._handle_chunk_error(batch_result['error'], offset, current_batch_size)
                            if error_action == 'skip':
                                offset += current_batch_size
                                logger.warning(f"ì²­í¬ ê±´ë„ˆë›°ê¸° (offset {offset})")
                            elif error_action == 'retry':
                                continue  # ì¬ì‹œë„
                            else:
                                raise Exception(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {batch_result['error']}")
                        
                    except (psycopg2.OperationalError, psycopg2.InterfaceError) as db_error:
                        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ ì‹œ ì„¸ì…˜ ì¬ìƒì„±
                        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {db_error}")
                        self._refresh_database_session()
                        
                        # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„
                        continue
                        
                    except Exception as chunk_error:
                        # ê¸°íƒ€ ì˜¤ë¥˜ ì‹œ ë¡œê¹… ë° ì¬ì‹œë„
                        logger.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜ (offset {offset}): {chunk_error}")
                        error_action = self._handle_chunk_error(chunk_error, offset, current_batch_size)
                        if error_action == 'skip':
                            offset += current_batch_size
                        elif error_action == 'retry':
                                continue
                        else:
                            raise chunk_error
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
            if enable_checkpoint:
                self._cleanup_checkpoint(csv_path)
            
            # ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸
            total_time = time.time() - performance_metrics['start_time']
            self._log_final_performance_report(performance_metrics, total_exported, total_time)
            
            logger.info(f"ì²­í¬ ë°©ì‹ CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {table_name} -> {csv_path}, ì´ {total_exported}í–‰")
            return total_exported
            
        except Exception as e:
            logger.error(f"ì²­í¬ ë°©ì‹ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _export_to_csv_legacy(
        self, 
        table_name: str, 
        csv_path: str, 
        where_clause: str | None, 
        batch_size: int, 
        order_by_field: str | None
    ) -> int:
        """
        ê¸°ì¡´ ë°©ì‹ì˜ CSV ë‚´ë³´ë‚´ê¸° (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì‹)
        """
        try:
            # ì „ì²´ í–‰ ìˆ˜ ì¡°íšŒ
            total_count = self.db_ops.get_table_row_count(
                table_name, where_clause=where_clause
            )

            if total_count == 0:
                logger.warning(f"í…Œì´ë¸” {table_name}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in table_name:
                schema, table = table_name.split(".", 1)
            else:
                schema = "public"
                table = table_name

            # ë¨¼ì € í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            try:
                schema_query = """
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_schema = %s AND table_name = %s
                    ORDER BY ordinal_position
                """
                schema_columns = self.source_hook.get_records(
                    schema_query, parameters=(schema, table)
                )
                if schema_columns:
                    columns = [col[0] for col in schema_columns]
                    logger.info(f"ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {columns}")
                else:
                    raise Exception("ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                raise

            # ORDER BY ì ˆ êµ¬ì„±
            if order_by_field:
                order_by_clause = f"ORDER BY {order_by_field}"
                logger.info(f"ì •ë ¬ ê¸°ì¤€: {order_by_field}")
            else:
                order_by_clause = "ORDER BY 1"
                logger.info("ê¸°ë³¸ ì •ë ¬ ì‚¬ìš©")

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì¶”ì¶œ
            offset = 0
            all_data = []

            while offset < total_count:
                if where_clause:
                    query = f"""
                        SELECT * FROM {table_name}
                        WHERE {where_clause}
                        {order_by_clause}
                        LIMIT {batch_size} OFFSET {offset}
                    """
                else:
                    query = f"""
                        SELECT * FROM {table_name}
                        {order_by_clause}
                        LIMIT {batch_size} OFFSET {offset}
                    """

                batch_data = self.source_hook.get_records(query)
                all_data.extend(batch_data)
                offset += batch_size

                logger.info(
                    f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {min(offset, total_count)}/{total_count}"
                )

            # DataFrameìœ¼ë¡œ ë³€í™˜
            if all_data:
                # ì´ë¯¸ ê°€ì ¸ì˜¨ ì»¬ëŸ¼ëª… ì‚¬ìš©
                df = pd.DataFrame(all_data, columns=columns)

                # ğŸš¨ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ - CSV ì €ì¥ ì „ ì²˜ë¦¬
                logger.info("=== ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘ ===")
                df = self._validate_and_convert_data_types(df, table_name)
                logger.info("=== ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì™„ë£Œ ===")

                # CSVë¡œ ì €ì¥
                df.to_csv(csv_path, index=False, encoding="utf-8")

                logger.info(
                    f"CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {table_name} -> {csv_path}, í–‰ ìˆ˜: {len(df)}"
                )
                return len(df)
            else:
                logger.warning(f"í…Œì´ë¸” {table_name}ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return 0

        except Exception as e:
            logger.error(f"CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {table_name} -> {csv_path}, ì˜¤ë¥˜: {e!s}")
            raise

    def _get_table_columns(self, table_name: str) -> list[str]:
        """
        í…Œì´ë¸”ì˜ ì»¬ëŸ¼ëª… ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in table_name:
                schema, table = table_name.split(".", 1)
            else:
                schema = "public"
                table = table_name

            schema_query = """
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = %s AND table_name = %s
                ORDER BY ordinal_position
            """
            schema_columns = self.source_hook.get_records(
                schema_query, parameters=(schema, table)
            )
            
            if schema_columns:
                columns = [col[0] for col in schema_columns]
                logger.info(f"í…Œì´ë¸” {table_name}ì˜ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {columns}")
                return columns
            else:
                raise Exception(f"í…Œì´ë¸” {table_name}ì—ì„œ ì»¬ëŸ¼ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"í…Œì´ë¸” ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _refresh_database_session(self):
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ê°±ì‹  (ê°œì„ ëœ ë²„ì „)
        """
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ê°±ì‹  ì‹œì‘")
            
            # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ í™•ì¸
            if hasattr(self, 'db_ops') and hasattr(self.db_ops, 'connection'):
                # ì—°ê²° ìƒíƒœ í™•ì¸
                if self.db_ops.connection.closed:
                    logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ëŠì–´ì§, ì¬ì—°ê²° ì‹œë„")
                    self.db_ops.reconnect()
                    logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì¬ì—°ê²° ì™„ë£Œ")
                else:
                    # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ì„¸ì…˜ ìƒíƒœ í™•ì¸
                    try:
                        self.db_ops.execute_query("SELECT 1")
                        logger.debug("ì„¸ì…˜ ìƒíƒœ ì •ìƒ")
                    except Exception as query_error:
                        logger.warning(f"ì„¸ì…˜ ìƒíƒœ í™•ì¸ ì¿¼ë¦¬ ì‹¤íŒ¨: {query_error}")
                        # ì¿¼ë¦¬ ì‹¤íŒ¨ ì‹œ ì¬ì—°ê²° ì‹œë„
                        logger.info("ì„¸ì…˜ ì¬ì—°ê²° ì‹œë„")
                        self.db_ops.reconnect()
            else:
                logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            # ê°•ì œ ì¬ì—°ê²°
            try:
                logger.info("ê°•ì œ ì¬ì—°ê²° ì‹œë„")
                self.db_ops.reconnect()
                logger.info("ê°•ì œ ì¬ì—°ê²° ì™„ë£Œ")
            except Exception as reconnect_error:
                logger.error(f"ì¬ì—°ê²° ì‹¤íŒ¨: {reconnect_error}")
                # ì¬ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¼ì • ì‹œê°„ ëŒ€ê¸° í›„ ì¬ì‹œë„
                logger.info("5ì´ˆ í›„ ì¬ì—°ê²° ì¬ì‹œë„")
                time.sleep(5)
                try:
                    self.db_ops.reconnect()
                    logger.info("ì§€ì—° ì¬ì—°ê²° ì„±ê³µ")
                except Exception as final_error:
                    logger.error(f"ìµœì¢… ì¬ì—°ê²° ì‹¤íŒ¨: {final_error}")
                    raise Exception(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë³µêµ¬ ì‹¤íŒ¨: {final_error}")

    def _check_session_health(self) -> bool:
        """
        ì„¸ì…˜ ìƒíƒœ ì¢…í•© ì ê²€ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            # ì—°ê²° ìƒíƒœ í™•ì¸
            if hasattr(self, 'db_ops') and hasattr(self.db_ops, 'connection'):
                if self.db_ops.connection.closed:
                    logger.debug("ì„¸ì…˜ ìƒíƒœ: ì—°ê²°ì´ ëŠì–´ì§")
                    return False
                
                # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ì„¸ì…˜ ìƒíƒœ í™•ì¸ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                try:
                    # ë” ë¹ ë¥¸ ì¿¼ë¦¬ë¡œ ì„¸ì…˜ ìƒíƒœ í™•ì¸
                    self.db_ops.execute_query("SELECT 1", timeout=5)
                    logger.debug("ì„¸ì…˜ ìƒíƒœ: ì •ìƒ")
                    return True
                except Exception as query_error:
                    # íŠ¹ì • ì—ëŸ¬ëŠ” ì„¸ì…˜ ìƒíƒœì™€ ë¬´ê´€í•  ìˆ˜ ìˆìŒ
                    if "timeout" in str(query_error).lower() or "connection" in str(query_error).lower():
                        logger.debug(f"ì„¸ì…˜ ìƒíƒœ: ì—°ê²° ë¬¸ì œ - {query_error}")
                        return False
                    else:
                        # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì„¸ì…˜ ìƒíƒœì™€ ë¬´ê´€í•  ìˆ˜ ìˆìŒ
                        logger.debug(f"ì„¸ì…˜ ìƒíƒœ: ì¿¼ë¦¬ ì—ëŸ¬ (ì„¸ì…˜ ìƒíƒœì™€ ë¬´ê´€) - {query_error}")
                        return True  # ì„¸ì…˜ì€ ì •ìƒìœ¼ë¡œ ê°„ì£¼
            else:
                logger.debug("ì„¸ì…˜ ìƒíƒœ: ì—°ê²° ê°ì²´ ì—†ìŒ")
                return False
                
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ìƒíƒœ ì ê²€ ì‹¤íŒ¨: {e}")
            return False

    def _check_memory_usage(self) -> float:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬
        """
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            # ìƒìˆ˜ë¡œ ì •ì˜ëœ ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì‚¬ìš©
            if memory_mb > self.MemoryThresholds.EMERGENCY_MB:
                logger.critical(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸´ê¸‰ ìƒí™©: {memory_mb:.1f}MB (ì„ê³„ê°’: {self.MemoryThresholds.EMERGENCY_MB}MB)")
                self._force_memory_cleanup()
            elif memory_mb > self.MemoryThresholds.CRITICAL_MB:
                logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìœ„í—˜: {memory_mb:.1f}MB (ì„ê³„ê°’: {self.MemoryThresholds.CRITICAL_MB}MB)")
                self._force_memory_cleanup()
            elif memory_mb > self.MemoryThresholds.WARNING_MB:
                logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB (ì„ê³„ê°’: {self.MemoryThresholds.WARNING_MB}MB)")
                gc.collect()
            
            return memory_mb
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ ì‹¤íŒ¨: {e}")
            return 0.0

    @contextmanager
    def _create_chunk_context(self, table_name: str, csv_path: str):
        """
        ì²­í¬ ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
        
        Args:
            table_name: ì²˜ë¦¬í•  í…Œì´ë¸”ëª…
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
        """
        context = {
            "start_time": time.time(),
            "table_name": table_name,
            "csv_path": csv_path,
            "chunks_processed": 0,
            "total_rows": 0,
            "memory_usage_history": [],
            "performance_metrics": {}
        }
        
        try:
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì‹œì‘: {table_name}")
            yield context
        except Exception as e:
            logger.error(f"ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {table_name}, ì˜¤ë¥˜: {e}")
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë¡œê·¸ì— ê¸°ë¡
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ìƒíƒœ: {context}")
            raise
        finally:
            # ì²˜ë¦¬ ì™„ë£Œ í›„ ì •ë¦¬ ì‘ì—…
            execution_time = time.time() - context["start_time"]
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ: {table_name}, "
                       f"ì²˜ë¦¬ëœ ì²­í¬: {context['chunks_processed']}, "
                       f"ì´ í–‰ ìˆ˜: {context['total_rows']}, "
                       f"ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if context["memory_usage_history"]:
                avg_memory = sum(context["memory_usage_history"]) / len(context["memory_usage_history"])
                logger.info(f"í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {avg_memory:.1f}MB")

    def _force_memory_cleanup(self):
        """
        ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            logger.info("ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")
            
            # 1ë‹¨ê³„: ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            collected = gc.collect()
            logger.info(f"ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì™„ë£Œ: {collected}ê°œ ê°ì²´ ì •ë¦¬")
            
            # 2ë‹¨ê³„: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            process = psutil.Process()
            before_memory = process.memory_info().rss / 1024 / 1024
            
            # 3ë‹¨ê³„: Python ê°ì²´ ì°¸ì¡° ì •ë¦¬ (ì•ˆì „í•˜ê²Œ)
            cleared_count = 0
            for obj in gc.get_objects():
                try:
                    if hasattr(obj, '__dict__') and obj.__dict__:
                        # ì¤‘ìš”í•œ ì‹œìŠ¤í…œ ê°ì²´ëŠ” ê±´ë„ˆë›°ê¸°
                        if not obj.__class__.__name__.startswith('_'):
                            obj.__dict__.clear()
                            cleared_count += 1
                except Exception:
                    continue
            
            logger.info(f"ê°ì²´ ì°¸ì¡° ì •ë¦¬ ì™„ë£Œ: {cleared_count}ê°œ ê°ì²´")
            
            # 4ë‹¨ê³„: ì¶”ê°€ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # 5ë‹¨ê³„: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬í™•ì¸
            after_memory = process.memory_info().rss / 1024 / 1024
            memory_reduced = before_memory - after_memory
            
            logger.info(f"ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {before_memory:.1f}MB â†’ {after_memory:.1f}MB (ê°ì†Œ: {memory_reduced:.1f}MB)")
            
        except Exception as e:
            logger.error(f"ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì€ ìˆ˜í–‰
            try:
                gc.collect()
                logger.info("ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰")
            except Exception:
                pass

    def _process_single_chunk_with_transaction(
        self, 
        table_name: str, 
        where_clause: str | None, 
        batch_size: int, 
        offset: int, 
        order_by_field: str | None, 
        writer: csv.writer, 
        max_retries: int = 3
    ) -> dict:
        """
        íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
        """
        retry_count = 0
        start_time = time.time()
        
        while retry_count < max_retries:
            try:
                # íŠ¸ëœì­ì…˜ ì‹œì‘
                with self.source_hook.get_conn() as conn:
                    # íŠ¸ëœì­ì…˜ ì„¤ì •
                    conn.autocommit = False
                    
                    with conn.cursor() as cursor:
                        # ì²­í¬ ë°ì´í„° ì¡°íšŒ
                        query = self._build_chunk_query(table_name, where_clause, batch_size, offset, order_by_field)
                        
                        # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
                        query_start = time.time()
                        cursor.execute(query)
                        query_time = time.time() - query_start
                        
                        # ë°ì´í„° ì²˜ë¦¬ ë° CSV ì“°ê¸°
                        rows_processed = 0
                        csv_write_start = time.time()
                        
                        for row in cursor:
                            writer.writerow(row)
                            rows_processed += 1
                        
                        csv_write_time = time.time() - csv_write_start
                        
                        # íŠ¸ëœì­ì…˜ ì»¤ë°‹
                        commit_start = time.time()
                        conn.commit()
                        commit_time = time.time() - commit_start
                        
                        # ì„±ëŠ¥ ë¡œê¹…
                        total_time = time.time() - start_time
                        logger.debug(f"ì²­í¬ ì²˜ë¦¬ ì„±ê³µ: {rows_processed}í–‰, ì¿¼ë¦¬: {query_time:.2f}ì´ˆ, CSV: {csv_write_time:.2f}ì´ˆ, ì»¤ë°‹: {commit_time:.2f}ì´ˆ, ì´: {total_time:.2f}ì´ˆ")
                        
                        return {
                            'rows_processed': rows_processed,
                            'success': True,
                            'error': None,
                            'performance': {
                                'query_time': query_time,
                                'csv_write_time': csv_write_time,
                                'commit_time': commit_time,
                                'total_time': total_time
                            }
                        }
                        
            except Exception as e:
                # íŠ¸ëœì­ì…˜ ë¡¤ë°±
                try:
                    if 'conn' in locals():
                        conn.rollback()
                        logger.debug("íŠ¸ëœì­ì…˜ ë¡¤ë°± ì™„ë£Œ")
                except Exception as rollback_error:
                    logger.warning(f"íŠ¸ëœì­ì…˜ ë¡¤ë°± ì‹¤íŒ¨: {rollback_error}")
                
                retry_count += 1
                
                if retry_count >= max_retries:
                    logger.error(f"ì²­í¬ ì²˜ë¦¬ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}")
                    return {
                        'rows_processed': 0,
                        'success': False,
                        'error': str(e),
                        'retry_count': retry_count
                    }
                
                # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„° ì¶”ê°€
                backoff_time = min(2 ** retry_count + (time.time() % 1), 60)  # ìµœëŒ€ 60ì´ˆ
                logger.warning(f"ì²­í¬ ì²˜ë¦¬ ì¬ì‹œë„ {retry_count}/{max_retries}: {e}, {backoff_time:.1f}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(backoff_time)

    def _build_chunk_query(
        self, 
        table_name: str, 
        where_clause: str | None, 
        batch_size: int, 
        offset: int, 
        order_by_field: str | None
    ) -> str:
        """
        ì²­í¬ ì¿¼ë¦¬ êµ¬ì„±
        """
        # ORDER BY ì ˆ êµ¬ì„±
        if order_by_field:
            order_by_clause = f"ORDER BY {order_by_field}"
        else:
            order_by_clause = "ORDER BY 1"
        
        # WHERE ì ˆ êµ¬ì„±
        if where_clause:
            query = f"""
                SELECT * FROM {table_name}
                WHERE {where_clause}
                {order_by_clause}
                LIMIT {batch_size} OFFSET {offset}
            """
        else:
            query = f"""
                SELECT * FROM {table_name}
                {order_by_clause}
                LIMIT {batch_size} OFFSET {offset}
            """
        
        return query

    def _handle_chunk_error(self, error: str, offset: int, batch_size: int) -> str:
        """
        ì²­í¬ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            # ì˜¤ë¥˜ íƒ€ì… í™•ì¸
            if isinstance(error, str):
                # ë¬¸ìì—´ ì˜¤ë¥˜ì¸ ê²½ìš° íƒ€ì… ì¶”ë¡ 
                if "connection" in error.lower() or "timeout" in error.lower():
                    error_type = "connection_error"
                elif "data" in error.lower() or "integrity" in error.lower():
                    error_type = "data_error"
                elif "internal" in error.lower() or "server" in error.lower():
                    error_type = "internal_error"
                else:
                    error_type = "unknown_error"
            else:
                # ì‹¤ì œ ì˜ˆì™¸ ê°ì²´ì¸ ê²½ìš°
                error_type = type(error).__name__
            
            logger.info(f"ì²­í¬ ì˜¤ë¥˜ ë¶„ì„: íƒ€ì…={error_type}, offset={offset}, ì˜¤ë¥˜={error}")
            
            # ì˜¤ë¥˜ íƒ€ì…ë³„ ì²˜ë¦¬ ì „ëµ
            if error_type in ["OperationalError", "InterfaceError", "connection_error"]:
                # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ - ì¬ì‹œë„ ê°€ëŠ¥
                logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ì¬ì‹œë„ ì˜ˆì •")
                return 'retry'
                
            elif error_type in ["DataError", "IntegrityError", "data_error"]:
                # ë°ì´í„° ì˜¤ë¥˜ - ì¬ì‹œë„ ë¶ˆê°€ëŠ¥, ê±´ë„ˆë›°ê¸°
                logger.error(f"ë°ì´í„° ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ê±´ë„ˆë›°ê¸°")
                return 'skip'
                
            elif error_type in ["InternalError", "internal_error"]:
                # ë‚´ë¶€ ì˜¤ë¥˜ - ì¬ì‹œë„ ê°€ëŠ¥
                logger.warning(f"ë‚´ë¶€ ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ì¬ì‹œë„ ì˜ˆì •")
                return 'retry'
                
            elif error_type in ["TimeoutError", "timeout"]:
                # íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ - ì¬ì‹œë„ ê°€ëŠ¥
                logger.warning(f"íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ì¬ì‹œë„ ì˜ˆì •")
                return 'retry'
                
            elif error_type in ["MemoryError", "memory"]:
                # ë©”ëª¨ë¦¬ ì˜¤ë¥˜ - ê°•ì œ ì •ë¦¬ í›„ ì¬ì‹œë„
                logger.error(f"ë©”ëª¨ë¦¬ ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„")
                self._force_memory_cleanup()
                return 'retry'
                
            else:
                # ê¸°íƒ€ ì˜¤ë¥˜ - ë¡œê¹… í›„ ì¬ì‹œë„
                logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}): {error}")
                return 'retry'
                
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return 'retry'  # ì•ˆì „í•˜ê²Œ ì¬ì‹œë„

    def _save_checkpoint(self, table_name: str, offset: int, total_exported: int, csv_path: str):
        """
        ì²­í¬ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì €ì¥ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
            memory_usage = self._check_memory_usage()
            
            checkpoint_data = {
                'table_name': table_name,
                'offset': offset,
                'total_exported': total_exported,
                'csv_path': csv_path,
                'timestamp': pd.Timestamp.now().isoformat(),
                'status': 'in_progress',
                'checksum': self._calculate_csv_checksum(csv_path),
                'memory_usage_mb': memory_usage,
                'system_info': {
                    'python_version': f"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}",
                    'platform': os.sys.platform,
                    'process_id': os.getpid()
                }
            }
            
            checkpoint_file = f"{csv_path}.checkpoint"
            
            # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê¸° (ì›ìì„± ë³´ì¥)
            temp_checkpoint_file = f"{checkpoint_file}.tmp"
            with open(temp_checkpoint_file, 'w') as f:
                json.dump(checkpoint_data, f, indent=2)
            
            # ì›ìì  ì´ë™
            os.rename(temp_checkpoint_file, checkpoint_file)
            
            logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: offset {offset}, ì²˜ë¦¬ëœ í–‰ {total_exported}, ë©”ëª¨ë¦¬ {memory_usage:.1f}MB")
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
            logger.warning("ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨í–ˆì§€ë§Œ ì²˜ë¦¬ ê³„ì† ì§„í–‰")

    def _resume_from_checkpoint(self, table_name: str, csv_path: str) -> tuple[int, int]:
        """
        ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬ (ê°œì„ ëœ ë²„ì „)
        """
        checkpoint_file = f"{csv_path}.checkpoint"
        
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, 'r') as f:
                    checkpoint_data = json.load(f)
                
                if checkpoint_data['status'] == 'in_progress':
                    # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë¡œê¹…
                    logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_data['table_name']}, offset: {checkpoint_data['offset']}, ì²˜ë¦¬ëœ í–‰: {checkpoint_data['total_exported']}")
                    
                    # ì²´í¬ì„¬ ê²€ì¦
                    if self._verify_checkpoint_integrity(checkpoint_data, csv_path):
                        # ì²´í¬í¬ì¸íŠ¸ ìœ íš¨ì„± ì¶”ê°€ ê²€ì¦
                        if self._validate_checkpoint_data(checkpoint_data):
                            logger.info(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬: {checkpoint_data['offset']}ë¶€í„° ì‹œì‘")
                            
                            # ì²´í¬í¬ì¸íŠ¸ ë°±ì—… ìƒì„±
                            self._backup_checkpoint(checkpoint_file)
                            
                            return checkpoint_data['offset'], checkpoint_data['total_exported']
                        else:
                            logger.warning("ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨, ì²˜ìŒë¶€í„° ì‹œì‘")
                            return 0, 0
                    else:
                        logger.warning("ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨, ì²˜ìŒë¶€í„° ì‹œì‘")
                        return 0, 0
                else:
                    logger.info(f"ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ: {checkpoint_data['status']}")
                    if checkpoint_data['status'] == 'completed':
                        logger.info("ì´ë¯¸ ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤.")
                    return 0, 0
                    
            except Exception as e:
                logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì½ê¸° ì‹¤íŒ¨: {e}")
                # ì†ìƒëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë°±ì—…
                try:
                    corrupted_file = f"{checkpoint_file}.corrupted"
                    os.rename(checkpoint_file, corrupted_file)
                    logger.info(f"ì†ìƒëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ {corrupted_file}ë¡œ ë°±ì—…")
                except Exception:
                    pass
                return 0, 0
        
        return 0, 0

    def _verify_checkpoint_integrity(self, checkpoint_data: dict, csv_path: str) -> bool:
        """
        ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            if not os.path.exists(csv_path):
                logger.warning("CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(csv_path)
            if file_size == 0:
                logger.warning("CSV íŒŒì¼ í¬ê¸°ê°€ 0")
                return False
            
            # ì²´í¬ì„¬ ê²€ì¦
            current_checksum = self._calculate_csv_checksum(csv_path)
            if current_checksum != checkpoint_data.get('checksum'):
                logger.warning(f"ì²´í¬ì„¬ ë¶ˆì¼ì¹˜: ì˜ˆìƒ={checkpoint_data.get('checksum')}, ì‹¤ì œ={current_checksum}")
                return False
            
            # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸ (ì„ íƒì )
            file_mtime = os.path.getmtime(csv_path)
            checkpoint_time = pd.Timestamp(checkpoint_data['timestamp']).timestamp()
            
            # ì²´í¬í¬ì¸íŠ¸ ì‹œê°„ë³´ë‹¤ íŒŒì¼ì´ ìµœì‹ ì´ì–´ì•¼ í•¨
            if file_mtime < checkpoint_time:
                logger.warning("CSV íŒŒì¼ì´ ì²´í¬í¬ì¸íŠ¸ë³´ë‹¤ ì˜¤ë˜ë¨")
                return False
            
            logger.debug("ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
            return True
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def _validate_checkpoint_data(self, checkpoint_data: dict) -> bool:
        """
        ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        """
        try:
            required_fields = ['table_name', 'offset', 'total_exported', 'timestamp', 'status']
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            for field in required_fields:
                if field not in checkpoint_data:
                    logger.warning(f"ì²´í¬í¬ì¸íŠ¸ì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    return False
            
            # ë°ì´í„° íƒ€ì… ë° ê°’ ê²€ì¦
            if not isinstance(checkpoint_data['offset'], int) or checkpoint_data['offset'] < 0:
                logger.warning(f"ì²´í¬í¬ì¸íŠ¸ offset ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {checkpoint_data['offset']}")
                return False
            
            if not isinstance(checkpoint_data['total_exported'], int) or checkpoint_data['total_exported'] < 0:
                logger.warning(f"ì²´í¬í¬ì¸íŠ¸ total_exported ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {checkpoint_data['total_exported']}")
                return False
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê²€ì¦
            try:
                timestamp = pd.Timestamp(checkpoint_data['timestamp'])
                if timestamp > pd.Timestamp.now():
                    logger.warning("ì²´í¬í¬ì¸íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë¯¸ë˜ ì‹œê°„")
                    return False
            except Exception:
                logger.warning("ì²´í¬í¬ì¸íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
                return False
            
            logger.debug("ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ í†µê³¼")
            return True
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def _backup_checkpoint(self, checkpoint_file: str):
        """
        ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë°±ì—…
        """
        try:
            backup_file = f"{checkpoint_file}.backup"
            import shutil
            shutil.copy2(checkpoint_file, backup_file)
            logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ë°±ì—… ìƒì„±: {backup_file}")
        except Exception as e:
            logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")

    def _calculate_csv_checksum(self, csv_path: str) -> str | None:
        """
        CSV íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)
        """
        import hashlib
        
        try:
            # íŒŒì¼ í¬ê¸°ê°€ í° ê²½ìš° ì²­í¬ ë‹¨ìœ„ë¡œ ì²´í¬ì„¬ ê³„ì‚°
            chunk_size = 8192  # 8KB ì²­í¬
            hasher = hashlib.md5()
            
            with open(csv_path, 'rb') as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    hasher.update(chunk)
            
            checksum = hasher.hexdigest()
            logger.debug(f"CSV ì²´í¬ì„¬ ê³„ì‚° ì™„ë£Œ: {csv_path} -> {checksum[:8]}...")
            return checksum
            
        except Exception as e:
            logger.error(f"CSV ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨: {csv_path}, ì˜¤ë¥˜: {e}")
            return None

    def _cleanup_checkpoint(self, csv_path: str):
        """
        ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì •ë¦¬ (ê°œì„ ëœ ë²„ì „)
        """
        checkpoint_file = f"{csv_path}.checkpoint"
        if os.path.exists(checkpoint_file):
            try:
                # ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                with open(checkpoint_file, 'r') as f:
                    checkpoint_data = json.load(f)
                
                # ì™„ë£Œ ì •ë³´ ì¶”ê°€
                checkpoint_data['status'] = 'completed'
                checkpoint_data['completed_at'] = pd.Timestamp.now().isoformat()
                checkpoint_data['final_memory_usage_mb'] = self._check_memory_usage()
                
                # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê¸° (ì›ìì„± ë³´ì¥)
                temp_checkpoint_file = f"{checkpoint_file}.tmp"
                with open(temp_checkpoint_file, 'w') as f:
                    json.dump(checkpoint_data, f, indent=2)
                
                # ì›ìì  ì´ë™
                os.rename(temp_checkpoint_file, checkpoint_file)
                
                logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸: {csv_path}")
                
                # ë°±ì—… íŒŒì¼ ì •ë¦¬ (ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ)
                self._cleanup_old_checkpoint_backups(csv_path)
                
                # ì™„ë£Œëœ ì²´í¬í¬ì¸íŠ¸ëŠ” ì¼ì • ì‹œê°„ í›„ ìë™ ì‚­ì œ (ì„ íƒì‚¬í•­)
                # self._schedule_checkpoint_cleanup(checkpoint_file)
                
            except Exception as e:
                logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                # ì •ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì€ ë³´ì¡´
                logger.warning("ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì‹¤íŒ¨í–ˆì§€ë§Œ íŒŒì¼ì€ ë³´ì¡´ë¨")

    def _cleanup_old_checkpoint_backups(self, csv_path: str):
        """
        ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ë°±ì—… íŒŒì¼ ì •ë¦¬
        """
        try:
            base_path = csv_path.replace('.csv', '')
            backup_pattern = f"{base_path}.csv.checkpoint.backup*"
            
            import glob
            backup_files = glob.glob(backup_pattern)
            
            # 7ì¼ ì´ìƒ ëœ ë°±ì—… íŒŒì¼ ì‚­ì œ
            current_time = time.time()
            for backup_file in backup_files:
                try:
                    file_age = current_time - os.path.getmtime(backup_file)
                    if file_age > 7 * 24 * 3600:  # 7ì¼
                        os.remove(backup_file)
                        logger.debug(f"ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì‚­ì œ: {backup_file}")
                except Exception:
                    continue
                    
        except Exception as e:
            logger.debug(f"ë°±ì—… íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _schedule_checkpoint_cleanup(self, checkpoint_file: str):
        """
        ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìë™ ì‚­ì œ ìŠ¤ì¼€ì¤„ë§ (ì„ íƒì )
        """
        try:
            # 24ì‹œê°„ í›„ ì‚­ì œë¥¼ ìœ„í•œ íƒ€ì´ë¨¸ ì„¤ì •
            import threading
            
            def delayed_delete():
                time.sleep(24 * 3600)  # 24ì‹œê°„ ëŒ€ê¸°
                try:
                    if os.path.exists(checkpoint_file):
                        os.remove(checkpoint_file)
                        logger.info(f"ì™„ë£Œëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìë™ ì‚­ì œ: {checkpoint_file}")
                except Exception as e:
                    logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ìë™ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            cleanup_thread = threading.Thread(target=delayed_delete, daemon=True)
            cleanup_thread.start()
            
        except Exception as e:
            logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ìë™ ì‚­ì œ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨: {e}")

    def _optimize_batch_size_dynamically(
        self, 
        initial_batch_size: int, 
        memory_usage: float, 
        processing_time: float, 
        session_health: bool
    ) -> int:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì²˜ë¦¬ ì‹œê°„, ì„¸ì…˜ ìƒíƒœì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
        """
        if not session_health:
            # ì„¸ì…˜ ìƒíƒœ ë¶ˆëŸ‰ ì‹œì—ë„ ë„ˆë¬´ ê¸‰ê²©í•˜ê²Œ ê°ì†Œí•˜ì§€ ì•Šë„ë¡ ê°œì„ 
            if initial_batch_size > 1000:
                new_batch_size = max(initial_batch_size // 2, 1000)
            else:
                new_batch_size = max(initial_batch_size // 2, 500)
            logger.info(f"ì„¸ì…˜ ìƒíƒœ ë¶ˆëŸ‰, ë°°ì¹˜ í¬ê¸° ì¡°ì •: {initial_batch_size} â†’ {new_batch_size}")
            return new_batch_size
        
        elif memory_usage > 200:  # 200MBë¡œ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
            new_batch_size = max(initial_batch_size // 2, 500)
            logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ, ë°°ì¹˜ í¬ê¸° ì¡°ì •: {initial_batch_size} â†’ {new_batch_size}")
            return new_batch_size
        
        elif processing_time > 60:  # 60ì´ˆë¡œ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
            new_batch_size = max(initial_batch_size // 2, 500)
            logger.info(f"ì²˜ë¦¬ ì‹œê°„ ê¸¸ìŒ, ë°°ì¹˜ í¬ê¸° ì¡°ì •: {initial_batch_size} â†’ {new_batch_size}")
            return new_batch_size
        
        elif memory_usage < 100 and processing_time < 20 and session_health:  # ì—¬ìœ ë¡œìš´ ìƒí™©
            new_batch_size = min(initial_batch_size * 1.5, 5000)  # ë” ì ê·¹ì ìœ¼ë¡œ ì¦ê°€
            logger.info(f"ì—¬ìœ ë¡œìš´ ìƒí™©, ë°°ì¹˜ í¬ê¸° ì¦ê°€: {initial_batch_size} â†’ {new_batch_size}")
            return new_batch_size
        
        return initial_batch_size

    def _log_performance_metrics(
        self, 
        performance_metrics: dict, 
        chunk_count: int, 
        total_exported: int, 
        total_count: int
    ):
        """
        ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
        """
        try:
            if performance_metrics['chunk_processing_times']:
                avg_chunk_time = sum(performance_metrics['chunk_processing_times'][-100:]) / min(len(performance_metrics['chunk_processing_times']), 100)
                logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ - ì²­í¬ {chunk_count}: í‰ê·  ì²˜ë¦¬ì‹œê°„ {avg_chunk_time:.2f}ì´ˆ")
            
            if performance_metrics['memory_usage_history']:
                recent_memory = performance_metrics['memory_usage_history'][-1]
                logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {recent_memory:.1f}MB")
            
            logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ - ì§„í–‰ë¥ : {total_exported}/{total_count} ({total_exported/total_count*100:.1f}%)")
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹… ì‹¤íŒ¨: {e}")

    def _log_final_performance_report(
        self, 
        performance_metrics: dict, 
        total_exported: int, 
        total_time: float
    ):
        """
        ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë¡œê¹…
        """
        try:
            logger.info("=== ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ===")
            logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            logger.info(f"ì´ ì²˜ë¦¬ëœ í–‰: {total_exported}")
            
            if performance_metrics['chunk_processing_times']:
                avg_chunk_time = sum(performance_metrics['chunk_processing_times']) / len(performance_metrics['chunk_processing_times'])
                min_chunk_time = min(performance_metrics['chunk_processing_times'])
                max_chunk_time = max(performance_metrics['chunk_processing_times'])
                logger.info(f"ì²­í¬ ì²˜ë¦¬ ì‹œê°„ - í‰ê· : {avg_chunk_time:.2f}ì´ˆ, ìµœì†Œ: {min_chunk_time:.2f}ì´ˆ, ìµœëŒ€: {max_chunk_time:.2f}ì´ˆ")
            
            if performance_metrics['memory_usage_history']:
                avg_memory = sum(performance_metrics['memory_usage_history']) / len(performance_metrics['memory_usage_history'])
                max_memory = max(performance_metrics['memory_usage_history'])
                logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ - í‰ê· : {avg_memory:.1f}MB, ìµœëŒ€: {max_memory:.1f}MB")
            
            logger.info(f"ì„¸ì…˜ ê°±ì‹  íšŸìˆ˜: {performance_metrics['session_refresh_count']}")
            logger.info(f"ì´ˆë‹¹ ì²˜ë¦¬ í–‰ ìˆ˜: {total_exported/total_time:.1f}")
            logger.info("=== ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì™„ë£Œ ===")
            
        except Exception as e:
            logger.error(f"ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")

    def _export_to_csv_parallel(
        self, 
        table_name: str, 
        csv_path: str, 
        where_clause: str | None, 
        batch_size: int, 
        order_by_field: str | None, 
        num_workers: int = 2
    ) -> int:
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì„±ëŠ¥ í–¥ìƒ (ì„ íƒì )
        """
        try:
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            # ì „ì²´ í–‰ ìˆ˜ ì¡°íšŒ
            total_count = self.db_ops.get_table_row_count(table_name, where_clause=where_clause)
            
            if total_count == 0:
                logger.warning(f"í…Œì´ë¸” {table_name}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            # ì²­í¬ ë²”ìœ„ ê³„ì‚°
            chunk_ranges = self._calculate_chunk_ranges(total_count, batch_size, num_workers)
            
            # ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=num_workers) as executor:
                futures = []
                
                for chunk_start, chunk_end in chunk_ranges:
                    future = executor.submit(
                        self._process_chunk_range,
                        table_name, where_clause, chunk_start, chunk_end, order_by_field
                    )
                    futures.append(future)
                
                # ê²°ê³¼ ìˆ˜ì§‘ ë° CSV ë³‘í•©
                all_chunks = []
                for future in as_completed(futures):
                    chunk_data = future.result()
                    all_chunks.append(chunk_data)
                
                # ì²­í¬ë³„ë¡œ CSVì— ìˆœì°¨ì ìœ¼ë¡œ ì“°ê¸°
                self._merge_chunks_to_csv(all_chunks, csv_path)
                
                return total_count
                
        except Exception as e:
            logger.error(f"ë³‘ë ¬ ì²˜ë¦¬ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _calculate_chunk_ranges(self, total_count: int, batch_size: int, num_workers: int) -> list[tuple[int, int]]:
        """
        ì²­í¬ ë²”ìœ„ ê³„ì‚°
        """
        chunk_ranges = []
        for i in range(num_workers):
            start = i * (total_count // num_workers)
            end = (i + 1) * (total_count // num_workers) if i < num_workers - 1 else total_count
            chunk_ranges.append((start, end))
        return chunk_ranges

    def _process_chunk_range(
        self, 
        table_name: str, 
        where_clause: str | None, 
        chunk_start: int, 
        chunk_end: int, 
        order_by_field: str | None
    ) -> list:
        """
        íŠ¹ì • ë²”ìœ„ì˜ ì²­í¬ ì²˜ë¦¬
        """
        try:
            # ORDER BY ì ˆ êµ¬ì„±
            if order_by_field:
                order_by_clause = f"ORDER BY {order_by_field}"
            else:
                order_by_clause = "ORDER BY 1"
            
            # WHERE ì ˆ êµ¬ì„±
            if where_clause:
                query = f"""
                    SELECT * FROM {table_name}
                    WHERE {where_clause}
                    {order_by_clause}
                    LIMIT {chunk_end - chunk_start} OFFSET {chunk_start}
                """
            else:
                query = f"""
                    SELECT * FROM {table_name}
                    {order_by_clause}
                    LIMIT {chunk_end - chunk_start} OFFSET {chunk_start}
                """
            
            # ë°ì´í„° ì¡°íšŒ
            chunk_data = self.source_hook.get_records(query)
            return chunk_data
            
        except Exception as e:
            logger.error(f"ì²­í¬ ë²”ìœ„ ì²˜ë¦¬ ì‹¤íŒ¨ ({chunk_start}-{chunk_end}): {e}")
            return []

    def _merge_chunks_to_csv(self, all_chunks: list, csv_path: str):
        """
        ì²­í¬ë³„ë¡œ CSVì— ìˆœì°¨ì ìœ¼ë¡œ ì“°ê¸°
        """
        try:
            if not all_chunks:
                return
            
            # ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ ì»¬ëŸ¼ëª… ì¶”ì¶œ
            columns = list(all_chunks[0][0].keys()) if all_chunks[0] else []
            
            with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                
                # í—¤ë” ì‘ì„±
                writer.writerow(columns)
                
                # ê° ì²­í¬ì˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì“°ê¸°
                for chunk_data in all_chunks:
                    for row in chunk_data:
                        writer.writerow([row[col] for col in columns])
                        
            logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ CSV ë³‘í•© ì™„ë£Œ: {csv_path}")
            
        except Exception as e:
            logger.error(f"ì²­í¬ CSV ë³‘í•© ì‹¤íŒ¨: {e}")
            raise

    def get_source_max_xmin(self, source_table: str) -> int:
        """
        ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ìµœëŒ€ xmin ê°’ ì¡°íšŒ
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            
        Returns:
            ìµœëŒ€ xmin ê°’ (ì‹¤íŒ¨ ì‹œ 0)
        """
        try:
            logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì˜ ìµœëŒ€ xmin ê°’ ì¡°íšŒ ì‹œì‘")
            
            # PostgreSQL ì‹œìŠ¤í…œ í•„ë“œ xminì˜ ìµœëŒ€ê°’ ì¡°íšŒ
            max_xmin_sql = f"SELECT MAX(xmin::text::bigint) FROM {source_table}"
            result = self.source_hook.get_first(max_xmin_sql)
            
            if result and result[0]:
                max_xmin = result[0]
                logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì˜ ìµœëŒ€ xmin: {max_xmin}")
                return max_xmin
            else:
                logger.warning(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì—ì„œ xmin ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return 0
                
        except Exception as e:
            logger.error(f"ì†ŒìŠ¤ í…Œì´ë¸” xmin ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0

    def export_to_csv_with_xmin(self, source_table: str, csv_file: str, batch_size: int = None) -> int:
        """
        xmin ê°’ì„ í¬í•¨í•˜ì—¬ CSV ë‚´ë³´ë‚´ê¸° (ì›Œì»¤ ì„¤ì • ì ìš©)
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            csv_file: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ì›Œì»¤ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ê²°ì •)
            
        Returns:
            ë‚´ë³´ë‚¸ í–‰ ìˆ˜
        """
        try:
            # 1. ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ìµœëŒ€ xmin ê°’ ì¡°íšŒ
            max_xmin = self.get_source_max_xmin(source_table)
            logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì˜ ìµœëŒ€ xmin: {max_xmin}")
            
            # 2. ë°°ì¹˜ í¬ê¸° ê²°ì • (ì›Œì»¤ ì„¤ì • ê¸°ë°˜)
            if batch_size is None:
                # ì›Œì»¤ ìˆ˜ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ì¡°ì •
                default_workers = self.worker_config.get("default_workers", 4)
                if default_workers <= 2:
                    batch_size = 5000
                elif default_workers <= 4:
                    batch_size = 10000
                else:
                    batch_size = 15000
                logger.info(f"ì›Œì»¤ ìˆ˜({default_workers}) ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ìë™ ì„¤ì •: {batch_size}")
            
            # 3. ì „ì²´ í–‰ ìˆ˜ ì¡°íšŒ
            count_sql = f"SELECT COUNT(*) FROM {source_table}"
            total_rows = self.source_hook.get_first(count_sql)[0]
            
            if total_rows == 0:
                logger.warning(f"í…Œì´ë¸” {source_table}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            # 4. ì›Œì»¤ ìˆ˜ ì„¤ì •
            num_workers = self.worker_config.get("default_workers", 4)
            
            logger.info(f"CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘: {source_table} -> {csv_file}")
            logger.info(f"ì´ í–‰ ìˆ˜: {total_rows}, ë°°ì¹˜ í¬ê¸°: {batch_size}, ì›Œì»¤ ìˆ˜: {num_workers}")
            
            # 5. ë°°ì¹˜ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬í•˜ì—¬ CSV íŒŒì¼ì— ì“°ê¸°
            exported_rows = 0
            with open(csv_file, 'w', encoding='utf-8') as f:
                # í—¤ë” ì‘ì„± (xmin ì»¬ëŸ¼ í¬í•¨)
                header_written = False
                
                for offset in range(0, total_rows, batch_size):
                    query = f"""
                        SELECT 
                            *,
                            {max_xmin} as source_xmin
                        FROM {source_table}
                        ORDER BY 1
                        LIMIT {batch_size} OFFSET {offset}
                    """
                    
                    batch_data = self.source_hook.get_records(query)
                    
                    if not header_written and batch_data:
                        # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ í—¤ë” ì‘ì„±
                        columns = list(batch_data[0].keys())
                        f.write(','.join(columns) + '\n')
                        header_written = True
                    
                    # CSV í˜•ì‹ìœ¼ë¡œ ì“°ê¸°
                    for row in batch_data:
                        f.write(','.join(str(cell) if cell is not None else '' for cell in row) + '\n')
                        exported_rows += 1
                    
                    # ì§„í–‰ë¥  ë¡œê¹…
                    current_progress = min(offset + batch_size, total_rows)
                    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {current_progress}/{total_rows} ({exported_rows}í–‰ ì²˜ë¦¬ë¨)")
            
            logger.info(f"xmin ê°’({max_xmin})ì„ í¬í•¨í•œ CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {csv_file}, ì´ {exported_rows}í–‰")
            return exported_rows
            
        except Exception as e:
            logger.error(f"xmin ê°’ì„ í¬í•¨í•œ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _validate_and_convert_data_types(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
        """
        ë°ì´í„°í”„ë ˆì„ì˜ ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ìˆ˜í–‰

        Args:
            df: ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„
            table_name: í…Œì´ë¸”ëª… (ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒìš©)

        Returns:
            ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info(f"í…Œì´ë¸” {table_name}ì˜ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘")

            # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
            source_schema = self.db_ops.get_table_schema(table_name)
            if not source_schema or not source_schema.get("columns"):
                logger.warning(f"í…Œì´ë¸” {table_name}ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë³€í™˜ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                return self._basic_data_type_conversion(df)

            # ì»¬ëŸ¼ë³„ íƒ€ì… ì •ë³´ ë§¤í•‘
            column_types = {}
            for col in source_schema["columns"]:
                column_types[col["name"]] = col["type"]

            logger.info(f"ì»¬ëŸ¼ íƒ€ì… ì •ë³´: {column_types}")

            # ê° ì»¬ëŸ¼ë³„ë¡œ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            for col_name in df.columns:
                if col_name in column_types:
                    col_type = column_types[col_name]
                    logger.info(f"ì»¬ëŸ¼ {col_name} ({col_type}) ê²€ì¦ ë° ë³€í™˜ ì‹œì‘")

                    # ì»¬ëŸ¼ íƒ€ì…ë³„ ë³€í™˜ í•¨ìˆ˜ í˜¸ì¶œ
                    df[col_name] = self._convert_column_by_type(df[col_name], col_type, col_name)

                    # ë³€í™˜ í›„ ê²€ì¦
                    self._validate_column_after_conversion(df[col_name], col_type, col_name)
                else:
                    logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ íƒ€ì… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            logger.info(f"í…Œì´ë¸” {table_name}ì˜ ëª¨ë“  ì»¬ëŸ¼ ë³€í™˜ ì™„ë£Œ")
            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ë³€í™˜ì€ ìˆ˜í–‰
            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return self._basic_data_type_conversion(df)

    def _convert_column_by_type(self, column_series: pd.Series, col_type: str, col_name: str) -> pd.Series:
        """
        ì»¬ëŸ¼ íƒ€ì…ì— ë”°ë¼ ë°ì´í„° ë³€í™˜ ìˆ˜í–‰

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_type: ì»¬ëŸ¼ íƒ€ì…
            col_type: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            # ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬
            if col_name == "priority" and col_type in ["BIGINT", "INTEGER", "SMALLINT"]:
                logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘: {col_type}")
                return self._convert_priority_column(column_series)

            # ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            if col_type in ["BIGINT", "INTEGER", "SMALLINT"]:
                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_integer_column(column_series, col_name)

            # ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            elif col_type in ["DOUBLE PRECISION", "REAL", "NUMERIC", "DECIMAL"]:
                logger.info(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_float_column(column_series, col_name)

            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            elif col_type in ["DATE", "TIMESTAMP", "TIME"]:
                logger.info(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_datetime_column(column_series, col_name)

            # ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            elif col_type in ["BOOLEAN"]:
                logger.info(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_boolean_column(column_series, col_name)

            # ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            else:
                logger.info(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_string_column(column_series, col_name)

        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë°˜í™˜
            return column_series

    def _convert_priority_column(self, column_series: pd.Series) -> pd.Series:
        """
        PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ - ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info("ğŸš¨ PRIORITY ì»¬ëŸ¼ ë³€í™˜ ì‹œì‘")

            # í˜„ì¬ ê°’ í™•ì¸
            sample_values = column_series.head(10).tolist()
            logger.info(f"PRIORITY ì»¬ëŸ¼ í˜„ì¬ ê°’ ìƒ˜í”Œ: {sample_values}")

            # ì†Œìˆ˜ì  ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
            decimal_count = column_series.astype(str).str.contains(r'\.', na=False).sum()
            logger.info(f"PRIORITY ì»¬ëŸ¼ ì†Œìˆ˜ì  ê°’ ê°œìˆ˜: {decimal_count}")

            if decimal_count > 0:
                logger.info("ğŸš¨ PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

                # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                def convert_priority_value(x):
                    if pd.isna(x) or x is None:
                        return None
                    if isinstance(x, str):
                        x = x.strip()
                        if not x or x in ["", "nan", "None", "NULL", "null"]:
                            return None
                        if "." in x:
                            try:
                                float_val = float(x)
                                int_val = int(float_val)
                                logger.debug(f"PRIORITY ê°’ ë³€í™˜: '{x}' â†’ '{int_val}'")
                                return str(int_val)
                            except (ValueError, TypeError):
                                logger.warning(f"PRIORITY ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                return x
                    elif isinstance(x, (int, float)):
                        return str(int(x))
                    return str(x) if x is not None else None

                # ë³€í™˜ ì „í›„ ë¹„êµ
                before_values = column_series.copy()
                column_series = column_series.apply(convert_priority_value)
                after_values = column_series.copy()

                # ë³€í™˜ëœ ê°’ í™•ì¸
                changed_mask = before_values != after_values
                changed_count = changed_mask.sum()
                if changed_count > 0:
                    logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ {changed_count}ê°œ ê°’ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    # ë³€í™˜ëœ ê°’ë“¤ ë¡œê¹…
                    changed_indices = changed_mask[changed_mask].index
                    for idx in changed_indices[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                        logger.info(f"PRIORITY ë³€í™˜: í–‰ {idx}: '{before_values[idx]}' â†’ '{after_values[idx]}'")

                # ìµœì¢… ê²€ì¦
                final_decimal_count = column_series.astype(str).str.contains(r'\.', na=False).sum()
                if final_decimal_count == 0:
                    logger.info("âœ… PRIORITY ì»¬ëŸ¼ì˜ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    logger.warning(f"âš ï¸ PRIORITY ì»¬ëŸ¼ì— ì—¬ì „íˆ {final_decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            else:
                logger.info("âœ… PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")

            return column_series

        except Exception as e:
            logger.error(f"PRIORITY ì»¬ëŸ¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_integer_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ì†Œìˆ˜ì  ê°’, ë¹ˆ ë¬¸ìì—´, NaN ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
            def convert_decimal_to_int(x):
                if pd.isna(x) or x is None:
                    return None
                if isinstance(x, str):
                    x = x.strip()
                    if not x or x in ["", "nan", "None", "NULL", "null"]:
                        return None
                    if "." in x:
                        try:
                            float_val = float(x)
                            int_val = int(float_val)
                            return str(int_val)
                        except (ValueError, TypeError):
                            logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            return x
                elif isinstance(x, (int, float)):
                    return str(int(x))
                return str(x) if x is not None else None

            # ë³€í™˜ ì „í›„ ê°’ í™•ì¸
            before_values = column_series.copy()
            column_series = column_series.apply(convert_decimal_to_int)
            after_values = column_series.copy()

            # ë³€í™˜ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
            changed_count = sum(1 for b, a in zip(before_values, after_values) if b != a)
            if changed_count > 0:
                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ {changed_count}ê°œ ê°’ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_float_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¹ˆ ë¬¸ìì—´, NaN ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_datetime_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¹ˆ ë¬¸ìì—´, ì˜ëª»ëœ í˜•ì‹ ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_boolean_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¬¸ìì—´ ê°’ì„ ë¶ˆë¦°ìœ¼ë¡œ ë³€í™˜

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ë¬¸ìì—´ ê°’ì„ ë¶ˆë¦°ìœ¼ë¡œ ë³€í™˜
            def convert_to_boolean(x):
                if pd.isna(x) or x is None:
                    return None
                if isinstance(x, str):
                    x = x.strip().lower()
                    if x in ["true", "1", "yes", "on"]:
                        return "true"
                    elif x in ["false", "0", "no", "off"]:
                        return "false"
                    else:
                        return x
                elif isinstance(x, bool):
                    return str(x).lower()
                elif isinstance(x, (int, float)):
                    return "true" if x else "false"
                return str(x) if x is not None else None

            column_series = column_series.apply(convert_to_boolean)

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_string_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¹ˆ ë¬¸ìì—´, NaN ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _validate_column_after_conversion(self, column_series: pd.Series, col_type: str, col_name: str) -> None:
        """
        ë³€í™˜ í›„ ì»¬ëŸ¼ ê²€ì¦

        Args:
            column_series: ê²€ì¦í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_type: ì»¬ëŸ¼ íƒ€ì…
            col_name: ì»¬ëŸ¼ëª…
        """
        try:
            # ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ì˜ ê²½ìš° ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
            if col_type in ["BIGINT", "INTEGER", "SMALLINT"]:
                decimal_count = column_series.astype(str).str.contains(r'\.', na=False).sum()
                if decimal_count > 0:
                    logger.warning(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                    # ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤ì„ ë¡œê¹…
                    problem_values = column_series[column_series.astype(str).str.contains(r'\.', na=False)]
                    logger.warning(f"ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤: {problem_values.head(5).tolist()}")
                else:
                    logger.info(f"ì»¬ëŸ¼ {col_name} ê²€ì¦ ì™„ë£Œ: ì†Œìˆ˜ì  ê°’ ì—†ìŒ")

            # ëª¨ë“  íƒ€ì… ì»¬ëŸ¼ì˜ ê²½ìš° ë¹ˆ ë¬¸ìì—´ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
            empty_count = (column_series.astype(str).str.strip() == "").sum()
            if empty_count > 0:
                logger.warning(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {empty_count}ê°œì˜ ë¹ˆ ë¬¸ìì—´ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
            else:
                logger.info(f"ì»¬ëŸ¼ {col_name} ê²€ì¦ ì™„ë£Œ: ë¹ˆ ë¬¸ìì—´ ì—†ìŒ")

        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ {col_name} ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _basic_data_type_conversion(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ (ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì„ ë•Œ ì‚¬ìš©)

        Args:
            df: ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„

        Returns:
            ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ ì‹œì‘")

            for col_name in df.columns:
                # ëª¨ë“  ì»¬ëŸ¼ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì²˜ë¦¬í•  ë³€í™˜
                df[col_name] = df[col_name].replace({
                    "": None,
                    " ": None,
                    "nan": None,
                    "None": None,
                    "NULL": None,
                    "null": None
                })

                # pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
                df[col_name] = df[col_name].replace({pd.NA: None, pd.NaT: None})

            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ")
            return df

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return df

    def _validate_and_convert_data_types_after_csv_read(self, df: pd.DataFrame, source_schema: dict) -> pd.DataFrame:
        """
        CSV ì½ê¸° í›„ ë°ì´í„°í”„ë ˆì„ì˜ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ìˆ˜í–‰

        Args:
            df: ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„
            source_schema: ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´

        Returns:
            ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info("=== CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘ ===")

            # ì»¬ëŸ¼ë³„ íƒ€ì… ì •ë³´ ë§¤í•‘
            column_types = {}
            for col in source_schema["columns"]:
                column_types[col["name"]] = col["type"]

            logger.info(f"ì»¬ëŸ¼ íƒ€ì… ì •ë³´: {column_types}")

            # ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬
            logger.info("=== ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘ ===")
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                if col_name == "priority" and col_type in ["BIGINT", "INTEGER"]:
                    logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ ë°œê²¬! íƒ€ì…: {col_type}")
                    if col_name in df.columns:
                        # í˜„ì¬ ê°’ í™•ì¸
                        sample_values = df[col_name].head(10).tolist()
                        logger.info(f"PRIORITY ì»¬ëŸ¼ í˜„ì¬ ê°’ ìƒ˜í”Œ: {sample_values}")

                        # ì†Œìˆ˜ì  ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                        decimal_count = df[col_name].astype(str).str.contains(r'\.', na=False).sum()
                        logger.info(f"PRIORITY ì»¬ëŸ¼ ì†Œìˆ˜ì  ê°’ ê°œìˆ˜: {decimal_count}")

                        if decimal_count > 0:
                            logger.info("ğŸš¨ PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

                            # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                            def convert_priority_value(x):
                                if pd.isna(x) or x is None:
                                    return None
                                if isinstance(x, str):
                                    x = x.strip()
                                    if not x or x in ["", "nan", "None", "NULL", "null"]:
                                        return None
                                    if "." in x:
                                        try:
                                            float_val = float(x)
                                            int_val = int(float_val)
                                            logger.debug(f"PRIORITY ê°’ ë³€í™˜: '{x}' â†’ '{int_val}'")
                                            return str(int_val)
                                        except (ValueError, TypeError):
                                            logger.warning(f"PRIORITY ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                            return x
                                elif isinstance(x, (int, float)):
                                    return str(int(x))
                                return str(x) if x is not None else None

                            # ë³€í™˜ ì „í›„ ë¹„êµ
                            before_values = df[col_name].copy()
                            df[col_name] = df[col_name].apply(convert_priority_value)
                            after_values = df[col_name].copy()

                            # ë³€í™˜ëœ ê°’ í™•ì¸
                            changed_mask = before_values != after_values
                            changed_count = changed_mask.sum()
                            if changed_count > 0:
                                logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ {changed_count}ê°œ ê°’ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                # ë³€í™˜ëœ ê°’ë“¤ ë¡œê¹…
                                changed_indices = changed_mask[changed_mask].index
                                for idx in changed_indices[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                                    logger.info(f"PRIORITY ë³€í™˜: í–‰ {idx}: '{before_values[idx]}' â†’ '{after_values[idx]}'")

                            # ìµœì¢… ê²€ì¦
                            final_decimal_count = df[col_name].astype(str).str.contains(r'\.', na=False).sum()
                            if final_decimal_count == 0:
                                logger.info("âœ… PRIORITY ì»¬ëŸ¼ì˜ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            else:
                                logger.warning(f"âš ï¸ PRIORITY ì»¬ëŸ¼ì— ì—¬ì „íˆ {final_decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
                        else:
                            logger.info("âœ… PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        logger.warning(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            # ê° ì»¬ëŸ¼ë³„ë¡œ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                if col_name in df.columns:
                    logger.info(f"ì»¬ëŸ¼ {col_name} ({col_type}) ê²€ì¦ ë° ë³€í™˜ ì‹œì‘")

                    # ì»¬ëŸ¼ íƒ€ì…ë³„ ë³€í™˜ í•¨ìˆ˜ í˜¸ì¶œ
                    df[col_name] = self._convert_column_by_type(df[col_name], col_type, col_name)

                    # ë³€í™˜ í›„ ê²€ì¦
                    self._validate_column_after_conversion(df[col_name], col_type, col_name)
                else:
                    logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ íƒ€ì… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            logger.info("=== CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì™„ë£Œ ===")
            return df

        except Exception as e:
            logger.error(f"CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ë³€í™˜ì€ ìˆ˜í–‰
            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return self._basic_data_type_conversion(df)

    def create_temp_table_and_import_csv(
        self, target_table: str, source_schema: dict[str, Any], csv_path: str, batch_size: int = 1000
    ) -> tuple[str, int]:
        """
        ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸°ë¥¼ í•˜ë‚˜ì˜ ì—°ê²°ì—ì„œ ì‹¤í–‰

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            (ì„ì‹œ í…Œì´ë¸”ëª…, ê°€ì ¸ì˜¨ í–‰ ìˆ˜) íŠœí”Œ
        """
        try:
            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„±
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]  # data_type -> typeìœ¼ë¡œ ë³€ê²½
                is_nullable = col["nullable"]  # nullable ì •ë³´ ë³µì›

                # PostgreSQL ë°ì´í„° íƒ€ì… ë§¤í•‘
                if col_type.upper() in ["VARCHAR", "CHAR", "TEXT"]:
                    pg_type = "TEXT"
                elif (
                    col_type.upper() in ["INTEGER", "INT", "BIGINT", "SMALLINT"]
                    or col_type.upper()
                    in ["DECIMAL", "NUMERIC", "REAL", "DOUBLE PRECISION"]
                    or col_type.upper() in ["DATE", "TIMESTAMP", "TIME"]
                ):
                    pg_type = "TEXT"  # ì•ˆì „ì„±ì„ ìœ„í•´ TEXTë¡œ ë³€í™˜
                else:
                    pg_type = "TEXT"  # ê¸°ë³¸ê°’

                # ì›ë³¸ ìŠ¤í‚¤ë§ˆì˜ nullable ì •ë³´ë¥¼ ìœ ì§€
                nullable_clause = "NOT NULL" if not is_nullable else ""
                column_definitions.append(
                    f"{col_name} {pg_type} {nullable_clause}".strip()
                )

            # CREATE TABLE ë¬¸ ìƒì„± (ì„ì‹œ í…Œì´ë¸” ëŒ€ì‹  ì˜êµ¬ í…Œì´ë¸” ì‚¬ìš©)
            create_sql = f"""
                CREATE TABLE {temp_table} (
                    {', '.join(column_definitions)}
                )
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_sql}")

            # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            target_hook = self.target_hook

            # í•˜ë‚˜ì˜ ì—°ê²°ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸° ì‹¤í–‰
            with target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # 1. ì„ì‹œ í…Œì´ë¸” ìƒì„±
                    cursor.execute(create_sql)
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # 2. CSV íŒŒì¼ ì½ê¸° (NA ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ì˜ëª» ì¸ì‹í•˜ì§€ ì•Šë„ë¡ ì„¤ì •)
                    df = pd.read_csv(
                        csv_path, encoding="utf-8", na_values=[], keep_default_na=False
                    )

                    if df.empty:
                        logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        return temp_table, 0

                    # 3. ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                    if source_schema and source_schema.get("columns"):
                        temp_columns = [col["name"] for col in source_schema["columns"]]
                        logger.info(
                            f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {temp_columns}"
                        )

                        # CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬
                        df_reordered = df[temp_columns]
                        logger.info(
                            f"CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬í–ˆìŠµë‹ˆë‹¤: {temp_columns}"
                        )
                    else:
                        temp_columns = list(df.columns)
                        logger.info(
                            f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì–´ CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤: {temp_columns}"
                        )
                        df_reordered = df

                    # 4. ë°ì´í„° íƒ€ì… ë³€í™˜ ë° null ê°’ ê²€ì¦
                    # NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼ë“¤ í™•ì¸
                    not_null_columns = []
                    for col in source_schema["columns"]:
                        if not col["nullable"]:
                            not_null_columns.append(col["name"])

                    logger.info(f"NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼: {not_null_columns}")

                    # null ê°’ì´ ìˆëŠ” í–‰ ê²€ì¦ (ì‹¤ì œ null ê°’ë§Œ, "NA" ë¬¸ìì—´ì€ ì œì™¸)
                    null_violations = []
                    for col_name in not_null_columns:
                        if col_name in df_reordered.columns:
                            # ì‹¤ì œ null ê°’ë§Œ ê²€ì‚¬ (None, numpy.nan ë“±)
                            null_rows = df_reordered[df_reordered[col_name].isna()]
                            if not null_rows.empty:
                                null_violations.append(
                                    {
                                        "column": col_name,
                                        "count": len(null_rows),
                                        "sample_rows": null_rows.head(3).to_dict(
                                            "records"
                                        ),
                                    }
                                )

                    if null_violations:
                        logger.warning(
                            f"NOT NULL ì œì•½ì¡°ê±´ ìœ„ë°˜ ë°œê²¬: {null_violations}"
                        )
                        # null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œë¡œ ì²˜ë¦¬
                        for col_name in not_null_columns:
                            if col_name in df_reordered.columns:
                                df_reordered[col_name] = df_reordered[col_name].fillna(
                                    ""
                                )
                                logger.info(
                                    f"ì»¬ëŸ¼ {col_name}ì˜ null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤."
                                )

                    # ì¼ë°˜ì ì¸ ë°ì´í„° íƒ€ì… ë³€í™˜
                    for col in df_reordered.columns:
                        # ì‹¤ì œ null ê°’ë§Œ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜ ("NA" ë¬¸ìì—´ì€ ë³´ì¡´)
                        df_reordered[col] = df_reordered[col].fillna("")

                        # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                        if df_reordered[col].dtype in ["int64", "float64"]:
                            df_reordered[col] = df_reordered[col].astype(str)

                    logger.info(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ: {len(df_reordered)}í–‰")

                    # ë°ì´í„° ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                    logger.info("ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3í–‰):")
                    for i, row in df_reordered.head(3).iterrows():
                        logger.info(f"í–‰ {i}: {dict(row)}")

                    # 5. INSERT ì‹¤í–‰
                    total_inserted = 0

                    for i in range(0, len(df_reordered), batch_size):
                        batch_df = df_reordered.iloc[i : i + batch_size]
                        batch_data = [tuple(row) for row in batch_df.values]

                        insert_query = f"""
                            INSERT INTO {temp_table} ({', '.join(temp_columns)})
                            VALUES ({', '.join(['%s'] * len(temp_columns))})
                        """

                        cursor.executemany(insert_query, batch_data)
                        total_inserted += len(batch_data)

                        if i % 10000 == 0:
                            logger.info(
                                f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}"
                            )

                    # ì»¤ë°‹
                    conn.commit()

                    logger.info(
                        f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}"
                    )
                    return temp_table, total_inserted

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise Exception(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    def create_temp_table(
        self, target_table: str, source_schema: dict[str, Any]
    ) -> str:
        """
        ì„ì‹œ í…Œì´ë¸” ìƒì„±

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´

        Returns:
            ìƒì„±ëœ ì„ì‹œ í…Œì´ë¸”ëª…
        """
        try:
            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„±
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                # PostgreSQL íƒ€ì… ë§¤í•‘ - _convert_to_postgres_type ë©”ì„œë“œ ì‚¬ìš©
                col_type = self._convert_to_postgres_type(col_type, col.get('max_length'))

                nullable = "" if col["nullable"] else " NOT NULL"
                column_definitions.append(f"{col_name} {col_type}{nullable}")

            # ì„ì‹œ ìŠ¤í…Œì´ì§• í…Œì´ë¸” ìƒì„± (UNLOGGED ê¶Œì¥)
            # NOTE: TEMP í…Œì´ë¸”ì€ ì„¸ì…˜ ë²”ìœ„ ì œí•œì´ ìˆì–´ ì—¬ëŸ¬ ì»¤ë„¥ì…˜ì„ ì‚¬ìš©í•˜ëŠ” ë³¸ ì—”ì§„ê³¼ ë§ì§€ ì•ŠìŒ
            # ìš´ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ í•­ìƒ UNLOGGEDë¥¼ ì‚¬ìš©í•˜ê³ , ë¨¸ì§€ í›„ ëª…ì‹œì ìœ¼ë¡œ DROP ì²˜ë¦¬í•œë‹¤
            create_temp_table_sql = f"""
                CREATE UNLOGGED TABLE {temp_table} (
                    {', '.join(column_definitions)}
                )
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_temp_table_sql}")

            # ê°™ì€ ì—°ê²°ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(create_temp_table_sql)
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # ì„ì‹œ í…Œì´ë¸”ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    cursor.execute(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{temp_table}')")
                    exists = cursor.fetchone()[0]
                    if not exists:
                        raise Exception(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                    return temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {target_table}, ì˜¤ë¥˜: {e!s}")
            raise

    def import_from_csv(
        self,
        csv_path: str,
        temp_table: str,
        source_schema: dict[str, Any] | None = None,
        batch_size: int = 1000,
    ) -> int:
        """
        CSVë¥¼ ì„ì‹œ í…Œì´ë¸”ë¡œ ê°€ì ¸ì˜¤ê¸°

        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            temp_table: ì„ì‹œ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì„ íƒì‚¬í•­)
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ê°€ì ¸ì˜¨ í–‰ ìˆ˜
        """
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_path, encoding="utf-8")

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ì„ì‹œ í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            try:
                if source_schema and source_schema.get("columns"):
                    # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                    schema_columns = [col["name"] for col in source_schema["columns"]]
                    logger.info(
                        f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {schema_columns}"
                    )

                    # CSV ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                    csv_columns = list(df.columns)
                    logger.info(f"CSV íŒŒì¼ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª…: {csv_columns}")

                    # CSV ì»¬ëŸ¼ëª…ê³¼ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ëª…ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if len(csv_columns) != len(schema_columns):
                        logger.warning(
                            f"CSV ì»¬ëŸ¼ ìˆ˜({len(csv_columns)})ì™€ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ ìˆ˜({len(schema_columns)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

                    # CSVì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ì•ˆì „í•œ ì²˜ë¦¬)
                    available_columns = [col for col in schema_columns if col in csv_columns]
                    missing_columns = [col for col in schema_columns if col not in csv_columns]

                    if missing_columns:
                        logger.warning(f"CSVì— ì—†ëŠ” ì»¬ëŸ¼: {missing_columns}")

                    if not available_columns:
                        logger.warning("CSVì™€ ìŠ¤í‚¤ë§ˆ ê°„ì— ê³µí†µ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        available_columns = csv_columns

                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ì¬êµ¬ì„±
                    df_reordered = df[available_columns]
                    temp_columns = available_columns

                    logger.info(
                        f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ì¬êµ¬ì„±: {len(available_columns)}ê°œ ì»¬ëŸ¼"
                    )
                    logger.info(f"ìµœì¢… ì‚¬ìš© ì»¬ëŸ¼: {temp_columns}")
                    logger.info(f"ë°ì´í„°í”„ë ˆì„ í˜•íƒœ: {df_reordered.shape}")
                else:
                    # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    temp_columns = list(df.columns)
                    logger.info(
                        f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì–´ CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤: {temp_columns}"
                    )
                    df_reordered = df

            except Exception as e:
                logger.error(f"ì»¬ëŸ¼ ì •ë³´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise

            # ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ê²€ì¦
            try:
                # ë°ì´í„°í”„ë ˆì„ì˜ ë°ì´í„° íƒ€ì…ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
                for col in df_reordered.columns:
                    # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                    df_reordered[col] = df_reordered[col].where(
                        pd.notna(df_reordered[col]), None
                    )

                    # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    if df_reordered[col].dtype in ["int64", "float64"]:
                        df_reordered[col] = df_reordered[col].astype(str)

                logger.info(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ: {len(df_reordered)}í–‰")

            except Exception as e:
                logger.error(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {e}")
                raise Exception(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {e}")

            # INSERT ì‹¤í–‰
            try:
                # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
                total_inserted = 0

                # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì„¸ì…˜ ìµœì í™”
                target_hook = self.target_hook
                
                # ì„¸ì…˜ ìµœì í™” ì ìš©
                self._optimize_target_session(target_hook)

                with target_hook.get_conn() as conn:
                    with conn.cursor() as cursor:
                        for i in range(0, len(df_reordered), batch_size):
                            batch_df = df_reordered.iloc[i : i + batch_size]
                            batch_data = [tuple(row) for row in batch_df.values]

                            # INSERT ë¬¸ ì‹¤í–‰
                            insert_query = f"""
                                INSERT INTO {temp_table} ({', '.join(temp_columns)})
                                VALUES ({', '.join(['%s'] * len(temp_columns))})
                            """

                            cursor.executemany(insert_query, batch_data)
                            total_inserted += len(batch_data)

                            if i % 10000 == 0:
                                logger.info(
                                    f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}"
                                )

                        # ì»¤ë°‹
                        conn.commit()

                logger.info(
                    f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}"
                )
                return total_inserted

            except Exception as e:
                logger.error(f"INSERT ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                raise Exception(f"INSERT ì‹¤í–‰ ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {csv_path} -> {temp_table}, ì˜¤ë¥˜: {e!s}")
            raise

    def execute_merge_operation(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "full_sync",
    ) -> dict[str, Any]:
        """
        MERGE ì‘ì—… ì‹¤í–‰

        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª… (ì„ì‹œ í…Œì´ë¸”)
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            sync_mode: ë™ê¸°í™” ëª¨ë“œ ('full_sync' ë˜ëŠ” 'incremental_sync')

        Returns:
            MERGE ì‘ì—… ê²°ê³¼
        """
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            non_pk_columns = self.target_hook.get_records(all_columns_query)
            non_pk_column_names = [col[0] for col in non_pk_columns]

            # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not non_pk_column_names:
                logger.warning(
                    "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                )
                # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if sync_mode == "full_sync":
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # MERGE ì‹¤í–‰
                start_time = pd.Timestamp.now()
                self.target_hook.run(merge_sql)
                end_time = pd.Timestamp.now()

                # ê²°ê³¼ í™•ì¸
                source_count = self.db_ops.get_table_row_count(source_table, use_target_db=False)
                target_count = self.db_ops.get_table_row_count(target_table, use_target_db=True)

                merge_result = {
                    "source_count": source_count,
                    "target_count": target_count,
                    "sync_mode": sync_mode,
                    "execution_time": (end_time - start_time).total_seconds(),
                    "status": "success",
                    "message": (
                        f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                        f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                    ),
                }

                logger.info(merge_result["message"])
                return merge_result

            # MERGE ì¿¼ë¦¬ ìƒì„±
            if sync_mode == "full_sync":
                # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                if non_pk_column_names:
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì…
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
            else:
                # ì¦ë¶„ ë™ê¸°í™”: UPSERT (INSERT ... ON CONFLICT)
                if non_pk_column_names:
                    update_set_clause = ", ".join(
                        [f"{col} = EXCLUDED.{col}" for col in non_pk_column_names]
                    )

                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO UPDATE SET {update_set_clause};
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # ì¦ë¶„ ë™ê¸°í™”: ì¤‘ë³µí‚¤ ìŠ¤í‚µ (ON CONFLICT DO NOTHING)
                if non_pk_column_names:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

            # MERGE ì‹¤í–‰
            start_time = pd.Timestamp.now()
            self.target_hook.run(merge_sql)
            end_time = pd.Timestamp.now()

            # ê²°ê³¼ í™•ì¸
            source_count = self.db_ops.get_table_row_count(source_table, use_target_db=False)
            target_count = self.db_ops.get_table_row_count(target_table, use_target_db=True)

            merge_result = {
                "source_count": source_count,
                "target_count": target_count,
                "sync_mode": sync_mode,
                "execution_time": (end_time - start_time).total_seconds(),
                "status": "success",
                "message": (
                    f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                ),
            }

            logger.info(merge_result["message"])
            return merge_result

        except Exception as e:
            logger.error(
                f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e!s}"
            )
            raise

    def _convert_to_postgres_type(
        self, col_type: str, max_length: int | None = None
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…ì„ PostgreSQL íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        col_type_lower = col_type.lower()

        # ë¬¸ìì—´ íƒ€ì…
        if "char" in col_type_lower or "text" in col_type_lower:
            if max_length and max_length > 0:
                return f"VARCHAR({max_length})"
            else:
                return "TEXT"

        # ìˆ«ì íƒ€ì…
        elif "int" in col_type_lower:
            if "bigint" in col_type_lower:
                return "BIGINT"
            elif "smallint" in col_type_lower:
                return "SMALLINT"
            else:
                return "INTEGER"

        elif "decimal" in col_type_lower or "numeric" in col_type_lower:
            return "NUMERIC"

        elif "float" in col_type_lower or "double" in col_type_lower:
            return "DOUBLE PRECISION"

        elif "real" in col_type_lower:
            return "REAL"

        # ë‚ ì§œ/ì‹œê°„ íƒ€ì…
        elif "date" in col_type_lower:
            return "DATE"

        elif "time" in col_type_lower:
            if "timestamp" in col_type_lower:
                return "TIMESTAMP"
            else:
                return "TIME"

        # ë¶ˆë¦° íƒ€ì…
        elif "bool" in col_type_lower:
            return "BOOLEAN"

        # ê¸°íƒ€ íƒ€ì…
        elif "json" in col_type_lower:
            return "JSONB"

        elif "uuid" in col_type_lower:
            return "UUID"

        # ê¸°ë³¸ê°’ - ì•ˆì „í•˜ê²Œ TEXTë¡œ ì²˜ë¦¬
        else:
            return "TEXT"

    def _create_temp_table_in_session(
        self, cursor, target_table: str, source_schema: dict
    ) -> str:
        """í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±"""
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            timestamp = int(pd.Timestamp.now().timestamp())
            temp_table = f"temp_{schema}_{table}_{timestamp}"

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„± - ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ì‹¤ì œ ë°ì´í„° íƒ€ì… ì‚¬ìš©
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]
                is_nullable = col["nullable"]

                # ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ì‹¤ì œ ë°ì´í„° íƒ€ì…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # PostgreSQLê³¼ í˜¸í™˜ë˜ëŠ” íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                pg_type = self._convert_to_postgres_type(
                    col_type, col.get("max_length")
                )

                # ìˆ«ì íƒ€ì… ì»¬ëŸ¼ì€ NULL í—ˆìš©ìœ¼ë¡œ ì„¤ì • (CSV ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬)
                # DOUBLE PRECISION, BIGINT, INTEGER ë“±ì—ì„œ ë¹ˆ ë¬¸ìì—´ ì˜¤ë¥˜ ë°©ì§€
                if pg_type in ["DOUBLE PRECISION", "BIGINT", "INTEGER", "NUMERIC", "REAL"]:
                    nullable_clause = ""  # NULL í—ˆìš©
                    logger.info(f"ìˆ«ì íƒ€ì… ì»¬ëŸ¼ {col_name} ({pg_type})ì„ NULL í—ˆìš©ìœ¼ë¡œ ì„¤ì •")
                else:
                    # ì›ë³¸ ìŠ¤í‚¤ë§ˆì˜ nullable ì •ë³´ë¥¼ ìœ ì§€
                    nullable_clause = "NOT NULL" if not is_nullable else ""

                column_definitions.append(
                    f"{col_name} {pg_type} {nullable_clause}".strip()
                )

            # CREATE TABLE ë¬¸ ìƒì„± (ì„ì‹œ í…Œì´ë¸” ëŒ€ì‹  ì˜êµ¬ í…Œì´ë¸” ì‚¬ìš©)
            create_sql = f"""
                CREATE TABLE {temp_table} (
                    {', '.join(column_definitions)}
                )
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_sql}")

            # ì„ì‹œ í…Œì´ë¸” ìƒì„±
            cursor.execute(create_sql)
            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

            return temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            raise Exception(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")

    def _import_csv_in_session(
        self, cursor, temp_table: str, csv_path: str, source_schema: dict, batch_size: int = 1000
    ) -> int:
        """í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ CSV ë°ì´í„°ë¥¼ ì„ì‹œ í…Œì´ë¸”ì— ì‚½ì…"""
        try:
            # CSV íŒŒì¼ ì½ê¸° (NA ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ì˜ëª» ì¸ì‹í•˜ì§€ ì•Šë„ë¡ ì„¤ì •)
            df = pd.read_csv(
                csv_path, encoding="utf-8", na_values=[], keep_default_na=False
            )

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            if source_schema and source_schema.get("columns"):
                temp_columns = [col["name"] for col in source_schema["columns"]]
                logger.info(f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {temp_columns}")

                # CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬
                df_reordered = df[temp_columns]
                logger.info(
                    f"CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬í–ˆìŠµë‹ˆë‹¤: {temp_columns}"
                )
            else:
                temp_columns = list(df.columns)
                logger.info(
                    f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì–´ CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤: {temp_columns}"
                )
                df_reordered = df

            # ğŸš¨ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ - CSV ì½ê¸° í›„ ì¬ì‹¤í–‰
            logger.info("=== ğŸš¨ CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘ ===")
            df_reordered = self._validate_and_convert_data_types_after_csv_read(df_reordered, source_schema)
            logger.info("=== CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì™„ë£Œ ===")

            # ë°ì´í„° íƒ€ì… ë³€í™˜ ë° null ê°’ ê²€ì¦
            # NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼ë“¤ í™•ì¸
            not_null_columns = []
            for col in source_schema["columns"]:
                if not col["nullable"]:
                    not_null_columns.append(col["name"])

            logger.info(f"NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼: {not_null_columns}")

            # null ê°’ì´ ìˆëŠ” í–‰ ê²€ì¦ (ì‹¤ì œ null ê°’ë§Œ, "NA" ë¬¸ìì—´ì€ ì œì™¸)
            null_violations = []
            for col_name in not_null_columns:
                if col_name in df_reordered.columns:
                    # ì‹¤ì œ null ê°’ë§Œ ê²€ì‚¬ (None, numpy.nan ë“±)
                    null_rows = df_reordered[df_reordered[col_name].isna()]
                    if not null_rows.empty:
                        null_violations.append(
                            {
                                "column": col_name,
                                "count": len(null_rows),
                                "sample_rows": null_rows.head(3).to_dict("records"),
                            }
                        )

            if null_violations:
                logger.warning(f"NOT NULL ì œì•½ì¡°ê±´ ìœ„ë°˜ ë°œê²¬: {null_violations}")
                # null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œë¡œ ì²˜ë¦¬
                for col_name in not_null_columns:
                    if col_name in df_reordered.columns:
                        df_reordered[col_name] = df_reordered[col_name].fillna("")
                        logger.info(
                            f"ì»¬ëŸ¼ {col_name}ì˜ null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤."
                        )

            # ì¼ë°˜ì ì¸ ë°ì´í„° íƒ€ì… ë³€í™˜
            for col in df_reordered.columns:
                # ì‹¤ì œ null ê°’ë§Œ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜ ("NA" ë¬¸ìì—´ì€ ë³´ì¡´)
                df_reordered[col] = df_reordered[col].fillna("")

                # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                if df_reordered[col].dtype in ["int64", "float64"]:
                    df_reordered[col] = df_reordered[col].astype(str)

                # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
                df_reordered[col] = df_reordered[col].astype(str)

            # ìˆ«ì íƒ€ì… ì»¬ëŸ¼ì˜ ë¹ˆ ë¬¸ìì—´ì„ NULLë¡œ ë³€í™˜
            # PostgreSQLì—ì„œ ë¹ˆ ë¬¸ìì—´ì„ ìˆ«ì íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì˜¤ë¥˜ ë°©ì§€
            logger.info(f"=== ì»¬ëŸ¼ íƒ€ì… ê²€ì‚¬ ì‹œì‘ ===")
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]
                logger.info(f"ì»¬ëŸ¼ {col_name}: íƒ€ì…={col_type}")

                # priority ì»¬ëŸ¼ íŠ¹ë³„ ë¡œê¹…
                if col_name == "priority":
                    logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ ë°œê²¬! íƒ€ì…: {col_type}, BIGINT í¬í•¨ ì—¬ë¶€: {col_type in ['BIGINT', 'INTEGER']}")
                    logger.info(f"ë°ì´í„°í”„ë ˆì„ì— ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€: {col_name in df_reordered.columns}")
                    if col_name in df_reordered.columns:
                        sample_values = df_reordered[col_name].head(5).tolist()
                        logger.info(f"PRIORITY ì»¬ëŸ¼ ìƒ˜í”Œ ê°’: {sample_values}")

                if col_name in df_reordered.columns and col_type in ["DOUBLE PRECISION", "BIGINT", "INTEGER", "NUMERIC", "REAL"]:
                    # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
                    df_reordered[col_name] = df_reordered[col_name].replace({
                        "": None,
                        " ": None,
                        "nan": None,
                        "None": None,
                        "NULL": None,
                        "null": None
                    })

                    # BIGINT/INTEGER ì»¬ëŸ¼ì˜ ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                    logger.info(f"ì»¬ëŸ¼ {col_name} íƒ€ì… ê²€ì‚¬: {col_type} (BIGINT/INTEGER í¬í•¨ ì—¬ë¶€: {col_type in ['BIGINT', 'INTEGER']})")
                    if col_type in ["BIGINT", "INTEGER"]:
                        logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ ì†Œìˆ˜ì  ê°’ ë³€í™˜ ì‹œì‘")
                        try:
                            # ë” ê°•ë ¥í•œ ì†Œìˆ˜ì  ê°’ ë³€í™˜ ë¡œì§
                            def convert_decimal_to_int(x):
                                if pd.isna(x) or x is None:
                                    return None
                                if isinstance(x, str):
                                    x = x.strip()
                                    if not x or x in ["", "nan", "None", "NULL", "null"]:
                                        return None
                                    if "." in x:
                                        try:
                                            # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                                            float_val = float(x)
                                            int_val = int(float_val)
                                            return str(int_val)
                                        except (ValueError, TypeError):
                                            logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                            return x
                                elif isinstance(x, (int, float)):
                                    return str(int(x))
                                return str(x) if x is not None else None

                            # ë³€í™˜ ì „í›„ ê°’ í™•ì¸
                            before_values = df_reordered[col_name].tolist()
                            df_reordered[col_name] = df_reordered[col_name].apply(convert_decimal_to_int)
                            after_values = df_reordered[col_name].tolist()

                            # ë³€í™˜ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                            changed_count = sum(1 for b, a in zip(before_values, after_values) if b != a)
                            if changed_count > 0:
                                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ {changed_count}ê°œ ì†Œìˆ˜ì  ê°’ì´ ì •ìˆ˜ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                # ë³€í™˜ëœ ê°’ ìƒ˜í”Œ ë¡œê¹…
                                for i, (b, a) in enumerate(zip(before_values, after_values)):
                                    if b != a:
                                        logger.info(f"ì»¬ëŸ¼ {col_name} í–‰ {i}: '{b}' â†’ '{a}'")
                                        if i >= 2:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…
                                            break
                            else:
                                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì— ë³€í™˜í•  ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")

                            # ë³€í™˜ í›„ ì¶”ê°€ ê²€ì¦: ì—¬ì „íˆ ì†Œìˆ˜ì ì´ ìˆëŠ” ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                            remaining_decimals = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                            if remaining_decimals > 0:
                                logger.warning(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {remaining_decimals}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
                                # ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤ì„ ë¡œê¹…
                                problem_values = df_reordered[col_name][df_reordered[col_name].astype(str).str.contains(r'\.', na=False)]
                                logger.warning(f"ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤: {problem_values.head(5).tolist()}")

                                # ê°•ì œë¡œ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                                logger.info(f"ì»¬ëŸ¼ {col_name}ì˜ ë‚¨ì€ ì†Œìˆ˜ì  ê°’ë“¤ì„ ê°•ì œ ë³€í™˜í•©ë‹ˆë‹¤.")
                                def force_convert_decimal(x):
                                    if pd.isna(x) or x is None:
                                        return None
                                    if isinstance(x, str) and "." in x:
                                        try:
                                            return str(int(float(x)))
                                        except (ValueError, TypeError):
                                            logger.error(f"ê°•ì œ ë³€í™˜ ì‹¤íŒ¨: '{x}' â†’ NULLë¡œ ì„¤ì •")
                                            return None
                                    return x

                                df_reordered[col_name] = df_reordered[col_name].apply(force_convert_decimal)

                                # ìµœì¢… ê²€ì¦
                                final_decimals = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                                if final_decimals > 0:
                                    logger.error(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {final_decimals}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                                else:
                                    logger.info(f"ì»¬ëŸ¼ {col_name}ì˜ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ì†Œìˆ˜ì  ê°’ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                    # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
                    df_reordered[col_name] = df_reordered[col_name].replace({pd.NA: None, pd.NaT: None})
                    logger.info(f"ìˆ«ì íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ ë¹ˆ ë¬¸ìì—´ê³¼ NaNì„ NULLë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")

            logger.info(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ: {len(df_reordered)}í–‰")

            # ğŸš¨ CSV ì €ì¥ ì „ ìµœì¢… PRIORITY ì»¬ëŸ¼ ê²€ì¦
            logger.info("=== ğŸš¨ CSV ì €ì¥ ì „ ìµœì¢… PRIORITY ì»¬ëŸ¼ ê²€ì¦ ===")
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                if col_name == "priority" and col_type in ["BIGINT", "INTEGER"]:
                    if col_name in df_reordered.columns:
                        # ìµœì¢… ì†Œìˆ˜ì  ê°’ ê²€ì‚¬
                        final_decimal_count = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                        if final_decimal_count > 0:
                            logger.error(f"ğŸš¨ ğŸš¨ ğŸš¨ CSV ì €ì¥ ì „ PRIORITY ì»¬ëŸ¼ì— ì—¬ì „íˆ {final_decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                            # ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤ì„ ê°•ì œë¡œ ë³€í™˜
                            problem_values = df_reordered[col_name][df_reordered[col_name].astype(str).str.contains(r'\.', na=False)]
                            logger.error(f"ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤: {problem_values.head(10).tolist()}")

                            # ê°•ì œ ë³€í™˜
                            def force_convert_final(x):
                                if pd.isna(x) or x is None:
                                    return None
                                if isinstance(x, str) and "." in x:
                                    try:
                                        return str(int(float(x)))
                                    except (ValueError, TypeError):
                                        logger.error(f"ìµœì¢… ê°•ì œ ë³€í™˜ ì‹¤íŒ¨: '{x}' â†’ NULLë¡œ ì„¤ì •")
                                        return None
                                return x

                            df_reordered[col_name] = df_reordered[col_name].apply(force_convert_final)

                            # ìµœì¢… ê²€ì¦
                            final_check = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                            if final_check == 0:
                                logger.info("âœ… ìµœì¢… ê°•ì œ ë³€í™˜ í›„ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            else:
                                logger.error(f"ğŸš¨ ğŸš¨ ğŸš¨ ìµœì¢… ê°•ì œ ë³€í™˜ í›„ì—ë„ {final_check}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                        else:
                            logger.info("âœ… PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤. CSV ì €ì¥ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

            # ë°ì´í„° ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            logger.info("ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3í–‰):")
            for i, row in df_reordered.head(3).iterrows():
                logger.info(f"í–‰ {i}: {dict(row)}")

            # INSERT ì‹¤í–‰ - íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ batch_size ì‚¬ìš©
            total_inserted = 0

            for i in range(0, len(df_reordered), batch_size):
                batch_df = df_reordered.iloc[i : i + batch_size]

                # None ê°’ì„ ì œëŒ€ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°ì´í„° ë³€í™˜
                batch_data = []
                for _, row in batch_df.iterrows():
                    # None ê°’ì„ PostgreSQLì˜ NULLë¡œ ë³€í™˜
                    row_data = []
                    for col_idx, value in enumerate(row):
                        # ì»¬ëŸ¼ëª…ê³¼ íƒ€ì… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        col_name = temp_columns[col_idx]
                        col_type = None
                        for col in source_schema["columns"]:
                            if col["name"] == col_name:
                                col_type = col["type"]
                                break

                        # priority ì»¬ëŸ¼ íŠ¹ë³„ ë¡œê¹… (INSERT ë‹¨ê³„)
                        if col_name == "priority":
                            logger.info(f"ğŸš¨ INSERT ë‹¨ê³„ - PRIORITY ì»¬ëŸ¼: ê°’='{value}', íƒ€ì…='{col_type}'")
                            if isinstance(value, str) and "." in value:
                                logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ ë°œê²¬: '{value}'")

                        # None ê°’ ì²˜ë¦¬
                        if value is None or value == "None" or value == "null" or value == "NULL":
                            row_data.append(None)
                        elif value == "" or value == " " or value == "nan":
                            row_data.append(None)
                        # BIGINT/INTEGER ì»¬ëŸ¼ì˜ ì†Œìˆ˜ì  ê°’ ì²˜ë¦¬
                        elif col_type in ["BIGINT", "INTEGER"] and isinstance(value, str) and "." in value:
                            try:
                                # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                                int_value = int(float(value))
                                row_data.append(str(int_value))
                                logger.debug(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ '{int_value}'ë¡œ ë³€í™˜")
                            except (ValueError, TypeError):
                                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê°’ ì‚¬ìš©
                                row_data.append(value)
                                logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: BIGINT/INTEGER ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ ê²€ì¦
                        elif col_type in ["BIGINT", "INTEGER"] and isinstance(value, str):
                            # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ì„ NULLë¡œ ë³€í™˜
                            if not value.strip():
                                row_data.append(None)
                            # ì†Œìˆ˜ì ì´ ìˆëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ê³  ë³€í™˜
                            elif "." in value:
                                try:
                                    int_value = int(float(value))
                                    row_data.append(str(int_value))
                                    logger.debug(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ '{int_value}'ë¡œ ë³€í™˜ (ì¶”ê°€ ê²€ì¦)")
                                except (ValueError, TypeError):
                                    logger.error(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. NULLë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                                    row_data.append(None)
                            else:
                                row_data.append(value)
                        else:
                            row_data.append(value)
                    batch_data.append(tuple(row_data))

                # COPY ëª…ë ¹ì–´ë¡œ ê³ ì† ì‚½ì… (INSERTë³´ë‹¤ 10-50ë°° ë¹ ë¦„)
                try:
                    # COPY ëª…ë ¹ì–´ ì‹œì‘
                    copy_sql = f"COPY {temp_table} ({', '.join(temp_columns)}) FROM STDIN"
                    
                    # StringIOë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ê°ì²´ ìƒì„±
                    from io import StringIO
                    copy_buffer = StringIO()
                    
                    # ë°ì´í„°ë¥¼ COPY í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë²„í¼ì— ì“°ê¸°
                    for row in batch_data:
                        # ê° í–‰ì„ íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
                        row_str = '\t'.join(str(val) if val is not None else '\\N' for val in row)
                        copy_buffer.write(row_str + '\n')
                    
                    # ë²„í¼ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
                    copy_buffer.seek(0)
                    
                    # COPY ëª…ë ¹ì–´ ì‹¤í–‰
                    cursor.copy_expert(copy_sql, copy_buffer)
                    total_inserted += len(batch_data)

                    if i % 10000 == 0:
                        logger.info(f"COPY ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")
                        
                except Exception as copy_error:
                    logger.warning(f"COPY ëª…ë ¹ì–´ ì‹¤íŒ¨, INSERTë¡œ í´ë°±: {copy_error}")
                    # COPY ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ INSERT ë°©ì‹ìœ¼ë¡œ í´ë°±
                    insert_query = f"""
                        INSERT INTO {temp_table} ({', '.join(temp_columns)})
                        VALUES ({', '.join(['%s'] * len(temp_columns))})
                    """
                    cursor.executemany(insert_query, batch_data)
                    total_inserted += len(batch_data)

                    if i % 10000 == 0:
                        logger.info(f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")

            logger.info(
                f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}"
            )
            return total_inserted

        except Exception as e:
            logger.error(f"CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise Exception(f"CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    def _execute_merge_in_session(
        self,
        cursor,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
    ) -> dict:
        """í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ MERGE ì‘ì—… ì‹¤í–‰"""
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            cursor.execute(all_columns_query)
            non_pk_columns = cursor.fetchall()
            non_pk_column_names = [col[0] for col in non_pk_columns]

            # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not non_pk_column_names:
                logger.warning(
                    "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                )
                # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if sync_mode == "full_sync":
                    merge_sql = f"""
                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};
                    """
                else:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # MERGE ì‹¤í–‰
                start_time = pd.Timestamp.now()
                cursor.execute(merge_sql)
                end_time = pd.Timestamp.now()

                # ê²°ê³¼ í™•ì¸
                cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                source_count = cursor.fetchone()[0]

                cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                target_count = cursor.fetchone()[0]

                merge_result = {
                    "source_count": source_count,
                    "target_count": target_count,
                    "sync_mode": sync_mode,
                    "execution_time": (end_time - start_time).total_seconds(),
                    "status": "success",
                    "message": (
                        f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                        f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                    ),
                }

                logger.info(merge_result["message"])
                return merge_result

            # MERGE ì¿¼ë¦¬ ìƒì„±
            if sync_mode == "full_sync":
                # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                if non_pk_column_names:
                    merge_sql = f"""
                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì…
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table};
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};
                    """
            else:
                # ì¦ë¶„ ë™ê¸°í™”: ì¤‘ë³µí‚¤ ìŠ¤í‚µ (ON CONFLICT DO NOTHING)
                if non_pk_column_names:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

            # MERGE ì‹¤í–‰
            start_time = pd.Timestamp.now()
            cursor.execute(merge_sql)
            end_time = pd.Timestamp.now()

            # ê²°ê³¼ í™•ì¸
            cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
            source_count = cursor.fetchone()[0]

            cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
            target_count = cursor.fetchone()[0]

            merge_result = {
                "source_count": source_count,
                "target_count": target_count,
                "sync_mode": sync_mode,
                "execution_time": (end_time - start_time).total_seconds(),
                "status": "success",
                "message": (
                    f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                ),
            }

            logger.info(merge_result["message"])
            return merge_result

        except Exception as e:
            logger.error(
                f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e!s}"
            )
            raise

    def _build_where_clause(
        self,
        custom_where: str | None = None,
        where_clause: str | None = None,
        sync_mode: str = "full_sync",
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        target_table: str | None = None,
    ) -> str | None:
        """
        ìµœì¢… WHERE ì¡°ê±´ êµ¬ì„±

        Args:
            custom_where: ì»¤ìŠ¤í…€ WHERE ì¡°ê±´
            where_clause: ê¸°ë³¸ WHERE ì ˆ ì¡°ê±´
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…

        Returns:
            êµ¬ì„±ëœ WHERE ì¡°ê±´ ë¬¸ìì—´
        """
        conditions = []

        # 1. custom_whereê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš©
        if custom_where:
            conditions.append(f"({custom_where})")
            logger.info(f"ì»¤ìŠ¤í…€ WHERE ì¡°ê±´ ì¶”ê°€: {custom_where}")

        # 2. ì¦ë¶„ ë™ê¸°í™” ëª¨ë“œì¼ ë•Œ incremental_field ì¡°ê±´ ì¶”ê°€
        if sync_mode == "incremental_sync" and incremental_field and target_table:
            incremental_condition = self._build_incremental_condition(
                incremental_field, incremental_field_type, target_table
            )
            if incremental_condition:
                conditions.append(f"({incremental_condition})")
                logger.info(f"ì¦ë¶„ ì¡°ê±´ ì¶”ê°€: {incremental_condition}")

        # 3. ê¸°ë³¸ where_clauseê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if where_clause:
            conditions.append(f"({where_clause})")
            logger.info(f"ê¸°ë³¸ WHERE ì¡°ê±´ ì¶”ê°€: {where_clause}")

        # 4. ëª¨ë“  ì¡°ê±´ì„ ANDë¡œ ê²°í•©
        if conditions:
            final_where = " AND ".join(conditions)
            logger.info(f"ìµœì¢… WHERE ì¡°ê±´ êµ¬ì„±: {final_where}")
            return final_where
        else:
            logger.info("WHERE ì¡°ê±´ ì—†ìŒ - ì „ì²´ ë°ì´í„° ì²˜ë¦¬")
            return None

    def _build_incremental_condition(
        self,
        incremental_field: str,
        incremental_field_type: str | None,
        target_table: str,
    ) -> str | None:
        """
        ì¦ë¶„ ë™ê¸°í™”ë¥¼ ìœ„í•œ WHERE ì¡°ê±´ êµ¬ì„±

        Args:
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…

        Returns:
            ì¦ë¶„ ì¡°ê±´ ë¬¸ìì—´
        """
        try:
            # íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¡°íšŒ
            last_update_query = f"""
                SELECT MAX({incremental_field})
                FROM {target_table}
                WHERE {incremental_field} IS NOT NULL
            """

            last_update_result = self.target_hook.get_first(last_update_query)
            last_update_time = last_update_result[0] if last_update_result and last_update_result[0] else None

            if not last_update_time:
                logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” {target_table}ì— ì¦ë¶„ í•„ë“œ {incremental_field} ë°ì´í„°ê°€ ì—†ìŒ - ì „ì²´ ë°ì´í„° ì²˜ë¦¬")
                return None

            # í•„ë“œ íƒ€ì…ì— ë”°ë¥¸ ì¡°ê±´ êµ¬ì„±
            if incremental_field_type == "yyyymmdd":
                # YYYYMMDD í˜•ì‹ (ì˜ˆ: 20250812) - ì´ìƒ(>=) ì¡°ê±´ ì‚¬ìš©
                # ë‚ ì§œ í˜•ì‹ì—ì„œëŠ” ì •í™•í•œ ì‹œì ì„ ì¡ê¸° ì–´ë ¤ìš°ë¯€ë¡œ >= ì‚¬ìš©
                condition = f"{incremental_field} >= '{last_update_time}'"
                logger.info(f"YYYYMMDD í˜•ì‹ ê°ì§€ - ì´ìƒ(>=) ì¡°ê±´ ì‚¬ìš©: {condition}")
            elif incremental_field_type == "timestamp":
                # TIMESTAMP í˜•ì‹ - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš© (ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì •ë°€ë„)
                condition = f"{incremental_field} > '{last_update_time}'"
            elif incremental_field_type == "date":
                # DATE í˜•ì‹ - ì´ìƒ(>=) ì¡°ê±´ ì‚¬ìš© (ë‚ ì§œ ë‹¨ìœ„)
                condition = f"{incremental_field} >= '{last_update_time}'"
            elif incremental_field_type == "datetime":
                # DATETIME í˜•ì‹ - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš© (ì´ˆ ë‹¨ìœ„ ì •ë°€ë„)
                condition = f"{incremental_field} > '{last_update_time}'"
            elif incremental_field_type == "integer":
                # ì •ìˆ˜í˜• (ì˜ˆ: Unix timestamp) - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš©
                condition = f"{incremental_field} > {last_update_time}"
            else:
                # ê¸°ë³¸ê°’: ë¬¸ìì—´ ë¹„êµ - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš©
                condition = f"{incremental_field} > '{last_update_time}'"

            logger.info(f"ì¦ë¶„ ì¡°ê±´ êµ¬ì„±: {condition} (ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update_time})")
            return condition

        except Exception as e:
            logger.warning(f"ì¦ë¶„ ì¡°ê±´ êµ¬ì„± ì‹¤íŒ¨: {e!s} - ì¦ë¶„ ì¡°ê±´ ì—†ì´ ì§„í–‰")
            return None

    def copy_table_data(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        batch_size: int = 10000,
        custom_where: str | None = None,
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        # ì²­í¬ ë°©ì‹ íŒŒë¼ë¯¸í„° ì¶”ê°€
        chunk_mode: bool = True,
        enable_checkpoint: bool = True,
        max_retries: int = 3,
    ) -> dict[str, Any]:
        """
        í…Œì´ë¸” ë°ì´í„° ë³µì‚¬ (ê¸°ë³¸ ë©”ì„œë“œ)
        
        sync_mode:
            - full_sync: ì „ì²´ ë™ê¸°í™”
            - incremental_sync: íƒ€ì„ìŠ¤íƒ¬í”„/ë¹„ì¦ˆë‹ˆìŠ¤ í•„ë“œ ê¸°ë°˜ ì¦ë¶„
            - xmin_incremental: PostgreSQL xmin ê¸°ë°˜ ì¦ë¶„
        """
        # xmin ëª¨ë“œë©´ ì „ìš© íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (source_xmin ì €ì¥/ë¨¸ì§€ í¬í•¨)
        if sync_mode == "xmin_incremental":
            return self.copy_table_data_with_xmin(
                source_table=source_table,
                target_table=target_table,
                primary_keys=primary_keys,
                sync_mode=sync_mode,
                batch_size=batch_size,
                custom_where=custom_where,
            )

        return self._copy_table_data_internal(
            source_table=source_table,
            target_table=target_table,
            primary_keys=primary_keys,
            sync_mode=sync_mode,
            batch_size=batch_size,
            custom_where=custom_where,
            incremental_field=incremental_field,
            incremental_field_type=incremental_field_type,
        )

    def copy_table_data_with_custom_sql(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        batch_size: int = 10000,
        custom_where: str | None = None,
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        count_sql: str | None = None,
        select_sql: str | None = None,
        # ì²­í¬ ë°©ì‹ íŒŒë¼ë¯¸í„° ì¶”ê°€
        chunk_mode: bool = True,
        enable_checkpoint: bool = True,
        max_retries: int = 3,
    ) -> dict[str, Any]:
        """
        ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ë°ì´í„° ë³µì‚¬

        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            custom_where: ì»¤ìŠ¤í…€ WHERE ì¡°ê±´
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            count_sql: ì‚¬ìš©ì ì •ì˜ COUNT SQL
            select_sql: ì‚¬ìš©ì ì •ì˜ SELECT SQL

        Returns:
            ë³µì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # xmin ëª¨ë“œë©´ ì „ìš© íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¼ìš°íŒ… (ì‚¬ìš©ì ì •ì˜ SELECTëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ)
        if sync_mode == "xmin_incremental":
            return self.copy_table_data_with_xmin(
                source_table=source_table,
                target_table=target_table,
                primary_keys=primary_keys,
                sync_mode=sync_mode,
                batch_size=batch_size,
                custom_where=custom_where,
            )

        # ì‚¬ìš©ì ì •ì˜ SQLì´ ì œê³µëœ ê²½ìš° ì´ë¥¼ ì‚¬ìš©
        if count_sql and select_sql:
            return self._copy_table_data_with_custom_sql_internal(
                source_table=source_table,
                target_table=target_table,
                primary_keys=primary_keys,
                sync_mode=sync_mode,
                batch_size=batch_size,
                custom_where=custom_where,
                incremental_field=incremental_field,
                incremental_field_type=incremental_field_type,
                count_sql=count_sql,
                select_sql=select_sql,
            )
        else:
            # ê¸°ë³¸ ë©”ì„œë“œ ì‚¬ìš©
            return self._copy_table_data_internal(
                source_table=source_table,
                target_table=target_table,
                primary_keys=primary_keys,
                sync_mode=sync_mode,
                batch_size=batch_size,
                custom_where=custom_where,
                incremental_field=incremental_field,
                incremental_field_type=incremental_field_type,
            )

    def _copy_table_data_internal(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        batch_size: int,
        custom_where: str | None,
        incremental_field: str | None,
        incremental_field_type: str | None,
    ) -> dict[str, Any]:
        """
        í…Œì´ë¸” ë°ì´í„° ë³µì‚¬ (ë‚´ë¶€ ë©”ì„œë“œ)
        """
        csv_path = None
        temp_table = None

        try:
            start_time = pd.Timestamp.now()

            # WHERE ì¡°ê±´ êµ¬ì„± (custom_where ìš°ì„ , where_clauseëŠ” ê¸°ë³¸ê°’)
            final_where_clause = self._build_where_clause(
                custom_where=custom_where,
                where_clause=None,
                sync_mode=sync_mode,
                incremental_field=incremental_field,
                incremental_field_type=incremental_field_type,
                target_table=target_table
            )

            logger.info(f"ìµœì¢… WHERE ì¡°ê±´: {final_where_clause}")

            # 1. ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹œì‘: {source_table}")
            source_schema = self.db_ops.get_table_schema(source_table)

            if not source_schema or not source_schema.get("columns"):
                raise Exception(
                    f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            # 2. CSVë¡œ ë‚´ë³´ë‚´ê¸° (ì†ŒìŠ¤ DBì—ì„œ - ë‹¤ë¥¸ ì„¸ì…˜)
            csv_path = os.path.join(
                self.temp_dir, f"temp_{os.path.basename(source_table)}.csv"
            )
            logger.info(f"CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘: {source_table} -> {csv_path}")
            exported_rows = self.export_to_csv(
                source_table, csv_path, final_where_clause, batch_size, incremental_field
            )

            if exported_rows == 0:
                return {
                    "status": "warning",
                    "message": f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì— ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. WHERE: {final_where_clause}",
                    "execution_time": 0,
                }

            # 3. í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±, ë°ì´í„° ì‚½ì…, MERGE ì‘ì—… ìˆ˜í–‰ (ì„¸ì…˜ ê²©ë¦¬ ë¬¸ì œ í•´ê²°)
            logger.info(
                f"íƒ€ê²Ÿ DBì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±, ë°ì´í„° ì‚½ì…, MERGE ì‘ì—… ì‹œì‘: {target_table}"
            )

            # íƒ€ê²Ÿ DBì—ì„œ í•˜ë‚˜ì˜ ì—°ê²°ë¡œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰
            with self.target_hook.get_conn() as target_conn:
                with target_conn.cursor() as cursor:
                    # 3-1. ì„ì‹œ í…Œì´ë¸” ìƒì„± (ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜)
                    temp_table = self._create_temp_table_in_session(
                        cursor, target_table, source_schema
                    )
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # 3-2. CSV ë°ì´í„° ì‚½ì…
                    imported_rows = self._import_csv_in_session(
                        cursor, temp_table, csv_path, source_schema, batch_size
                    )
                    logger.info(
                        f"CSV ë°ì´í„° ì‚½ì… ì™„ë£Œ: {temp_table}, í–‰ ìˆ˜: {imported_rows}"
                    )

                    # 3-3. MERGE ì‘ì—… ì‹¤í–‰ (ê°™ì€ ì—°ê²°ì—ì„œ)
                    merge_result = self._execute_merge_in_session(
                        cursor, temp_table, target_table, primary_keys, sync_mode
                    )
                    logger.info(f"MERGE ì‘ì—… ì™„ë£Œ: {temp_table} -> {target_table}")

                # ëª¨ë“  ì‘ì—…ì´ ì„±ê³µí•˜ë©´ ì»¤ë°‹
                target_conn.commit()
                logger.info("ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì»¤ë°‹ë˜ì—ˆìŠµë‹ˆë‹¤.")

            end_time = pd.Timestamp.now()
            total_time = (end_time - start_time).total_seconds()

            # 5. ê²°ê³¼ ì •ë¦¬
            final_result = {
                "status": "success",
                "source_table": source_table,
                "target_table": target_table,
                "exported_rows": exported_rows,
                "imported_rows": imported_rows,
                "merge_result": merge_result,
                "total_execution_time": total_time,
                "where_clause_used": final_where_clause,
                "sync_mode": sync_mode,
                "message": (
                    f"í…Œì´ë¸” ë³µì‚¬ ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"WHERE: {final_where_clause}, "
                    f"ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ"
                ),
            }

            logger.info(final_result["message"])
            return final_result

        except Exception as e:
            error_result = {
                "status": "error",
                "source_table": source_table,
                "target_table": target_table,
                "error": str(e),
                "message": (
                    f"í…Œì´ë¸” ë³µì‚¬ ì‹¤íŒ¨: {source_table} -> {target_table}, "
                    f"ì˜¤ë¥˜: {e!s}"
                ),
            }

            logger.error(error_result["message"])
            return error_result

        finally:
            # 6. ì •ë¦¬ ì‘ì—…
            if csv_path:
                self.cleanup_temp_files(csv_path)

            if temp_table:
                try:
                    # ì„ì‹œ í…Œì´ë¸”ì€ ìë™ìœ¼ë¡œ ì‚­ì œë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ
                    self.target_hook.run(f"DROP TABLE IF EXISTS {temp_table}")
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ì •ë¦¬ ì™„ë£Œ: {temp_table}")
                except Exception as e:
                    logger.warning(f"ì„ì‹œ í…Œì´ë¸” ì •ë¦¬ ì‹¤íŒ¨: {temp_table}, ì˜¤ë¥˜: {e!s}")

    def _copy_table_data_with_custom_sql_internal(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        batch_size: int,
        custom_where: str | None,
        incremental_field: str | None,
        incremental_field_type: str | None,
        count_sql: str,
        select_sql: str,
    ) -> dict[str, Any]:
        """
        ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ë°ì´í„° ë³µì‚¬ (ë‚´ë¶€ ë©”ì„œë“œ)
        """
        csv_path = None
        start_time = time.time()

        try:
            # 1ë‹¨ê³„: ë°ì´í„° ë‚´ë³´ë‚´ê¸° (ì‚¬ìš©ì ì •ì˜ SQL ì‚¬ìš©)
            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹œì‘: {source_table}")

            # ì „ì²´ í–‰ ìˆ˜ ì¡°íšŒ (ì‚¬ìš©ì ì •ì˜ COUNT SQL ì‚¬ìš©)
            if count_sql:
                total_count = self.source_hook.get_first(count_sql)[0]
                logger.info(f"ì‚¬ìš©ì ì •ì˜ COUNT SQLë¡œ ì¡°íšŒëœ í–‰ ìˆ˜: {total_count}")
            else:
                total_count = self.db_ops.get_table_row_count(source_table, where_clause=custom_where)

            if total_count == 0:
                logger.warning(f"í…Œì´ë¸” {source_table}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": 0,
                    "message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # CSV íŒŒì¼ ê²½ë¡œ ìƒì„±
            csv_path = f"/tmp/temp_{source_table.replace('.', '_')}.csv"

            # ì‚¬ìš©ì ì •ì˜ SELECT SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            if select_sql:
                logger.info(f"ì‚¬ìš©ì ì •ì˜ SELECT SQL ì‚¬ìš©: {select_sql}")
                exported_rows = self._export_with_custom_sql(select_sql, csv_path, batch_size)
            else:
                # ê¸°ë³¸ ë‚´ë³´ë‚´ê¸° ì‚¬ìš©
                exported_rows = self.export_to_csv(
                    source_table, csv_path, custom_where, batch_size, incremental_field
                )

            if exported_rows == 0:
                logger.warning(f"ë‚´ë³´ë‚¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": time.time() - start_time,
                    "message": "ë‚´ë³´ë‚¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # 2ë‹¨ê³„: ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            logger.info(f"íƒ€ê²Ÿ DBì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±, ë°ì´í„° ì‚½ì…, MERGE ì‘ì—… ì‹œì‘: {target_table}")
            imported_rows = self._import_and_merge_data(
                csv_path, target_table, primary_keys, sync_mode
            )

            # 3ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬
            total_time = time.time() - start_time

            result = {
                "status": "success",
                "exported_rows": exported_rows,
                "imported_rows": imported_rows,
                "total_execution_time": total_time,
                "message": f"ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {exported_rows}í–‰ ë‚´ë³´ë‚´ê¸°, {imported_rows}í–‰ ê°€ì ¸ì˜¤ê¸°"
            }

            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {result}")
            return result

        except Exception as e:
            total_time = time.time() - start_time
            error_msg = f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)

            result = {
                "status": "error",
                "exported_rows": 0,
                "imported_rows": 0,
                "total_execution_time": total_time,
                "error": error_msg
            }

            return result

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if csv_path and os.path.exists(csv_path):
                try:
                    os.remove(csv_path)
                    logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {csv_path}")
                except Exception as e:
                    logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def _export_with_custom_sql(self, select_sql: str, csv_path: str, batch_size: int, source_schema: dict[str, Any] = None) -> int:
        """
        ì‚¬ìš©ì ì •ì˜ SELECT SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            select_sql: SELECT SQL ì¿¼ë¦¬
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì»¬ëŸ¼ëª… ì¶”ì¶œìš©)
        """
        try:
            # ì‚¬ìš©ì ì •ì˜ SQLë¡œ ë°ì´í„° ì¶”ì¶œ
            all_data = []
            offset = 0

            while True:
                # LIMITì™€ OFFSETì„ ì¶”ê°€í•œ SQL ìƒì„±
                paginated_sql = f"{select_sql} LIMIT {batch_size} OFFSET {offset}"
                logger.info(f"í˜ì´ì§€ë„¤ì´ì…˜ SQL ì‹¤í–‰: OFFSET {offset}")

                batch_data = self.source_hook.get_records(paginated_sql)

                if not batch_data:
                    break

                all_data.extend(batch_data)
                offset += batch_size

                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {len(all_data)}í–‰ ìˆ˜ì§‘")

                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if len(batch_data) < batch_size:
                    break

            if not all_data:
                logger.warning("ì‚¬ìš©ì ì •ì˜ SQLë¡œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # DataFrameìœ¼ë¡œ ë³€í™˜ (ì»¬ëŸ¼ëª… ëª…ì‹œ)
            if source_schema and source_schema.get("columns"):
                # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                column_names = [col["name"] for col in source_schema["columns"]]
            else:
                # ê¸°ë³¸ ì»¬ëŸ¼ëª… ì‚¬ìš©
                column_names = [f"col_{i}" for i in range(len(all_data[0]) if all_data else 0)]

            df = pd.DataFrame(all_data, columns=column_names)

            # CSVë¡œ ì €ì¥ (ì»¬ëŸ¼ëª… í¬í•¨)
            df.to_csv(csv_path, index=False, header=True)
            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLë¡œ CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {csv_path}, í–‰ ìˆ˜: {len(all_data)}")

            return len(all_data)

        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì •ì˜ SQLë¡œ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _import_and_merge_data(self, csv_path: str, target_table: str, primary_keys: list[str], sync_mode: str) -> int:
        """
        ì„ì‹œ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  MERGE ì‘ì—… ìˆ˜í–‰ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            # 1ë‹¨ê³„: íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            target_schema = self.db_ops.get_table_schema(target_table)

            if not target_schema or not target_schema.get("columns"):
                raise Exception(f"íƒ€ê²Ÿ í…Œì´ë¸” {target_table}ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2ë‹¨ê³„: ì„ì‹œ í…Œì´ë¸” ìƒì„± (ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            temp_table = self.create_temp_table(target_table, target_schema)

            # 3ë‹¨ê³„: CSVì—ì„œ ì„ì‹œ í…Œì´ë¸”ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            imported_rows = self.import_from_csv(csv_path, temp_table, target_schema)

            if imported_rows == 0:
                logger.warning(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return 0

            # 4ë‹¨ê³„: MERGE ì‘ì—… ìˆ˜í–‰ (temp_tableì„ ì†ŒìŠ¤ë¡œ ì‚¬ìš©)
            merge_result = self.execute_merge_operation(
                temp_table, target_table, primary_keys, sync_mode
            )

            if merge_result["status"] != "success":
                logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {merge_result['message']}")
                return 0

            # 5ë‹¨ê³„: MERGE ì™„ë£Œ í›„ ANALYZE ì‹¤í–‰í•˜ì—¬ í†µê³„ ìµœì‹ í™”
            try:
                self._analyze_target_table(target_table)
            except Exception as e:
                logger.warning(f"ANALYZE ì‹¤í–‰ ì‹¤íŒ¨: {e}")

            # 6ë‹¨ê³„: MERGE í‚¤ì— ëŒ€í•œ ì¸ë±ìŠ¤ ì ê²€ ë° ìƒì„±
            try:
                self._check_and_create_indexes(target_table, primary_keys)
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ ì ê²€ ë° ìƒì„± ì‹¤íŒ¨: {e}")

            # 7ë‹¨ê³„: ìŠ¤í…Œì´ì§• í…Œì´ë¸” ì •ë¦¬
            try:
                with self.target_hook.get_conn() as conn:
                    with conn.cursor() as cursor:
                        cursor.execute(f"DROP TABLE IF EXISTS {temp_table}")
                        conn.commit()
                logger.info(f"ìŠ¤í…Œì´ì§• í…Œì´ë¸” ì •ë¦¬ ì™„ë£Œ: {temp_table}")
            except Exception as e:
                logger.warning(f"ìŠ¤í…Œì´ì§• í…Œì´ë¸” ì •ë¦¬ ì‹¤íŒ¨: {temp_table}, ì˜¤ë¥˜: {e}")

            return imported_rows

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE ì‘ì—… ì‹¤íŒ¨: {e}")
            return 0

    def create_target_table_and_verify_schema(
        self, target_table: str, source_schema: dict[str, Any]
    ) -> dict[str, Any]:
        """
        íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„±ê³¼ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ í•œ ë²ˆì— ì²˜ë¦¬

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´

        Returns:
            ê²€ì¦ëœ íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
        """
        try:
            logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘: {target_table}")

            # DatabaseOperationsì˜ ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
            verified_schema = self.db_ops.create_table_and_verify_schema(
                target_table, source_schema
            )

            logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ: {target_table}")
            return verified_schema

        except Exception as e:
            logger.error(f"íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {target_table}, ì˜¤ë¥˜: {e}")
            raise

    def copy_data_with_custom_sql(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        custom_where: str | None = None,
        batch_size: int = 10000,
        verified_target_schema: dict[str, Any] | None = None,
        # ì²­í¬/ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨ íŒŒë¼ë¯¸í„° (DAG í˜¸í™˜ì„± ìœ„í•´ ìˆ˜ìš©)
        chunk_mode: bool = True,
        enable_checkpoint: bool = True,
        max_retries: int = 3,
    ) -> dict[str, Any]:
        """
        ê²€ì¦ëœ íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ SQLë¡œ ë°ì´í„° ë³µì‚¬

        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ëª©ë¡
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            custom_where: ì‚¬ìš©ì ì •ì˜ WHERE ì¡°ê±´
            batch_size: ë°°ì¹˜ í¬ê¸°
            verified_target_schema: ê²€ì¦ëœ íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
            chunk_mode: ì²­í¬ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ (í˜¸í™˜ì„±ìš©; í˜„ì¬ í•¨ìˆ˜ëŠ” í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ì²˜ë¦¬)
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì—¬ë¶€ (í˜¸í™˜ì„±ìš©)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (í˜¸í™˜ì„±ìš©)

        Returns:
            ë³µì‚¬ ê²°ê³¼
        """
        try:
            start_time = time.time()

            # 1ë‹¨ê³„: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            source_schema = self.db_ops.get_table_schema(source_table)

            # 2ë‹¨ê³„: ì‚¬ìš©ì ì •ì˜ SQL ìƒì„±
            count_sql, select_sql = self._build_custom_sql_queries(
                source_table, source_schema, incremental_field,
                incremental_field_type, custom_where
            )

            # 3ë‹¨ê³„: ë°ì´í„° ê°œìˆ˜ í™•ì¸
            row_count = self._get_source_row_count(count_sql)

            if row_count == 0:
                logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì— ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": time.time() - start_time,
                    "message": "ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŒ"
                }

            # 4ë‹¨ê³„: CSV íŒŒì¼ë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
            csv_path = f"/tmp/temp_{source_table.replace('.', '_')}.csv"
            exported_rows = self._export_with_custom_sql(select_sql, csv_path, batch_size, source_schema)

            if exported_rows == 0:
                raise Exception("ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨")

            # 5ë‹¨ê³„: ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE (ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            if verified_target_schema:
                # ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
                imported_rows = self._import_and_merge_with_verified_schema(
                    csv_path, target_table, primary_keys, sync_mode, verified_target_schema
                )
            else:
                # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                imported_rows = self._import_and_merge_data(
                    csv_path, target_table, primary_keys, sync_mode
                )

            total_time = time.time() - start_time

            result = {
                "status": "success",
                "exported_rows": exported_rows,
                "imported_rows": imported_rows,
                "total_execution_time": total_time,
                "message": f"ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {exported_rows}í–‰ ë‚´ë³´ë‚´ê¸°, {imported_rows}í–‰ ê°€ì ¸ì˜¤ê¸°"
            }

            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {result}")
            return result

        except Exception as e:
            total_time = time.time() - start_time
            error_msg = f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)

            result = {
                "status": "error",
                "exported_rows": 0,
                "imported_rows": 0,
                "total_execution_time": total_time,
                "error": error_msg
            }

            return result

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if 'csv_path' in locals() and csv_path and os.path.exists(csv_path):
                try:
                    os.remove(csv_path)
                    logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {csv_path}")
                except Exception as e:
                    logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def _build_custom_sql_queries(
        self,
        source_table: str,
        source_schema: dict[str, Any],
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        custom_where: str | None = None
    ) -> tuple[str, str]:
        """ì‚¬ìš©ì ì •ì˜ SQL ì¿¼ë¦¬ ìƒì„±"""
        try:
            # ì›ë³¸ ì»¬ëŸ¼ëª… ì‚¬ìš© (ë³€í™˜ ì—†ì´)
            column_names = [col["name"] for col in source_schema["columns"]]
            transformed_columns = [f'"{col_name}"' for col_name in column_names]

            # ê¸°ë³¸ WHERE ì¡°ê±´
            where_conditions = []
            if custom_where:
                where_conditions.append(custom_where)
            # custom_whereê°€ ì´ë¯¸ ì¦ë¶„ ì¡°ê±´ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            elif incremental_field and incremental_field_type:
                # custom_whereê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¦ë¶„ ë™ê¸°í™” ì¡°ê±´ ì¶”ê°€
                if incremental_field_type == "yyyymmdd":
                    where_conditions.append(f"{incremental_field} >= '20250812'")
                # ë‹¤ë¥¸ ì¦ë¶„ í•„ë“œ íƒ€ì…ë“¤ë„ ì¶”ê°€ ê°€ëŠ¥

            where_clause = " AND ".join(where_conditions) if where_conditions else ""
            where_sql = f" WHERE {where_clause}" if where_clause else ""

            # COUNT ì¿¼ë¦¬
            count_sql = f'SELECT COUNT(*) FROM {source_table}{where_sql}'

            # SELECT ì¿¼ë¦¬
            select_sql = f'SELECT {", ".join(transformed_columns)} FROM {source_table}{where_sql} ORDER BY {incremental_field or "1"}'

            return count_sql, select_sql

        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì •ì˜ SQL ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _apply_column_transformation(self, column_name: str, data_type: str) -> str:
        """ì»¬ëŸ¼ë³„ ë°ì´í„° ë³€í™˜ ì ìš©"""
        # ê°„ë‹¨í•œ ë³€í™˜ ë¡œì§ (í•„ìš”ì— ë”°ë¼ í™•ì¥ ê°€ëŠ¥)
        if data_type.lower() in ["bigint", "integer", "smallint"]:
            return f"""
                CASE
                    WHEN "{column_name}" IS NULL THEN NULL
                    WHEN "{column_name}"::TEXT = '' THEN NULL
                    WHEN "{column_name}"::TEXT ~ '^[0-9]+\.[0]+$' THEN CAST("{column_name}" AS BIGINT)::TEXT
                    ELSE "{column_name}"::TEXT
                END AS "{column_name}"
            """
        elif data_type.lower() in ["text", "character varying", "varchar"]:
            return f"""
                CASE
                    WHEN "{column_name}" IS NULL THEN NULL
                    WHEN "{column_name}" = '' THEN NULL
                    WHEN "{column_name}" ~ '^[0-9]+$' THEN CAST("{column_name}" AS BIGINT)::TEXT
                    WHEN "{column_name}" ~ '^[0-9]{{8}}$' AND "{column_name}" ~ '^(19|20)[0-9]{{6}}$'
                        THEN TO_CHAR(TO_DATE("{column_name}", 'YYYYMMDD'), 'YYYY-MM-DD')
                    ELSE "{column_name}"
                END AS "{column_name}"
            """
        else:
            return f'"{column_name}"'

    def _get_source_row_count(self, count_sql: str) -> int:
        """ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ì¡°ê±´ì— ë§ëŠ” í–‰ ìˆ˜ ì¡°íšŒ"""
        try:
            result = self.source_hook.get_first(count_sql)
            return result[0] if result else 0
        except Exception as e:
            logger.error(f"ì†ŒìŠ¤ í…Œì´ë¸” í–‰ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0

    def _import_and_merge_with_verified_schema(
        self,
        csv_path: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        verified_schema: dict[str, Any]
    ) -> int:
        """ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE"""
        try:
            # í•˜ë‚˜ì˜ ì—°ê²°ì—ì„œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # 1ë‹¨ê³„: ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    imported_rows, temp_table = self._create_temp_table_and_import_csv_in_connection(
                        cursor, target_table, verified_schema, csv_path
                    )

                    if imported_rows == 0:
                        logger.warning(f"ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        return 0

                    # 2ë‹¨ê³„: ê°™ì€ ì—°ê²°ì—ì„œ MERGE ì‘ì—… ìˆ˜í–‰
                    merge_result = self._execute_merge_operation_with_cursor(
                        cursor, temp_table, target_table, primary_keys, sync_mode, verified_schema
                    )

                    if merge_result["status"] != "success":
                        logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {merge_result['message']}")
                        return 0

                    # ì»¤ë°‹
                    conn.commit()
                    return imported_rows

        except Exception as e:
            logger.error(f"ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE ì‹¤íŒ¨: {e}")
            return 0

    def _create_temp_table_and_import_csv(
        self,
        target_table: str,
        source_schema: dict[str, Any],
        csv_path: str,
        batch_size: int = 1000
    ) -> tuple[int, str]:
        """
        ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸°ë¥¼ í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ê°€ì ¸ì˜¨ í–‰ ìˆ˜
        """
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_path, encoding="utf-8")

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„± (ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì— ë”°ë¼ ì²˜ë¦¬)
            column_definitions = []
            
            # ìŠ¤í‚¤ë§ˆê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸
            if isinstance(source_schema, list):
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ: [{"column_name": "...", "data_type": "...", ...}]
                for col in source_schema:
                    col_name = col["column_name"]
                    col_type = col["data_type"]
                    col_type = self._convert_to_postgres_type(col_type, col.get('character_maximum_length'))
                    nullable = "" if col["is_nullable"] == "YES" else " NOT NULL"
                    column_definitions.append(f'"{col_name}" {col_type}{nullable}')
            elif isinstance(source_schema, dict) and "columns" in source_schema:
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ: {"columns": [{"name": "...", "type": "...", ...}]}
                for col in source_schema["columns"]:
                    col_name = col["name"]
                    col_type = col["type"]
                    col_type = self._convert_to_postgres_type(col_type, col.get('max_length'))
                    nullable = "" if col["nullable"] else " NOT NULL"
                    column_definitions.append(f"{col_name} {col_type}{nullable}")
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤í‚¤ë§ˆ êµ¬ì¡°: {type(source_schema)}")

            # ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL
            create_temp_table_sql = f"""
                CREATE TEMP TABLE {temp_table} (
                    {', '.join(column_definitions)}
                ) ON COMMIT DROP
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_temp_table_sql}")

            # ê°™ì€ ì—°ê²°ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # 1. ì„ì‹œ í…Œì´ë¸” ìƒì„±
                    cursor.execute(create_temp_table_sql)
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # 2. ì„ì‹œ í…Œì´ë¸”ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    cursor.execute(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{temp_table}')")
                    exists = cursor.fetchone()[0]
                    if not exists:
                        raise Exception(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                    # 3. CSV ë°ì´í„°ë¥¼ ì„ì‹œ í…Œì´ë¸”ë¡œ ê°€ì ¸ì˜¤ê¸°
                    # ì»¬ëŸ¼ëª… ë§¤í•‘ (ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì— ë”°ë¼ ì²˜ë¦¬)
                    if source_schema:
                        if isinstance(source_schema, list):
                            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ: [{"column_name": "...", "data_type": "...", ...}]
                            schema_columns = [col["column_name"] for col in source_schema]
                            available_columns = [col for col in schema_columns if col in df.columns]
                        elif isinstance(source_schema, dict) and source_schema.get("columns"):
                            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ: {"columns": [{"name": "...", "type": "...", ...}]}
                            schema_columns = [col["name"] for col in source_schema["columns"]]
                            available_columns = [col for col in schema_columns if col in df.columns]
                        else:
                            available_columns = list(df.columns)
                        
                        if not available_columns:
                            logger.warning("CSVì™€ ìŠ¤í‚¤ë§ˆ ê°„ì— ê³µí†µ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            return 0

                        df_reordered = df[available_columns]
                        temp_columns = available_columns
                    else:
                        temp_columns = list(df.columns)
                        df_reordered = df

                    # ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ì •ë¦¬
                    for col in df_reordered.columns:
                        # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                        df_reordered[col] = df_reordered[col].where(pd.notna(df_reordered[col]), None)

                        # ìŠ¤í‚¤ë§ˆì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ì˜ íƒ€ì… ì •ë³´ ì°¾ê¸° (ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì— ë”°ë¼ ì²˜ë¦¬)
                        if isinstance(source_schema, list):
                            col_schema = next((c for c in source_schema if c["column_name"] == col), None)
                            col_type = col_schema["data_type"].lower() if col_schema else "text"
                        elif isinstance(source_schema, dict) and "columns" in source_schema:
                            col_schema = next((c for c in source_schema["columns"] if c["name"] == col), None)
                            col_type = col_schema["type"].lower() if col_schema else "text"
                        else:
                            col_type = "text"

                        # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                        if df_reordered[col].dtype in ["int64", "float64"]:
                            df_reordered[col] = df_reordered[col].astype(str)

                        # ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œ 'nan', 'NaN' ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                        if df_reordered[col].dtype == "object":
                            df_reordered[col] = df_reordered[col].replace(['nan', 'NaN', 'None', ''], None)

                        # BIGINT ì»¬ëŸ¼ì˜ ê²½ìš° ë¶€ë™ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ì˜ˆ: "1.0" -> "1")
                        if col_type in ["bigint", "integer", "int"] and df_reordered[col].dtype == "object":
                            df_reordered[col] = df_reordered[col].apply(
                                lambda x: str(int(float(x))) if x and isinstance(x, str) and x.replace('.', '').replace('-', '').isdigit() and float(x).is_integer() else x
                            )

                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ INSERT ì‹¤í–‰
                    total_inserted = 0
                    for i in range(0, len(df_reordered), batch_size):
                        batch_df = df_reordered.iloc[i : i + batch_size]
                        batch_data = [tuple(row) for row in batch_df.values]

                        insert_query = f"""
                            INSERT INTO {temp_table} ({', '.join(temp_columns)})
                            VALUES ({', '.join(['%s'] * len(temp_columns))})
                        """

                        cursor.executemany(insert_query, batch_data)
                        total_inserted += len(batch_data)

                        if i % 10000 == 0:
                            logger.info(f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")

                    # ì»¤ë°‹
                    conn.commit()

                    logger.info(f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}")
                    return total_inserted, temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _create_temp_table_and_import_csv_in_connection(
        self,
        cursor,
        target_table: str,
        source_schema: dict[str, Any],
        csv_path: str,
        batch_size: int = 1000
    ) -> tuple[int, str]:
        """
        ê¸°ì¡´ ì»¤ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸°ë¥¼ ìˆ˜í–‰
        """
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_path, encoding="utf-8")

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0, ""

            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„± (ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì— ë”°ë¼ ì²˜ë¦¬)
            column_definitions = []
            
            # ìŠ¤í‚¤ë§ˆê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸
            if isinstance(source_schema, list):
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ: [{"column_name": "...", "data_type": "...", ...}]
                for col in source_schema:
                    col_name = col["column_name"]
                    col_type = col["data_type"]
                    col_type = self._convert_to_postgres_type(col_type, col.get('character_maximum_length'))
                    nullable = "" if col["is_nullable"] == "YES" else " NOT NULL"
                    column_definitions.append(f'"{col_name}" {col_type}{nullable}')
            elif isinstance(source_schema, dict) and "columns" in source_schema:
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ: {"columns": [{"name": "...", "type": "...", ...}]}
                for col in source_schema["columns"]:
                    col_name = col["name"]
                    col_type = col["type"]
                    col_type = self._convert_to_postgres_type(col_type, col.get('max_length'))
                    nullable = "" if col["nullable"] else " NOT NULL"
                    column_definitions.append(f"{col_name} {col_type}{nullable}")
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤í‚¤ë§ˆ êµ¬ì¡°: {type(source_schema)}")

            # ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL
            create_temp_table_sql = f"""
                CREATE TEMP TABLE {temp_table} (
                    {', '.join(column_definitions)}
                ) ON COMMIT DROP
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_temp_table_sql}")

            # 1. ì„ì‹œ í…Œì´ë¸” ìƒì„±
            cursor.execute(create_temp_table_sql)
            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

            # 2. ì„ì‹œ í…Œì´ë¸”ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            cursor.execute(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{temp_table}')")
            exists = cursor.fetchone()[0]
            if not exists:
                raise Exception(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # 3. CSV ë°ì´í„°ë¥¼ ì„ì‹œ í…Œì´ë¸”ë¡œ ê°€ì ¸ì˜¤ê¸°
            # ì»¬ëŸ¼ëª… ë§¤í•‘ (ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì— ë”°ë¼ ì²˜ë¦¬)
            if source_schema:
                if isinstance(source_schema, list):
                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ: [{"column_name": "...", "data_type": "...", ...}]
                    schema_columns = [col["column_name"] for col in source_schema]
                    available_columns = [col for col in schema_columns if col in df.columns]
                elif isinstance(source_schema, dict) and source_schema.get("columns"):
                    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ: {"columns": [{"name": "...", "type": "...", ...}]}
                    schema_columns = [col["name"] for col in source_schema["columns"]]
                    available_columns = [col for col in schema_columns if col in df.columns]
                else:
                    available_columns = list(df.columns)
                
                if not available_columns:
                    logger.warning("CSVì™€ ìŠ¤í‚¤ë§ˆ ê°„ì— ê³µí†µ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return 0, ""

                df_reordered = df[available_columns]
                temp_columns = available_columns
            else:
                temp_columns = list(df.columns)
                df_reordered = df

            # ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ì •ë¦¬
            for col in df_reordered.columns:
                # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                df_reordered[col] = df_reordered[col].where(pd.notna(df_reordered[col]), None)

                # ìŠ¤í‚¤ë§ˆì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ì˜ íƒ€ì… ì •ë³´ ì°¾ê¸° (ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì— ë”°ë¼ ì²˜ë¦¬)
                if isinstance(source_schema, list):
                    col_schema = next((c for c in source_schema if c["column_name"] == col), None)
                    col_type = col_schema["data_type"].lower() if col_schema else "text"
                elif isinstance(source_schema, dict) and "columns" in source_schema:
                    col_schema = next((c for c in source_schema["columns"] if c["name"] == col), None)
                    col_type = col_schema["type"].lower() if col_schema else "text"
                else:
                    col_type = "text"

                # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                if df_reordered[col].dtype in ["int64", "float64"]:
                    df_reordered[col] = df_reordered[col].astype(str)

                # ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œ 'nan', 'NaN' ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                if df_reordered[col].dtype == "object":
                    df_reordered[col] = df_reordered[col].replace(['nan', 'NaN', 'None', ''], None)

                # BIGINT ì»¬ëŸ¼ì˜ ê²½ìš° ë¶€ë™ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ì˜ˆ: "1.0" -> "1")
                if col_type in ["bigint", "integer", "int"] and df_reordered[col].dtype == "object":
                    df_reordered[col] = df_reordered[col].apply(
                        lambda x: str(int(float(x))) if x and isinstance(x, str) and x.replace('.', '').replace('-', '').isdigit() and float(x).is_integer() else x
                    )

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ COPY ëª…ë ¹ì–´ ì‹¤í–‰ (INSERTë³´ë‹¤ 10-50ë°° ë¹ ë¦„)
            total_inserted = 0
            for i in range(0, len(df_reordered), batch_size):
                batch_df = df_reordered.iloc[i : i + batch_size]
                batch_data = [tuple(row) for row in batch_df.values]

                # COPY ëª…ë ¹ì–´ë¡œ ê³ ì† ì‚½ì…
                try:
                    # COPY ëª…ë ¹ì–´ ì‹œì‘
                    copy_sql = f"COPY {temp_table} ({', '.join(temp_columns)}) FROM STDIN"
                    
                    # StringIOë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ê°ì²´ ìƒì„±
                    from io import StringIO
                    copy_buffer = StringIO()
                    
                    # ë°ì´í„°ë¥¼ COPY í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë²„í¼ì— ì“°ê¸°
                    for row in batch_data:
                        # ê° í–‰ì„ íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
                        row_str = '\t'.join(str(val) if val is not None else '\\N' for val in row)
                        copy_buffer.write(row_str + '\n')
                    
                    # ë²„í¼ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
                    copy_buffer.seek(0)
                    
                    # COPY ëª…ë ¹ì–´ ì‹¤í–‰
                    cursor.copy_expert(copy_sql, copy_buffer)
                    total_inserted += len(batch_data)

                    if i % 10000 == 0:
                        logger.info(f"COPY ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")
                        
                except Exception as copy_error:
                    logger.warning(f"COPY ëª…ë ¹ì–´ ì‹¤íŒ¨, INSERTë¡œ í´ë°±: {copy_error}")
                    # COPY ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ INSERT ë°©ì‹ìœ¼ë¡œ í´ë°±
                    insert_query = f"""
                        INSERT INTO {temp_table} ({', '.join(temp_columns)})
                        VALUES ({', '.join(['%s'] * len(temp_columns))})
                    """
                    cursor.executemany(insert_query, batch_data)
                    total_inserted += len(batch_data)

                    if i % 10000 == 0:
                        logger.info(f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")

            logger.info(f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}")
            return total_inserted, temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _execute_merge_operation_same_connection(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "full_sync",
        verified_schema: dict[str, Any] = None
    ) -> dict[str, Any]:
        """
        ê°™ì€ ì—°ê²°ì—ì„œ MERGE ì‘ì—… ì‹¤í–‰ (ì„ì‹œ í…Œì´ë¸”ê³¼ í•¨ê»˜ ì‚¬ìš©)
        """
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            # ê°™ì€ ì—°ê²°ì—ì„œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(all_columns_query)
                    non_pk_columns = cursor.fetchall()
                    non_pk_column_names = [col[0] for col in non_pk_columns]

                    # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                    if not non_pk_column_names:
                        logger.warning(
                            "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                        )
                        # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                        if sync_mode == "full_sync":
                            merge_sql = f"""
                                BEGIN;

                                -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                                DELETE FROM {target_table}
                                WHERE ({pk_columns}) IN (
                                    SELECT {pk_columns} FROM {source_table}
                                );

                                -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table};

                                COMMIT;
                            """
                        else:
                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO NOTHING;
                            """

                        # MERGE ì‹¤í–‰
                        start_time = pd.Timestamp.now()
                        cursor.execute(merge_sql)
                        conn.commit()
                        end_time = pd.Timestamp.now()

                        # ê²°ê³¼ í™•ì¸
                        cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                        source_count = cursor.fetchone()[0]
                        cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                        target_count = cursor.fetchone()[0]

                        merge_result = {
                            "source_count": source_count,
                            "target_count": target_count,
                            "sync_mode": sync_mode,
                            "execution_time": (end_time - start_time).total_seconds(),
                            "status": "success",
                            "message": (
                                f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                                f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                            ),
                        }

                        logger.info(merge_result["message"])
                        return merge_result

                    # MERGE ì¿¼ë¦¬ ìƒì„±
                    if sync_mode == "full_sync":
                        # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                        if non_pk_column_names:
                            merge_sql = f"""
                                BEGIN;

                                -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                                DELETE FROM {target_table}
                                WHERE ({pk_columns}) IN (
                                    SELECT {pk_columns} FROM {source_table}
                                );

                                -- ìƒˆ ë°ì´í„° ì‚½ì…
                                INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                                SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                                FROM {source_table};

                                COMMIT;
                            """
                        else:
                            # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                            merge_sql = f"""
                                BEGIN;

                                -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                                DELETE FROM {target_table}
                                WHERE ({pk_columns}) IN (
                                    SELECT {pk_columns} FROM {source_table}
                                );

                                -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table};

                                COMMIT;
                            """
                    else:
                        # ì¦ë¶„ ë™ê¸°í™”: UPSERT (INSERT ... ON CONFLICT)
                        if non_pk_column_names:
                            update_set_clause = ", ".join(
                                [f"{col} = EXCLUDED.{col}" for col in non_pk_column_names]
                            )

                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                                SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO UPDATE SET {update_set_clause};
                            """
                        else:
                            # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO NOTHING;
                            """

                        # ì¦ë¶„ ë™ê¸°í™”: ì¤‘ë³µí‚¤ ìŠ¤í‚µ (ON CONFLICT DO NOTHING)
                        if non_pk_column_names:
                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                                SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO NOTHING;
                            """
                        else:
                            # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO NOTHING;
                            """

                        # MERGE ì‹¤í–‰
                        start_time = pd.Timestamp.now()
                        cursor.execute(merge_sql)
                        conn.commit()
                        end_time = pd.Timestamp.now()

                        # ê²°ê³¼ í™•ì¸
                        cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                        source_count = cursor.fetchone()[0]
                        cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                        target_count = cursor.fetchone()[0]

                        merge_result = {
                            "source_count": source_count,
                            "target_count": target_count,
                            "sync_mode": sync_mode,
                            "execution_time": (end_time - start_time).total_seconds(),
                            "status": "success",
                            "message": (
                                f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                                f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                            ),
                        }

                        logger.info(merge_result["message"])
                        return merge_result

        except Exception as e:
            logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "message": str(e),
                "source_count": 0,
                "target_count": 0,
                "sync_mode": sync_mode,
                "execution_time": 0
            }

    def _execute_merge_operation_with_cursor(
        self,
        cursor,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "full_sync",
        verified_schema: dict[str, Any] = None
    ) -> dict[str, Any]:
        """
        ê¸°ì¡´ ì»¤ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ MERGE ì‘ì—… ì‹¤í–‰
        """
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
            constraint_name = f"pk_{table}_{'_'.join(primary_keys)}"
            check_constraint_sql = f"""
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.table_constraints
                    WHERE constraint_name = '{constraint_name}'
                    AND table_schema = '{schema}'
                    AND table_name = '{table}'
                )
            """
            cursor.execute(check_constraint_sql)
            constraint_exists = cursor.fetchone()[0]

            if not constraint_exists:
                logger.info(f"ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„±: {constraint_name}")
                create_constraint_sql = f"""
                    ALTER TABLE {target_table}
                    ADD CONSTRAINT {constraint_name}
                    PRIMARY KEY ({pk_columns})
                """
                cursor.execute(create_constraint_sql)
                logger.info(f"ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„± ì™„ë£Œ: {constraint_name}")

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            cursor.execute(all_columns_query)
            non_pk_columns = cursor.fetchall()
            non_pk_column_names = [col[0] for col in non_pk_columns]

            # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not non_pk_column_names:
                logger.warning(
                    "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                )
                # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if sync_mode == "full_sync":
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # MERGE ì‹¤í–‰
                start_time = pd.Timestamp.now()
                cursor.execute(merge_sql)
                end_time = pd.Timestamp.now()

                # ê²°ê³¼ í™•ì¸
                cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                source_count = cursor.fetchone()[0]
                cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                target_count = cursor.fetchone()[0]

                merge_result = {
                    "source_count": source_count,
                    "target_count": target_count,
                    "sync_mode": sync_mode,
                    "execution_time": (end_time - start_time).total_seconds(),
                    "status": "success",
                    "message": (
                        f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                        f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                    ),
                }

                logger.info(merge_result["message"])
                return merge_result

            # MERGE ì¿¼ë¦¬ ìƒì„±
            if sync_mode == "full_sync":
                # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                if non_pk_column_names:
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì…
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
            else:
                # ì¦ë¶„ ë™ê¸°í™”: UPSERT (INSERT ... ON CONFLICT)
                if non_pk_column_names:
                    update_set_clause = ", ".join(
                        [f"{col} = EXCLUDED.{col}" for col in non_pk_column_names]
                    )

                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO UPDATE SET {update_set_clause};
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # ì¦ë¶„ ë™ê¸°í™”: ì¤‘ë³µí‚¤ ìŠ¤í‚µ (ON CONFLICT DO NOTHING)
                if non_pk_column_names:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

            # MERGE ì‹¤í–‰
            start_time = pd.Timestamp.now()
            cursor.execute(merge_sql)
            end_time = pd.Timestamp.now()

            # ê²°ê³¼ í™•ì¸ (ì¦ë¶„ ë™ê¸°í™”ì˜ ê²½ìš° ì†ŒìŠ¤ëŠ” ì„ì‹œ í…Œì´ë¸”, íƒ€ê²Ÿì€ ì‹¤ì œ í…Œì´ë¸”)
            cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
            source_count = cursor.fetchone()[0]
            cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
            target_count = cursor.fetchone()[0]

            merge_result = {
                "source_count": source_count,
                "target_count": target_count,
                "sync_mode": sync_mode,
                "execution_time": (end_time - start_time).total_seconds(),
                "status": "success",
                "message": (
                    f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                ),
            }

            logger.info(merge_result["message"])
            return merge_result

        except Exception as e:
            logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "message": str(e),
                "source_count": 0,
                "target_count": 0,
                "sync_mode": sync_mode,
                "execution_time": 0
            }

    def copy_table_data_with_xmin(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "xmin_incremental",
        batch_size: int = 5000,
        custom_where: str | None = None,
        # ì²­í¬ ë°©ì‹ íŒŒë¼ë¯¸í„° ì¶”ê°€
        chunk_mode: bool = True,
        enable_checkpoint: bool = True,
        max_retries: int = 3,
    ) -> dict[str, Any]:
        """
        xmin ê¸°ë°˜ ì¦ë¶„ ë°ì´í„° ë³µì‚¬ (ì‹¤ì œ xmin ê°’ ì €ì¥)
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ëª©ë¡
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            custom_where: ì»¤ìŠ¤í…€ WHERE ì¡°ê±´
            
        Returns:
            ë³µì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            start_time = time.time()
            logger.info(f"xmin ê¸°ë°˜ ì¦ë¶„ ë°ì´í„° ë³µì‚¬ ì‹œì‘: {source_table} -> {target_table}")
            
            # 1ë‹¨ê³„: xmin ì•ˆì •ì„± ê²€ì¦
            xmin_stability = self.db_ops.validate_xmin_stability(source_table)
            if xmin_stability == "force_full_sync":
                logger.warning("xmin ìˆœí™˜ ìœ„í—˜ìœ¼ë¡œ ì „ì²´ ë™ê¸°í™” ëª¨ë“œë¡œ ì „í™˜")
                return self.copy_table_data(
                    source_table=source_table,
                    target_table=target_table,
                    primary_keys=primary_keys,
                    sync_mode="full_sync",
                    batch_size=batch_size,
                    custom_where=custom_where,
                    # ì²­í¬ ë°©ì‹ íŒŒë¼ë¯¸í„° ì „ë‹¬
                    chunk_mode=chunk_mode,
                    enable_checkpoint=enable_checkpoint,
                    max_retries=max_retries
                )
            
            # 2ë‹¨ê³„: ë³µì œ ìƒíƒœ í™•ì¸
            if not self.db_ops.check_replication_status(source_table):
                logger.warning("ë³µì œë³¸ í™˜ê²½ì—ì„œ xmin ê¸°ë°˜ ì²˜ë¦¬ ë¶ˆê°€, íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜")
                return self.copy_table_data(
                    source_table=source_table,
                    target_table=target_table,
                    primary_keys=primary_keys,
                    sync_mode="incremental_sync",
                    batch_size=batch_size,
                    custom_where=custom_where,
                    # ì²­í¬ ë°©ì‹ íŒŒë¼ë¯¸í„° ì „ë‹¬
                    chunk_mode=chunk_mode,
                    enable_checkpoint=enable_checkpoint,
                    max_retries=max_retries
                )
            
            # 3ë‹¨ê³„: íƒ€ê²Ÿ í…Œì´ë¸”ì— source_xmin ì»¬ëŸ¼ í™•ì¸/ì¶”ê°€
            self.db_ops.ensure_xmin_column_exists(target_table)
            
            # 4ë‹¨ê³„: ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬ëœ xmin ê°’ ì¡°íšŒ
            last_xmin = self.db_ops.get_last_processed_xmin_from_target(target_table)
            
            # 5ë‹¨ê³„: xmin ê¸°ë°˜ ì¦ë¶„ ì¡°ê±´ ìƒì„±
            incremental_condition, latest_xmin = self.db_ops.build_xmin_incremental_condition(
                source_table, target_table, last_xmin
            )
            
            # 6ë‹¨ê³„: ì»¤ìŠ¤í…€ WHERE ì¡°ê±´ê³¼ ê²°í•©
            if custom_where:
                if incremental_condition == "1=1":
                    where_clause = custom_where
                else:
                    where_clause = f"({incremental_condition}) AND ({custom_where})"
            else:
                where_clause = incremental_condition
            
            # 7ë‹¨ê³„: xmin í¬í•¨ ì¦ë¶„ ë°ì´í„° ì¡°íšŒ
            select_sql = f"""
                SELECT *, xmin as source_xmin
                FROM {source_table}
                WHERE {where_clause}
                ORDER BY xmin
            """
            
            logger.info(f"xmin ê¸°ë°˜ ì¦ë¶„ ë°ì´í„° ì¡°íšŒ SQL: {select_sql}")
            
            # 8ë‹¨ê³„: CSV íŒŒì¼ë¡œ ë°ì´í„° ì¶”ì¶œ (xmin í¬í•¨, ì²­í¬ ë°©ì‹ ì§€ì›)
            csv_path = self._export_to_csv_with_xmin(
                source_table, 
                select_sql, 
                batch_size,
                chunk_mode=chunk_mode,
                enable_checkpoint=enable_checkpoint,
                max_retries=max_retries
            )
            
            # 9ë‹¨ê³„: ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ë¡œë“œ
            temp_table = f"temp_{target_table.replace('.', '_')}"
            self._import_csv_in_session_with_xmin(temp_table, csv_path, batch_size)
            
            # 10ë‹¨ê³„: MERGE ì‘ì—… ì‹¤í–‰ (xmin ê°’ í¬í•¨)
            merge_result = self._execute_xmin_merge(
                source_table, target_table, temp_table, primary_keys
            )
            
            # 11ë‹¨ê³„: ì„ì‹œ íŒŒì¼ ì •ë¦¬
            self.cleanup_temp_files(csv_path)
            
            result = {
                "status": "success",
                "source_table": source_table,
                "target_table": target_table,
                "sync_mode": sync_mode,
                "xmin_stability": xmin_stability,
                "last_processed_xmin": last_xmin,
                "latest_source_xmin": latest_xmin,
                "merge_result": merge_result,
                "batch_size": batch_size,
                "execution_time": time.time() - start_time if 'start_time' in locals() else 0
            }
            
            logger.info(f"xmin ê¸°ë°˜ ì¦ë¶„ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"xmin ê¸°ë°˜ ì¦ë¶„ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹œë„
            try:
                if 'csv_path' in locals():
                    self.cleanup_temp_files(csv_path)
            except:
                pass
            
            return {
                "status": "error",
                "message": str(e),
                "source_table": source_table,
                "target_table": target_table,
                "sync_mode": sync_mode
            }

    def validate_xmin_incremental_integrity(self, source_table: str, target_table: str) -> dict[str, Any]:
        """
        xmin ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™”ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            
        Returns:
            ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼
        """
        try:
            logger.info(f"xmin ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™” ë¬´ê²°ì„± ê²€ì¦ ì‹œì‘: {source_table} -> {target_table}")
            
            # 1ë‹¨ê³„: ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ìµœì‹  xmin ê°’ ì¡°íšŒ
            source_max_xmin = self.get_source_max_xmin(source_table)
            
            # 2ë‹¨ê³„: íƒ€ê²Ÿ í…Œì´ë¸”ì˜ ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ xmin ê°’ ì¡°íšŒ
            target_last_xmin = self.db_ops.get_last_processed_xmin_from_target(target_table)
            
            # 3ë‹¨ê³„: ë™ê¸°í™” ìƒíƒœ í™•ì¸
            is_synchronized = target_last_xmin >= source_max_xmin if source_max_xmin > 0 else True
            sync_gap = max(0, source_max_xmin - target_last_xmin) if source_max_xmin > 0 else 0
            
            # 4ë‹¨ê³„: ë°ì´í„° ì¼ì¹˜ì„± í™•ì¸
            source_count = self.db_ops.get_table_row_count(source_table)
            target_count = self.db_ops.get_table_row_count(target_table)
            
            # 5ë‹¨ê³„: xmin ê°’ ë¶„í¬ í™•ì¸
            xmin_distribution = self._get_xmin_distribution(source_table)
            
            integrity_result = {
                "source_table": source_table,
                "target_table": target_table,
                "source_max_xmin": source_max_xmin,
                "target_last_xmin": target_last_xmin,
                "is_synchronized": is_synchronized,
                "sync_gap": sync_gap,
                "source_count": source_count,
                "target_count": target_count,
                "xmin_distribution": xmin_distribution,
                "integrity_score": self._calculate_integrity_score(
                    is_synchronized, sync_gap, source_count, target_count
                ),
                "status": "success"
            }
            
            logger.info(f"xmin ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ: {integrity_result}")
            return integrity_result
            
        except Exception as e:
            logger.error(f"xmin ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "message": str(e),
                "source_table": source_table,
                "target_table": target_table
            }

    def _get_xmin_distribution(self, source_table: str) -> dict[str, Any]:
        """
        ì†ŒìŠ¤ í…Œì´ë¸”ì˜ xmin ê°’ ë¶„í¬ ì¡°íšŒ
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            
        Returns:
            xmin ë¶„í¬ ì •ë³´
        """
        try:
            source_hook = self.source_hook
            
            # xmin ê°’ ë¶„í¬ ì¡°íšŒ
            distribution_sql = f"""
                SELECT 
                    COUNT(*) as total_records,
                    MIN(xmin::text::bigint) as min_xmin,
                    MAX(xmin::text::bigint) as max_xmin,
                    AVG(xmin::text::bigint) as avg_xmin,
                    STDDEV(xmin::text::bigint) as stddev_xmin
                FROM {source_table}
            """
            
            result = source_hook.get_first(distribution_sql)
            if not result:
                return {}
            
            total_records, min_xmin, max_xmin, avg_xmin, stddev_xmin = result
            
            return {
                "total_records": total_records,
                "min_xmin": min_xmin,
                "max_xmin": max_xmin,
                "avg_xmin": round(avg_xmin) if avg_xmin else 0,
                "stddev_xmin": round(stddev_xmin) if stddev_xmin else 0,
                "xmin_range": max_xmin - min_xmin if max_xmin and min_xmin else 0
            }
            
        except Exception as e:
            logger.error(f"xmin ë¶„í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_integrity_score(self, is_synchronized: bool, sync_gap: int, 
                                 source_count: int, target_count: int) -> float:
        """
        ë¬´ê²°ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            is_synchronized: ë™ê¸°í™” ìƒíƒœ
            sync_gap: ë™ê¸°í™” ê°„ê²©
            source_count: ì†ŒìŠ¤ í…Œì´ë¸” í–‰ ìˆ˜
            target_count: íƒ€ê²Ÿ í…Œì´ë¸” í–‰ ìˆ˜
            
        Returns:
            ë¬´ê²°ì„± ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            score = 1.0
            
            # ë™ê¸°í™” ìƒíƒœì— ë”°ë¥¸ ì ìˆ˜ ì°¨ê°
            if not is_synchronized:
                score -= 0.3
            
            # ë™ê¸°í™” ê°„ê²©ì— ë”°ë¥¸ ì ìˆ˜ ì°¨ê°
            if sync_gap > 1000000:  # 100ë§Œ ì´ìƒ
                score -= 0.2
            elif sync_gap > 100000:  # 10ë§Œ ì´ìƒ
                score -= 0.1
            
            # ë°ì´í„° ì¼ì¹˜ì„±ì— ë”°ë¥¸ ì ìˆ˜ ì°¨ê°
            if source_count > 0 and target_count > 0:
                count_ratio = min(source_count, target_count) / max(source_count, target_count)
                score -= (1.0 - count_ratio) * 0.2
            
            return max(0.0, score)
            
        except Exception as e:
            logger.error(f"ë¬´ê²°ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _export_to_csv_with_xmin(
        self,
        source_table: str,
        select_sql: str,
        batch_size: int
    ) -> str:
        """
        xmin í¬í•¨ ë°ì´í„°ë¥¼ CSVë¡œ ì¶”ì¶œ
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            select_sql: SELECT SQL ì¿¼ë¦¬
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            CSV íŒŒì¼ ê²½ë¡œ
        """
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ìƒì„±
            timestamp = int(time.time())
            safe_table_name = source_table.replace('.', '_').replace('-', '_')
            csv_path = f"{self.temp_dir}/{safe_table_name}_{timestamp}.csv"
            
            # psql ëª…ë ¹ì–´ë¡œ ë°ì´í„° ì¶”ì¶œ (xmin í¬í•¨)
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            source_conn = self.source_hook.get_conn()
            source_host = source_conn.info.host
            source_port = source_conn.info.port
            source_user = source_conn.info.user
            source_database = source_conn.info.dbname
            source_password = source_conn.info.password
            
            # psql ëª…ë ¹ì–´ êµ¬ì„±
            export_cmd = [
                "psql",
                "-h", str(source_host),
                "-p", str(source_port),
                "-U", str(source_user),
                "-d", str(source_database),
                "-c", f"\\copy ({select_sql}) TO '{csv_path}' WITH CSV HEADER"
            ]
            
            # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            env = os.environ.copy()
            if source_password:
                env["PGPASSWORD"] = str(source_password)
            
            # psql ëª…ë ¹ì–´ ì‹¤í–‰
            import subprocess
            result = subprocess.run(
                export_cmd, 
                env=env, 
                capture_output=True, 
                text=True,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode != 0:
                raise Exception(f"CSV ì¶”ì¶œ ì‹¤íŒ¨: {result.stderr}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            if os.path.exists(csv_path):
                file_size = os.path.getsize(csv_path)
                logger.info(f"xmin í¬í•¨ ë°ì´í„° CSV ì¶”ì¶œ ì™„ë£Œ: {csv_path}, í¬ê¸°: {file_size} bytes")
            else:
                raise Exception("CSV íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            return csv_path
            
        except Exception as e:
            logger.error(f"xmin í¬í•¨ ë°ì´í„° CSV ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise

    def _import_csv_in_session_with_xmin(
        self,
        temp_table: str,
        csv_path: str,
        batch_size: int
    ) -> None:
        """
        xmin í¬í•¨ CSV ë°ì´í„°ë¥¼ ì„ì‹œ í…Œì´ë¸”ì— ë¡œë“œ
        
        Args:
            temp_table: ì„ì‹œ í…Œì´ë¸”ëª…
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        try:
            target_hook = self.target_hook
            
            # CSV íŒŒì¼ì—ì„œ ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
            import pandas as pd
            df_sample = pd.read_csv(csv_path, nrows=1)
            columns = list(df_sample.columns)
            
            # source_xmin ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if 'source_xmin' not in columns:
                raise Exception("source_xmin ì»¬ëŸ¼ì´ CSVì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # ì„ì‹œ í…Œì´ë¸” ìƒì„±
            create_temp_sql = f"""
                CREATE TEMP TABLE {temp_table} (
                    {', '.join([f'"{col}" TEXT' for col in columns])}
                ) ON COMMIT DROP
            """
            
            target_hook.run(create_temp_sql)
            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")
            
            # CSV ë°ì´í„° ë¡œë“œ
            copy_sql = f"""
                COPY {temp_table} FROM '{csv_path}' WITH CSV HEADER
            """
            
            target_hook.run(copy_sql)
            
            # ë¡œë“œëœ ë°ì´í„° ìˆ˜ í™•ì¸
            count_sql = f"SELECT COUNT(*) FROM {temp_table}"
            loaded_count = target_hook.get_first(count_sql)[0]
            
            logger.info(f"CSV ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {temp_table}, {loaded_count}í–‰")
            
        except Exception as e:
            logger.error(f"CSV ë°ì´í„° ì„ì‹œ í…Œì´ë¸” ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def _execute_xmin_merge(
        self,
        source_table: str,
        target_table: str,
        temp_table: str,
        primary_keys: list[str]
    ) -> dict[str, Any]:
        """
        xmin ê¸°ë°˜ MERGE ì‘ì—… ì‹¤í–‰ (source_xmin ì»¬ëŸ¼ í¬í•¨)
        
        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            temp_table: ì„ì‹œ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ëª©ë¡
            
        Returns:
            MERGE ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            target_hook = self.target_hook
            
            # ê¸°ë³¸í‚¤ ì¡°ê±´ êµ¬ì„±
            pk_conditions = " AND ".join([
                f"target.{pk} = source.{pk}" for pk in primary_keys
            ])
            
            # ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼ ëª©ë¡ (source_xmin í¬í•¨)
            update_columns = self._get_update_columns_for_xmin(target_table)
            
            # MERGE SQL ì‹¤í–‰
            merge_sql = f"""
                MERGE INTO {target_table} AS target
                USING {temp_table} AS source
                ON {pk_conditions}
                
                WHEN MATCHED THEN
                    UPDATE SET
                        {', '.join([f"{col} = source.{col}" for col in update_columns])},
                        updated_at = CURRENT_TIMESTAMP
                
                WHEN NOT MATCHED BY TARGET THEN
                    INSERT ({', '.join(self._get_all_columns_for_xmin(target_table))})
                    VALUES ({', '.join([f"source.{col}" for col in self._get_all_columns_for_xmin(target_table)])});
            """
            
            logger.info(f"xmin ê¸°ë°˜ MERGE SQL ì‹¤í–‰: {merge_sql}")
            
            # MERGE ì‹¤í–‰
            start_time = pd.Timestamp.now()
            target_hook.run(merge_sql)
            end_time = pd.Timestamp.now()
            
            # ê²°ê³¼ í™•ì¸
            result_sql = f"SELECT COUNT(*) FROM {temp_table}"
            total_processed = target_hook.get_first(result_sql)[0]
            
            # ì„ì‹œ í…Œì´ë¸” ì‚­ì œ
            target_hook.run(f"DROP TABLE IF EXISTS {temp_table}")
            
            merge_result = {
                "total_processed": total_processed,
                "merge_sql": merge_sql,
                "execution_time": (end_time - start_time).total_seconds(),
                "status": "success"
            }
            
            logger.info(f"xmin ê¸°ë°˜ MERGE ì™„ë£Œ: {merge_result}")
            return merge_result
            
        except Exception as e:
            logger.error(f"xmin ê¸°ë°˜ MERGE ì‹¤íŒ¨: {e}")
            raise

    def _get_update_columns_for_xmin(self, target_table: str) -> list[str]:
        """
        xmin ê¸°ë°˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ (source_xmin í¬í•¨)
        
        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            
        Returns:
            ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼ ëª©ë¡
        """
        try:
            target_hook = self.target_hook
            
            # í…Œì´ë¸”ì˜ ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ
            if "." in target_table:
                schema_name, table_name = target_table.split(".", 1)
            else:
                schema_name = "raw_data"
                table_name = target_table
            
            columns_sql = """
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = %s AND table_name = %s
                ORDER BY ordinal_position
            """
            
            columns = target_hook.get_records(columns_sql, parameters=(schema_name, table_name))
            column_names = [col[0] for col in columns]
            
            # ì œì™¸í•  ì»¬ëŸ¼ë“¤
            exclude_columns = ["created_at", "id"]  # idëŠ” ê¸°ë³¸í‚¤ì´ë¯€ë¡œ ì œì™¸
            
            # source_xmin ì»¬ëŸ¼ì€ ì—…ë°ì´íŠ¸ ëŒ€ìƒì— í¬í•¨
            update_columns = [col for col in column_names if col not in exclude_columns]
            
            # source_xmin ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if "source_xmin" not in update_columns:
                update_columns.append("source_xmin")
            
            logger.info(f"xmin ì—…ë°ì´íŠ¸ ì»¬ëŸ¼ ëª©ë¡: {update_columns}")
            return update_columns
            
        except Exception as e:
            logger.error(f"ì—…ë°ì´íŠ¸ ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise

    def execute_merge_with_xmin(self, temp_table: str, target_table: str, max_xmin: int, primary_keys: list[str]) -> dict[str, Any]:
        """
        source_xminì„ í¬í•¨í•œ MERGE ì‹¤í–‰
        
        Args:
            temp_table: ì„ì‹œ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            max_xmin: ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ìµœëŒ€ xmin ê°’
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ëª©ë¡
            
        Returns:
            MERGE ì‹¤í–‰ ê²°ê³¼
        """
        try:
            logger.info(f"source_xmin({max_xmin})ì„ í¬í•¨í•œ MERGE ì‹¤í–‰: {temp_table} -> {target_table}")
            
            target_hook = self.target_hook
            
            # 1. source_xmin ì»¬ëŸ¼ì´ íƒ€ê²Ÿ í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            self.db_ops.ensure_xmin_column_exists(target_table)
            
            # 2. ê¸°ë³¸í‚¤ ì¡°ê±´ êµ¬ì„±
            pk_conditions = " AND ".join([
                f"target.{pk} = source.{pk}" for pk in primary_keys
            ])
            
            # 3. ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ (source_xmin í¬í•¨)
            update_columns = self._get_update_columns_for_xmin(target_table)
            
            # 4. INSERTìš© ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ (source_xmin í¬í•¨)
            insert_columns = self._get_all_columns_for_xmin(target_table)
            
            # 5. MERGE SQL êµ¬ì„±
            merge_sql = f"""
                MERGE INTO {target_table} AS target
                USING {temp_table} AS source
                ON {pk_conditions}
                
                WHEN MATCHED THEN
                    UPDATE SET
                        {', '.join([f"{col} = source.{col}" for col in update_columns])},
                        updated_at = CURRENT_TIMESTAMP
                
                WHEN NOT MATCHED BY TARGET THEN
                    INSERT ({', '.join(insert_columns)})
                    VALUES ({', '.join([f"source.{col}" for col in insert_columns])});
            """
            
            logger.info(f"MERGE SQL ì‹¤í–‰: {merge_sql}")
            
            # 6. MERGE ì‹¤í–‰
            start_time = time.time()
            target_hook.run(merge_sql)
            end_time = time.time()
            
            # 7. ê²°ê³¼ í™•ì¸
            source_count_sql = f"SELECT COUNT(*) FROM {temp_table}"
            source_count = target_hook.get_first(source_count_sql)[0]
            
            target_count_sql = f"SELECT COUNT(*) FROM {target_table}"
            target_count = target_hook.get_first(target_count_sql)[0]
            
            # 8. ì„ì‹œ í…Œì´ë¸” ì •ë¦¬
            target_hook.run(f"DROP TABLE IF EXISTS {temp_table}")
            
            # 9. ê²°ê³¼ ë°˜í™˜
            merge_result = {
                "source_count": source_count,
                "target_count": target_count,
                "max_xmin": max_xmin,
                "execution_time": end_time - start_time,
                "status": "success",
                "message": f"MERGE ì™„ë£Œ: {temp_table} -> {target_table}, "
                          f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰, "
                          f"ìµœëŒ€ xmin: {max_xmin}"
            }
            
            logger.info(merge_result["message"])
            return merge_result
            
        except Exception as e:
            logger.error(f"source_xminì„ í¬í•¨í•œ MERGE ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ì„ì‹œ í…Œì´ë¸” ì •ë¦¬ ì‹œë„
            try:
                self.target_hook.run(f"DROP TABLE IF EXISTS {temp_table}")
            except:
                pass
            
            return {
                "status": "error",
                "message": str(e),
                "source_count": 0,
                "target_count": 0,
                "max_xmin": max_xmin,
                "execution_time": 0
            }

    def _get_all_columns_for_xmin(self, target_table: str) -> list[str]:
        """
        xmin ê¸°ë°˜ INSERTë¥¼ ìœ„í•œ ëª¨ë“  ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ (source_xmin í¬í•¨)
        
        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            
        Returns:
            ëª¨ë“  ì»¬ëŸ¼ ëª©ë¡
        """
        try:
            target_hook = self.target_hook
            
            # í…Œì´ë¸”ì˜ ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ
            if "." in target_table:
                schema_name, table_name = target_table.split(".", 1)
            else:
                schema_name = "raw_data"
                table_name = target_table
            
            columns_sql = """
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = %s AND table_name = %s
                ORDER BY ordinal_position
            """
            
            columns = target_hook.get_records(columns_sql, parameters=(schema_name, table_name))
            column_names = [col[0] for col in columns]
            
            # source_xmin ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if "source_xmin" not in column_names:
                column_names.append("source_xmin")
            
            logger.info(f"xmin INSERT ì»¬ëŸ¼ ëª©ë¡: {column_names}")
            return column_names
            
        except Exception as e:
            logger.error(f"INSERT ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise

    def cleanup_temp_files(self, file_path: str) -> None:
        """
        ì„ì‹œ íŒŒì¼ ì •ë¦¬
        
        Args:
            file_path: ì •ë¦¬í•  íŒŒì¼ ê²½ë¡œ
        """
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {file_path}")
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")
