"""
Data Copy Engine Module
ë°ì´í„° ë³µì‚¬ í•µì‹¬ ë¡œì§, CSV ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°, ì„ì‹œ í…Œì´ë¸” ê´€ë¦¬, MERGE ì‘ì—… ë“±ì„ ë‹´ë‹¹
"""

import logging
import os
import time
from typing import Any

import pandas as pd

from .database_operations import DatabaseOperations

logger = logging.getLogger(__name__)


class DataCopyEngine:
    """ë°ì´í„° ë³µì‚¬ ì—”ì§„ í´ë˜ìŠ¤"""

    def __init__(self, db_ops: DatabaseOperations, temp_dir: str = "/tmp"):
        """
        ì´ˆê¸°í™”

        Args:
            db_ops: DatabaseOperations ì¸ìŠ¤í„´ìŠ¤
            temp_dir: ì„ì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.db_ops = db_ops
        self.temp_dir = temp_dir
        self.source_hook = db_ops.get_source_hook()
        self.target_hook = db_ops.get_target_hook()

    def export_to_csv(
        self,
        table_name: str,
        csv_path: str,
        where_clause: str | None = None,
        batch_size: int = 10000,
        order_by_field: str | None = None,
    ) -> int:
        """
        í…Œì´ë¸”ì„ CSVë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            table_name: í…Œì´ë¸”ëª…
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            where_clause: WHERE ì ˆ ì¡°ê±´
            batch_size: ë°°ì¹˜ í¬ê¸°
            order_by_field: ì •ë ¬ ê¸°ì¤€ í•„ë“œ

        Returns:
            ë‚´ë³´ë‚¸ í–‰ ìˆ˜
        """
        try:
            # ì „ì²´ í–‰ ìˆ˜ ì¡°íšŒ
            total_count = self.db_ops.get_table_row_count(
                table_name, where_clause=where_clause
            )

            if total_count == 0:
                logger.warning(f"í…Œì´ë¸” {table_name}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in table_name:
                schema, table = table_name.split(".", 1)
            else:
                schema = "public"
                table = table_name

            # ë¨¼ì € í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            try:
                schema_query = """
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_schema = %s AND table_name = %s
                    ORDER BY ordinal_position
                """
                schema_columns = self.source_hook.get_records(
                    schema_query, parameters=(schema, table)
                )
                if schema_columns:
                    columns = [col[0] for col in schema_columns]
                    logger.info(f"ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {columns}")
                else:
                    raise Exception("ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                raise

            # ORDER BY ì ˆ êµ¬ì„±
            if order_by_field:
                order_by_clause = f"ORDER BY {order_by_field}"
                logger.info(f"ì •ë ¬ ê¸°ì¤€: {order_by_field}")
            else:
                order_by_clause = "ORDER BY 1"
                logger.info("ê¸°ë³¸ ì •ë ¬ ì‚¬ìš©")

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì¶”ì¶œ
            offset = 0
            all_data = []

            while offset < total_count:
                if where_clause:
                    query = f"""
                        SELECT * FROM {table_name}
                        WHERE {where_clause}
                        {order_by_clause}
                        LIMIT {batch_size} OFFSET {offset}
                    """
                else:
                    query = f"""
                        SELECT * FROM {table_name}
                        {order_by_clause}
                        LIMIT {batch_size} OFFSET {offset}
                    """

                batch_data = self.source_hook.get_records(query)
                all_data.extend(batch_data)
                offset += batch_size

                logger.info(
                    f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {min(offset, total_count)}/{total_count}"
                )

            # DataFrameìœ¼ë¡œ ë³€í™˜
            if all_data:
                # ì´ë¯¸ ê°€ì ¸ì˜¨ ì»¬ëŸ¼ëª… ì‚¬ìš©
                df = pd.DataFrame(all_data, columns=columns)

                # ğŸš¨ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ - CSV ì €ì¥ ì „ ì²˜ë¦¬
                logger.info("=== ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘ ===")
                df = self._validate_and_convert_data_types(df, table_name)
                logger.info("=== ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì™„ë£Œ ===")

                # CSVë¡œ ì €ì¥
                df.to_csv(csv_path, index=False, encoding="utf-8")

                logger.info(
                    f"CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {table_name} -> {csv_path}, í–‰ ìˆ˜: {len(df)}"
                )
                return len(df)
            else:
                logger.warning(f"í…Œì´ë¸” {table_name}ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return 0

        except Exception as e:
            logger.error(f"CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {table_name} -> {csv_path}, ì˜¤ë¥˜: {e!s}")
            raise

    def _validate_and_convert_data_types(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
        """
        ë°ì´í„°í”„ë ˆì„ì˜ ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ìˆ˜í–‰

        Args:
            df: ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„
            table_name: í…Œì´ë¸”ëª… (ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒìš©)

        Returns:
            ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info(f"í…Œì´ë¸” {table_name}ì˜ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘")

            # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
            source_schema = self.db_ops.get_table_schema(table_name)
            if not source_schema or not source_schema.get("columns"):
                logger.warning(f"í…Œì´ë¸” {table_name}ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë³€í™˜ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                return self._basic_data_type_conversion(df)

            # ì»¬ëŸ¼ë³„ íƒ€ì… ì •ë³´ ë§¤í•‘
            column_types = {}
            for col in source_schema["columns"]:
                column_types[col["name"]] = col["type"]

            logger.info(f"ì»¬ëŸ¼ íƒ€ì… ì •ë³´: {column_types}")

            # ê° ì»¬ëŸ¼ë³„ë¡œ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            for col_name in df.columns:
                if col_name in column_types:
                    col_type = column_types[col_name]
                    logger.info(f"ì»¬ëŸ¼ {col_name} ({col_type}) ê²€ì¦ ë° ë³€í™˜ ì‹œì‘")

                    # ì»¬ëŸ¼ íƒ€ì…ë³„ ë³€í™˜ í•¨ìˆ˜ í˜¸ì¶œ
                    df[col_name] = self._convert_column_by_type(df[col_name], col_type, col_name)

                    # ë³€í™˜ í›„ ê²€ì¦
                    self._validate_column_after_conversion(df[col_name], col_type, col_name)
                else:
                    logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ íƒ€ì… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            logger.info(f"í…Œì´ë¸” {table_name}ì˜ ëª¨ë“  ì»¬ëŸ¼ ë³€í™˜ ì™„ë£Œ")
            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ë³€í™˜ì€ ìˆ˜í–‰
            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return self._basic_data_type_conversion(df)

    def _convert_column_by_type(self, column_series: pd.Series, col_type: str, col_name: str) -> pd.Series:
        """
        ì»¬ëŸ¼ íƒ€ì…ì— ë”°ë¼ ë°ì´í„° ë³€í™˜ ìˆ˜í–‰

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_type: ì»¬ëŸ¼ íƒ€ì…
            col_type: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            # ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬
            if col_name == "priority" and col_type in ["BIGINT", "INTEGER", "SMALLINT"]:
                logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘: {col_type}")
                return self._convert_priority_column(column_series)

            # ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            if col_type in ["BIGINT", "INTEGER", "SMALLINT"]:
                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_integer_column(column_series, col_name)

            # ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            elif col_type in ["DOUBLE PRECISION", "REAL", "NUMERIC", "DECIMAL"]:
                logger.info(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_float_column(column_series, col_name)

            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            elif col_type in ["DATE", "TIMESTAMP", "TIME"]:
                logger.info(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_datetime_column(column_series, col_name)

            # ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            elif col_type in ["BOOLEAN"]:
                logger.info(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_boolean_column(column_series, col_name)

            # ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
            else:
                logger.info(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜: {col_type}")
                return self._convert_string_column(column_series, col_name)

        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë°˜í™˜
            return column_series

    def _convert_priority_column(self, column_series: pd.Series) -> pd.Series:
        """
        PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ - ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info("ğŸš¨ PRIORITY ì»¬ëŸ¼ ë³€í™˜ ì‹œì‘")

            # í˜„ì¬ ê°’ í™•ì¸
            sample_values = column_series.head(10).tolist()
            logger.info(f"PRIORITY ì»¬ëŸ¼ í˜„ì¬ ê°’ ìƒ˜í”Œ: {sample_values}")

            # ì†Œìˆ˜ì  ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
            decimal_count = column_series.astype(str).str.contains(r'\.', na=False).sum()
            logger.info(f"PRIORITY ì»¬ëŸ¼ ì†Œìˆ˜ì  ê°’ ê°œìˆ˜: {decimal_count}")

            if decimal_count > 0:
                logger.info("ğŸš¨ PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

                # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                def convert_priority_value(x):
                    if pd.isna(x) or x is None:
                        return None
                    if isinstance(x, str):
                        x = x.strip()
                        if not x or x in ["", "nan", "None", "NULL", "null"]:
                            return None
                        if "." in x:
                            try:
                                float_val = float(x)
                                int_val = int(float_val)
                                logger.debug(f"PRIORITY ê°’ ë³€í™˜: '{x}' â†’ '{int_val}'")
                                return str(int_val)
                            except (ValueError, TypeError):
                                logger.warning(f"PRIORITY ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                return x
                    elif isinstance(x, (int, float)):
                        return str(int(x))
                    return str(x) if x is not None else None

                # ë³€í™˜ ì „í›„ ë¹„êµ
                before_values = column_series.copy()
                column_series = column_series.apply(convert_priority_value)
                after_values = column_series.copy()

                # ë³€í™˜ëœ ê°’ í™•ì¸
                changed_mask = before_values != after_values
                changed_count = changed_mask.sum()
                if changed_count > 0:
                    logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ {changed_count}ê°œ ê°’ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    # ë³€í™˜ëœ ê°’ë“¤ ë¡œê¹…
                    changed_indices = changed_mask[changed_mask].index
                    for idx in changed_indices[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                        logger.info(f"PRIORITY ë³€í™˜: í–‰ {idx}: '{before_values[idx]}' â†’ '{after_values[idx]}'")

                # ìµœì¢… ê²€ì¦
                final_decimal_count = column_series.astype(str).str.contains(r'\.', na=False).sum()
                if final_decimal_count == 0:
                    logger.info("âœ… PRIORITY ì»¬ëŸ¼ì˜ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    logger.warning(f"âš ï¸ PRIORITY ì»¬ëŸ¼ì— ì—¬ì „íˆ {final_decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            else:
                logger.info("âœ… PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")

            return column_series

        except Exception as e:
            logger.error(f"PRIORITY ì»¬ëŸ¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_integer_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ì†Œìˆ˜ì  ê°’, ë¹ˆ ë¬¸ìì—´, NaN ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
            def convert_decimal_to_int(x):
                if pd.isna(x) or x is None:
                    return None
                if isinstance(x, str):
                    x = x.strip()
                    if not x or x in ["", "nan", "None", "NULL", "null"]:
                        return None
                    if "." in x:
                        try:
                            float_val = float(x)
                            int_val = int(float_val)
                            return str(int_val)
                        except (ValueError, TypeError):
                            logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            return x
                elif isinstance(x, (int, float)):
                    return str(int(x))
                return str(x) if x is not None else None

            # ë³€í™˜ ì „í›„ ê°’ í™•ì¸
            before_values = column_series.copy()
            column_series = column_series.apply(convert_decimal_to_int)
            after_values = column_series.copy()

            # ë³€í™˜ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
            changed_count = sum(1 for b, a in zip(before_values, after_values) if b != a)
            if changed_count > 0:
                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ {changed_count}ê°œ ê°’ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_float_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¹ˆ ë¬¸ìì—´, NaN ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ì‹¤ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_datetime_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¹ˆ ë¬¸ìì—´, ì˜ëª»ëœ í˜•ì‹ ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_boolean_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¬¸ìì—´ ê°’ì„ ë¶ˆë¦°ìœ¼ë¡œ ë³€í™˜

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ë¬¸ìì—´ ê°’ì„ ë¶ˆë¦°ìœ¼ë¡œ ë³€í™˜
            def convert_to_boolean(x):
                if pd.isna(x) or x is None:
                    return None
                if isinstance(x, str):
                    x = x.strip().lower()
                    if x in ["true", "1", "yes", "on"]:
                        return "true"
                    elif x in ["false", "0", "no", "off"]:
                        return "false"
                    else:
                        return x
                elif isinstance(x, bool):
                    return str(x).lower()
                elif isinstance(x, (int, float)):
                    return "true" if x else "false"
                return str(x) if x is not None else None

            column_series = column_series.apply(convert_to_boolean)

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ë¶ˆë¦° íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _convert_string_column(self, column_series: pd.Series, col_name: str) -> pd.Series:
        """
        ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ ë³€í™˜ - ë¹ˆ ë¬¸ìì—´, NaN ì²˜ë¦¬

        Args:
            column_series: ë³€í™˜í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_name: ì»¬ëŸ¼ëª…

        Returns:
            ë³€í™˜ëœ ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
        """
        try:
            logger.info(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì‹œì‘")

            # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
            column_series = column_series.replace({
                "": None,
                " ": None,
                "nan": None,
                "None": None,
                "NULL": None,
                "null": None
            })

            # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
            column_series = column_series.replace({pd.NA: None, pd.NaT: None})
            logger.info(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì™„ë£Œ")

            return column_series

        except Exception as e:
            logger.error(f"ë¬¸ìì—´ íƒ€ì… ì»¬ëŸ¼ {col_name} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return column_series

    def _validate_column_after_conversion(self, column_series: pd.Series, col_type: str, col_name: str) -> None:
        """
        ë³€í™˜ í›„ ì»¬ëŸ¼ ê²€ì¦

        Args:
            column_series: ê²€ì¦í•  ì»¬ëŸ¼ ì‹œë¦¬ì¦ˆ
            col_type: ì»¬ëŸ¼ íƒ€ì…
            col_name: ì»¬ëŸ¼ëª…
        """
        try:
            # ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ì˜ ê²½ìš° ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
            if col_type in ["BIGINT", "INTEGER", "SMALLINT"]:
                decimal_count = column_series.astype(str).str.contains(r'\.', na=False).sum()
                if decimal_count > 0:
                    logger.warning(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                    # ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤ì„ ë¡œê¹…
                    problem_values = column_series[column_series.astype(str).str.contains(r'\.', na=False)]
                    logger.warning(f"ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤: {problem_values.head(5).tolist()}")
                else:
                    logger.info(f"ì»¬ëŸ¼ {col_name} ê²€ì¦ ì™„ë£Œ: ì†Œìˆ˜ì  ê°’ ì—†ìŒ")

            # ëª¨ë“  íƒ€ì… ì»¬ëŸ¼ì˜ ê²½ìš° ë¹ˆ ë¬¸ìì—´ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
            empty_count = (column_series.astype(str).str.strip() == "").sum()
            if empty_count > 0:
                logger.warning(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {empty_count}ê°œì˜ ë¹ˆ ë¬¸ìì—´ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
            else:
                logger.info(f"ì»¬ëŸ¼ {col_name} ê²€ì¦ ì™„ë£Œ: ë¹ˆ ë¬¸ìì—´ ì—†ìŒ")

        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ {col_name} ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _basic_data_type_conversion(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ (ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì„ ë•Œ ì‚¬ìš©)

        Args:
            df: ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„

        Returns:
            ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ ì‹œì‘")

            for col_name in df.columns:
                # ëª¨ë“  ì»¬ëŸ¼ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì²˜ë¦¬í•  ë³€í™˜
                df[col_name] = df[col_name].replace({
                    "": None,
                    " ": None,
                    "nan": None,
                    "None": None,
                    "NULL": None,
                    "null": None
                })

                # pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
                df[col_name] = df[col_name].replace({pd.NA: None, pd.NaT: None})

            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ")
            return df

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return df

    def _validate_and_convert_data_types_after_csv_read(self, df: pd.DataFrame, source_schema: dict) -> pd.DataFrame:
        """
        CSV ì½ê¸° í›„ ë°ì´í„°í”„ë ˆì„ì˜ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ìˆ˜í–‰

        Args:
            df: ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„
            source_schema: ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´

        Returns:
            ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info("=== CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘ ===")

            # ì»¬ëŸ¼ë³„ íƒ€ì… ì •ë³´ ë§¤í•‘
            column_types = {}
            for col in source_schema["columns"]:
                column_types[col["name"]] = col["type"]

            logger.info(f"ì»¬ëŸ¼ íƒ€ì… ì •ë³´: {column_types}")

            # ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬
            logger.info("=== ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘ ===")
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                if col_name == "priority" and col_type in ["BIGINT", "INTEGER"]:
                    logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ ë°œê²¬! íƒ€ì…: {col_type}")
                    if col_name in df.columns:
                        # í˜„ì¬ ê°’ í™•ì¸
                        sample_values = df[col_name].head(10).tolist()
                        logger.info(f"PRIORITY ì»¬ëŸ¼ í˜„ì¬ ê°’ ìƒ˜í”Œ: {sample_values}")

                        # ì†Œìˆ˜ì  ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                        decimal_count = df[col_name].astype(str).str.contains(r'\.', na=False).sum()
                        logger.info(f"PRIORITY ì»¬ëŸ¼ ì†Œìˆ˜ì  ê°’ ê°œìˆ˜: {decimal_count}")

                        if decimal_count > 0:
                            logger.info("ğŸš¨ PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

                            # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                            def convert_priority_value(x):
                                if pd.isna(x) or x is None:
                                    return None
                                if isinstance(x, str):
                                    x = x.strip()
                                    if not x or x in ["", "nan", "None", "NULL", "null"]:
                                        return None
                                    if "." in x:
                                        try:
                                            float_val = float(x)
                                            int_val = int(float_val)
                                            logger.debug(f"PRIORITY ê°’ ë³€í™˜: '{x}' â†’ '{int_val}'")
                                            return str(int_val)
                                        except (ValueError, TypeError):
                                            logger.warning(f"PRIORITY ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                            return x
                                elif isinstance(x, (int, float)):
                                    return str(int(x))
                                return str(x) if x is not None else None

                            # ë³€í™˜ ì „í›„ ë¹„êµ
                            before_values = df[col_name].copy()
                            df[col_name] = df[col_name].apply(convert_priority_value)
                            after_values = df[col_name].copy()

                            # ë³€í™˜ëœ ê°’ í™•ì¸
                            changed_mask = before_values != after_values
                            changed_count = changed_mask.sum()
                            if changed_count > 0:
                                logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ {changed_count}ê°œ ê°’ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                # ë³€í™˜ëœ ê°’ë“¤ ë¡œê¹…
                                changed_indices = changed_mask[changed_mask].index
                                for idx in changed_indices[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                                    logger.info(f"PRIORITY ë³€í™˜: í–‰ {idx}: '{before_values[idx]}' â†’ '{after_values[idx]}'")

                            # ìµœì¢… ê²€ì¦
                            final_decimal_count = df[col_name].astype(str).str.contains(r'\.', na=False).sum()
                            if final_decimal_count == 0:
                                logger.info("âœ… PRIORITY ì»¬ëŸ¼ì˜ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            else:
                                logger.warning(f"âš ï¸ PRIORITY ì»¬ëŸ¼ì— ì—¬ì „íˆ {final_decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
                        else:
                            logger.info("âœ… PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        logger.warning(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            # ê° ì»¬ëŸ¼ë³„ë¡œ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                if col_name in df.columns:
                    logger.info(f"ì»¬ëŸ¼ {col_name} ({col_type}) ê²€ì¦ ë° ë³€í™˜ ì‹œì‘")

                    # ì»¬ëŸ¼ íƒ€ì…ë³„ ë³€í™˜ í•¨ìˆ˜ í˜¸ì¶œ
                    df[col_name] = self._convert_column_by_type(df[col_name], col_type, col_name)

                    # ë³€í™˜ í›„ ê²€ì¦
                    self._validate_column_after_conversion(df[col_name], col_type, col_name)
                else:
                    logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ íƒ€ì… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            logger.info("=== CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì™„ë£Œ ===")
            return df

        except Exception as e:
            logger.error(f"CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ë³€í™˜ì€ ìˆ˜í–‰
            logger.info("ê¸°ë³¸ ë°ì´í„° íƒ€ì… ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return self._basic_data_type_conversion(df)

    def create_temp_table_and_import_csv(
        self, target_table: str, source_schema: dict[str, Any], csv_path: str, batch_size: int = 1000
    ) -> tuple[str, int]:
        """
        ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸°ë¥¼ í•˜ë‚˜ì˜ ì—°ê²°ì—ì„œ ì‹¤í–‰

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            (ì„ì‹œ í…Œì´ë¸”ëª…, ê°€ì ¸ì˜¨ í–‰ ìˆ˜) íŠœí”Œ
        """
        try:
            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„±
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]  # data_type -> typeìœ¼ë¡œ ë³€ê²½
                is_nullable = col["nullable"]  # nullable ì •ë³´ ë³µì›

                # PostgreSQL ë°ì´í„° íƒ€ì… ë§¤í•‘
                if col_type.upper() in ["VARCHAR", "CHAR", "TEXT"]:
                    pg_type = "TEXT"
                elif (
                    col_type.upper() in ["INTEGER", "INT", "BIGINT", "SMALLINT"]
                    or col_type.upper()
                    in ["DECIMAL", "NUMERIC", "REAL", "DOUBLE PRECISION"]
                    or col_type.upper() in ["DATE", "TIMESTAMP", "TIME"]
                ):
                    pg_type = "TEXT"  # ì•ˆì „ì„±ì„ ìœ„í•´ TEXTë¡œ ë³€í™˜
                else:
                    pg_type = "TEXT"  # ê¸°ë³¸ê°’

                # ì›ë³¸ ìŠ¤í‚¤ë§ˆì˜ nullable ì •ë³´ë¥¼ ìœ ì§€
                nullable_clause = "NOT NULL" if not is_nullable else ""
                column_definitions.append(
                    f"{col_name} {pg_type} {nullable_clause}".strip()
                )

            # CREATE TABLE ë¬¸ ìƒì„± (ì„ì‹œ í…Œì´ë¸” ëŒ€ì‹  ì˜êµ¬ í…Œì´ë¸” ì‚¬ìš©)
            create_sql = f"""
                CREATE TABLE {temp_table} (
                    {', '.join(column_definitions)}
                )
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_sql}")

            # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            target_hook = self.target_hook

            # í•˜ë‚˜ì˜ ì—°ê²°ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸° ì‹¤í–‰
            with target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # 1. ì„ì‹œ í…Œì´ë¸” ìƒì„±
                    cursor.execute(create_sql)
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # 2. CSV íŒŒì¼ ì½ê¸° (NA ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ì˜ëª» ì¸ì‹í•˜ì§€ ì•Šë„ë¡ ì„¤ì •)
                    df = pd.read_csv(
                        csv_path, encoding="utf-8", na_values=[], keep_default_na=False
                    )

                    if df.empty:
                        logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        return temp_table, 0

                    # 3. ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                    if source_schema and source_schema.get("columns"):
                        temp_columns = [col["name"] for col in source_schema["columns"]]
                        logger.info(
                            f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {temp_columns}"
                        )

                        # CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬
                        df_reordered = df[temp_columns]
                        logger.info(
                            f"CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬í–ˆìŠµë‹ˆë‹¤: {temp_columns}"
                        )
                    else:
                        temp_columns = list(df.columns)
                        logger.info(
                            f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì–´ CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤: {temp_columns}"
                        )
                        df_reordered = df

                    # 4. ë°ì´í„° íƒ€ì… ë³€í™˜ ë° null ê°’ ê²€ì¦
                    # NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼ë“¤ í™•ì¸
                    not_null_columns = []
                    for col in source_schema["columns"]:
                        if not col["nullable"]:
                            not_null_columns.append(col["name"])

                    logger.info(f"NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼: {not_null_columns}")

                    # null ê°’ì´ ìˆëŠ” í–‰ ê²€ì¦ (ì‹¤ì œ null ê°’ë§Œ, "NA" ë¬¸ìì—´ì€ ì œì™¸)
                    null_violations = []
                    for col_name in not_null_columns:
                        if col_name in df_reordered.columns:
                            # ì‹¤ì œ null ê°’ë§Œ ê²€ì‚¬ (None, numpy.nan ë“±)
                            null_rows = df_reordered[df_reordered[col_name].isna()]
                            if not null_rows.empty:
                                null_violations.append(
                                    {
                                        "column": col_name,
                                        "count": len(null_rows),
                                        "sample_rows": null_rows.head(3).to_dict(
                                            "records"
                                        ),
                                    }
                                )

                    if null_violations:
                        logger.warning(
                            f"NOT NULL ì œì•½ì¡°ê±´ ìœ„ë°˜ ë°œê²¬: {null_violations}"
                        )
                        # null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œë¡œ ì²˜ë¦¬
                        for col_name in not_null_columns:
                            if col_name in df_reordered.columns:
                                df_reordered[col_name] = df_reordered[col_name].fillna(
                                    ""
                                )
                                logger.info(
                                    f"ì»¬ëŸ¼ {col_name}ì˜ null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤."
                                )

                    # ì¼ë°˜ì ì¸ ë°ì´í„° íƒ€ì… ë³€í™˜
                    for col in df_reordered.columns:
                        # ì‹¤ì œ null ê°’ë§Œ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜ ("NA" ë¬¸ìì—´ì€ ë³´ì¡´)
                        df_reordered[col] = df_reordered[col].fillna("")

                        # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                        if df_reordered[col].dtype in ["int64", "float64"]:
                            df_reordered[col] = df_reordered[col].astype(str)

                    logger.info(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ: {len(df_reordered)}í–‰")

                    # ë°ì´í„° ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                    logger.info("ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3í–‰):")
                    for i, row in df_reordered.head(3).iterrows():
                        logger.info(f"í–‰ {i}: {dict(row)}")

                    # 5. INSERT ì‹¤í–‰
                    total_inserted = 0

                    for i in range(0, len(df_reordered), batch_size):
                        batch_df = df_reordered.iloc[i : i + batch_size]
                        batch_data = [tuple(row) for row in batch_df.values]

                        insert_query = f"""
                            INSERT INTO {temp_table} ({', '.join(temp_columns)})
                            VALUES ({', '.join(['%s'] * len(temp_columns))})
                        """

                        cursor.executemany(insert_query, batch_data)
                        total_inserted += len(batch_data)

                        if i % 10000 == 0:
                            logger.info(
                                f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}"
                            )

                    # ì»¤ë°‹
                    conn.commit()

                    logger.info(
                        f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}"
                    )
                    return temp_table, total_inserted

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise Exception(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    def create_temp_table(
        self, target_table: str, source_schema: dict[str, Any]
    ) -> str:
        """
        ì„ì‹œ í…Œì´ë¸” ìƒì„±

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´

        Returns:
            ìƒì„±ëœ ì„ì‹œ í…Œì´ë¸”ëª…
        """
        try:
            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„±
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                # PostgreSQL íƒ€ì… ë§¤í•‘ - _convert_to_postgres_type ë©”ì„œë“œ ì‚¬ìš©
                col_type = self._convert_to_postgres_type(col_type, col.get('max_length'))

                nullable = "" if col["nullable"] else " NOT NULL"
                column_definitions.append(f"{col_name} {col_type}{nullable}")

            # ì„ì‹œ í…Œì´ë¸” ìƒì„± (ìŠ¤í‚¤ë§ˆë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •)
            create_temp_table_sql = f"""
                CREATE TEMP TABLE {temp_table} (
                    {', '.join(column_definitions)}
                ) ON COMMIT DROP
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_temp_table_sql}")

            # ê°™ì€ ì—°ê²°ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(create_temp_table_sql)
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # ì„ì‹œ í…Œì´ë¸”ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    cursor.execute(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{temp_table}')")
                    exists = cursor.fetchone()[0]
                    if not exists:
                        raise Exception(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                    return temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {target_table}, ì˜¤ë¥˜: {e!s}")
            raise

    def import_from_csv(
        self,
        csv_path: str,
        temp_table: str,
        source_schema: dict[str, Any] | None = None,
        batch_size: int = 1000,
    ) -> int:
        """
        CSVë¥¼ ì„ì‹œ í…Œì´ë¸”ë¡œ ê°€ì ¸ì˜¤ê¸°

        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            temp_table: ì„ì‹œ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì„ íƒì‚¬í•­)
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ê°€ì ¸ì˜¨ í–‰ ìˆ˜
        """
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_path, encoding="utf-8")

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ì„ì‹œ í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            try:
                if source_schema and source_schema.get("columns"):
                    # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                    schema_columns = [col["name"] for col in source_schema["columns"]]
                    logger.info(
                        f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {schema_columns}"
                    )

                    # CSV ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                    csv_columns = list(df.columns)
                    logger.info(f"CSV íŒŒì¼ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª…: {csv_columns}")

                    # CSV ì»¬ëŸ¼ëª…ê³¼ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ëª…ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if len(csv_columns) != len(schema_columns):
                        logger.warning(
                            f"CSV ì»¬ëŸ¼ ìˆ˜({len(csv_columns)})ì™€ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ ìˆ˜({len(schema_columns)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

                    # CSVì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ì•ˆì „í•œ ì²˜ë¦¬)
                    available_columns = [col for col in schema_columns if col in csv_columns]
                    missing_columns = [col for col in schema_columns if col not in csv_columns]

                    if missing_columns:
                        logger.warning(f"CSVì— ì—†ëŠ” ì»¬ëŸ¼: {missing_columns}")

                    if not available_columns:
                        logger.warning("CSVì™€ ìŠ¤í‚¤ë§ˆ ê°„ì— ê³µí†µ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        available_columns = csv_columns

                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ì¬êµ¬ì„±
                    df_reordered = df[available_columns]
                    temp_columns = available_columns

                    logger.info(
                        f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ì¬êµ¬ì„±: {len(available_columns)}ê°œ ì»¬ëŸ¼"
                    )
                    logger.info(f"ìµœì¢… ì‚¬ìš© ì»¬ëŸ¼: {temp_columns}")
                    logger.info(f"ë°ì´í„°í”„ë ˆì„ í˜•íƒœ: {df_reordered.shape}")
                else:
                    # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    temp_columns = list(df.columns)
                    logger.info(
                        f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì–´ CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤: {temp_columns}"
                    )
                    df_reordered = df

            except Exception as e:
                logger.error(f"ì»¬ëŸ¼ ì •ë³´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise

            # ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ê²€ì¦
            try:
                # ë°ì´í„°í”„ë ˆì„ì˜ ë°ì´í„° íƒ€ì…ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
                for col in df_reordered.columns:
                    # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                    df_reordered[col] = df_reordered[col].where(
                        pd.notna(df_reordered[col]), None
                    )

                    # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    if df_reordered[col].dtype in ["int64", "float64"]:
                        df_reordered[col] = df_reordered[col].astype(str)

                logger.info(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ: {len(df_reordered)}í–‰")

            except Exception as e:
                logger.error(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {e}")
                raise Exception(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {e}")

            # INSERT ì‹¤í–‰
            try:
                # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
                total_inserted = 0

                # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
                target_hook = self.target_hook

                with target_hook.get_conn() as conn:
                    with conn.cursor() as cursor:
                        for i in range(0, len(df_reordered), batch_size):
                            batch_df = df_reordered.iloc[i : i + batch_size]
                            batch_data = [tuple(row) for row in batch_df.values]

                            # INSERT ë¬¸ ì‹¤í–‰
                            insert_query = f"""
                                INSERT INTO {temp_table} ({', '.join(temp_columns)})
                                VALUES ({', '.join(['%s'] * len(temp_columns))})
                            """

                            cursor.executemany(insert_query, batch_data)
                            total_inserted += len(batch_data)

                            if i % 10000 == 0:
                                logger.info(
                                    f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}"
                                )

                        # ì»¤ë°‹
                        conn.commit()

                logger.info(
                    f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}"
                )
                return total_inserted

            except Exception as e:
                logger.error(f"INSERT ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                raise Exception(f"INSERT ì‹¤í–‰ ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {csv_path} -> {temp_table}, ì˜¤ë¥˜: {e!s}")
            raise

    def execute_merge_operation(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "full_sync",
    ) -> dict[str, Any]:
        """
        MERGE ì‘ì—… ì‹¤í–‰

        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª… (ì„ì‹œ í…Œì´ë¸”)
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            sync_mode: ë™ê¸°í™” ëª¨ë“œ ('full_sync' ë˜ëŠ” 'incremental_sync')

        Returns:
            MERGE ì‘ì—… ê²°ê³¼
        """
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            non_pk_columns = self.target_hook.get_records(all_columns_query)
            non_pk_column_names = [col[0] for col in non_pk_columns]

            # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not non_pk_column_names:
                logger.warning(
                    "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                )
                # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if sync_mode == "full_sync":
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # MERGE ì‹¤í–‰
                start_time = pd.Timestamp.now()
                self.target_hook.run(merge_sql)
                end_time = pd.Timestamp.now()

                # ê²°ê³¼ í™•ì¸
                source_count = self.db_ops.get_table_row_count(source_table)
                target_count = self.db_ops.get_table_row_count(target_table)

                merge_result = {
                    "source_count": source_count,
                    "target_count": target_count,
                    "sync_mode": sync_mode,
                    "execution_time": (end_time - start_time).total_seconds(),
                    "status": "success",
                    "message": (
                        f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                        f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                    ),
                }

                logger.info(merge_result["message"])
                return merge_result

            # MERGE ì¿¼ë¦¬ ìƒì„±
            if sync_mode == "full_sync":
                # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                if non_pk_column_names:
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì…
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
            else:
                # ì¦ë¶„ ë™ê¸°í™”: UPSERT (INSERT ... ON CONFLICT)
                if non_pk_column_names:
                    update_set_clause = ", ".join(
                        [f"{col} = EXCLUDED.{col}" for col in non_pk_column_names]
                    )

                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO UPDATE SET {update_set_clause};
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

            # MERGE ì‹¤í–‰
            start_time = pd.Timestamp.now()
            self.target_hook.run(merge_sql)
            end_time = pd.Timestamp.now()

            # ê²°ê³¼ í™•ì¸
            source_count = self.db_ops.get_table_row_count(source_table)
            target_count = self.db_ops.get_table_row_count(target_table)

            merge_result = {
                "source_count": source_count,
                "target_count": target_count,
                "sync_mode": sync_mode,
                "execution_time": (end_time - start_time).total_seconds(),
                "status": "success",
                "message": (
                    f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                ),
            }

            logger.info(merge_result["message"])
            return merge_result

        except Exception as e:
            logger.error(
                f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e!s}"
            )
            raise

    def cleanup_temp_files(self, csv_path: str):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        try:
            if os.path.exists(csv_path):
                os.remove(csv_path)
                logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {csv_path}")
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {csv_path}, ì˜¤ë¥˜: {e!s}")

    def _convert_to_postgres_type(
        self, col_type: str, max_length: int | None = None
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…ì„ PostgreSQL íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        col_type_lower = col_type.lower()

        # ë¬¸ìì—´ íƒ€ì…
        if "char" in col_type_lower or "text" in col_type_lower:
            if max_length and max_length > 0:
                return f"VARCHAR({max_length})"
            else:
                return "TEXT"

        # ìˆ«ì íƒ€ì…
        elif "int" in col_type_lower:
            if "bigint" in col_type_lower:
                return "BIGINT"
            elif "smallint" in col_type_lower:
                return "SMALLINT"
            else:
                return "INTEGER"

        elif "decimal" in col_type_lower or "numeric" in col_type_lower:
            return "NUMERIC"

        elif "float" in col_type_lower or "double" in col_type_lower:
            return "DOUBLE PRECISION"

        elif "real" in col_type_lower:
            return "REAL"

        # ë‚ ì§œ/ì‹œê°„ íƒ€ì…
        elif "date" in col_type_lower:
            return "DATE"

        elif "time" in col_type_lower:
            if "timestamp" in col_type_lower:
                return "TIMESTAMP"
            else:
                return "TIME"

        # ë¶ˆë¦° íƒ€ì…
        elif "bool" in col_type_lower:
            return "BOOLEAN"

        # ê¸°íƒ€ íƒ€ì…
        elif "json" in col_type_lower:
            return "JSONB"

        elif "uuid" in col_type_lower:
            return "UUID"

        # ê¸°ë³¸ê°’ - ì•ˆì „í•˜ê²Œ TEXTë¡œ ì²˜ë¦¬
        else:
            return "TEXT"

    def _create_temp_table_in_session(
        self, cursor, target_table: str, source_schema: dict
    ) -> str:
        """í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±"""
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            timestamp = int(pd.Timestamp.now().timestamp())
            temp_table = f"temp_{schema}_{table}_{timestamp}"

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„± - ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ì‹¤ì œ ë°ì´í„° íƒ€ì… ì‚¬ìš©
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]
                is_nullable = col["nullable"]

                # ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ì‹¤ì œ ë°ì´í„° íƒ€ì…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # PostgreSQLê³¼ í˜¸í™˜ë˜ëŠ” íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                pg_type = self._convert_to_postgres_type(
                    col_type, col.get("max_length")
                )

                # ìˆ«ì íƒ€ì… ì»¬ëŸ¼ì€ NULL í—ˆìš©ìœ¼ë¡œ ì„¤ì • (CSV ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬)
                # DOUBLE PRECISION, BIGINT, INTEGER ë“±ì—ì„œ ë¹ˆ ë¬¸ìì—´ ì˜¤ë¥˜ ë°©ì§€
                if pg_type in ["DOUBLE PRECISION", "BIGINT", "INTEGER", "NUMERIC", "REAL"]:
                    nullable_clause = ""  # NULL í—ˆìš©
                    logger.info(f"ìˆ«ì íƒ€ì… ì»¬ëŸ¼ {col_name} ({pg_type})ì„ NULL í—ˆìš©ìœ¼ë¡œ ì„¤ì •")
                else:
                    # ì›ë³¸ ìŠ¤í‚¤ë§ˆì˜ nullable ì •ë³´ë¥¼ ìœ ì§€
                    nullable_clause = "NOT NULL" if not is_nullable else ""

                column_definitions.append(
                    f"{col_name} {pg_type} {nullable_clause}".strip()
                )

            # CREATE TABLE ë¬¸ ìƒì„± (ì„ì‹œ í…Œì´ë¸” ëŒ€ì‹  ì˜êµ¬ í…Œì´ë¸” ì‚¬ìš©)
            create_sql = f"""
                CREATE TABLE {temp_table} (
                    {', '.join(column_definitions)}
                )
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_sql}")

            # ì„ì‹œ í…Œì´ë¸” ìƒì„±
            cursor.execute(create_sql)
            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

            return temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            raise Exception(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")

    def _import_csv_in_session(
        self, cursor, temp_table: str, csv_path: str, source_schema: dict, batch_size: int = 1000
    ) -> int:
        """í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ CSV ë°ì´í„°ë¥¼ ì„ì‹œ í…Œì´ë¸”ì— ì‚½ì…"""
        try:
            # CSV íŒŒì¼ ì½ê¸° (NA ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ì˜ëª» ì¸ì‹í•˜ì§€ ì•Šë„ë¡ ì„¤ì •)
            df = pd.read_csv(
                csv_path, encoding="utf-8", na_values=[], keep_default_na=False
            )

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            if source_schema and source_schema.get("columns"):
                temp_columns = [col["name"] for col in source_schema["columns"]]
                logger.info(f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {temp_columns}")

                # CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬
                df_reordered = df[temp_columns]
                logger.info(
                    f"CSV ì»¬ëŸ¼ì„ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬í–ˆìŠµë‹ˆë‹¤: {temp_columns}"
                )
            else:
                temp_columns = list(df.columns)
                logger.info(
                    f"ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ì–´ CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤: {temp_columns}"
                )
                df_reordered = df

            # ğŸš¨ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ - CSV ì½ê¸° í›„ ì¬ì‹¤í–‰
            logger.info("=== ğŸš¨ CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì‹œì‘ ===")
            df_reordered = self._validate_and_convert_data_types_after_csv_read(df_reordered, source_schema)
            logger.info("=== CSV ì½ê¸° í›„ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜ ì™„ë£Œ ===")

            # ë°ì´í„° íƒ€ì… ë³€í™˜ ë° null ê°’ ê²€ì¦
            # NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼ë“¤ í™•ì¸
            not_null_columns = []
            for col in source_schema["columns"]:
                if not col["nullable"]:
                    not_null_columns.append(col["name"])

            logger.info(f"NOT NULL ì œì•½ì¡°ê±´ì´ ìˆëŠ” ì»¬ëŸ¼: {not_null_columns}")

            # null ê°’ì´ ìˆëŠ” í–‰ ê²€ì¦ (ì‹¤ì œ null ê°’ë§Œ, "NA" ë¬¸ìì—´ì€ ì œì™¸)
            null_violations = []
            for col_name in not_null_columns:
                if col_name in df_reordered.columns:
                    # ì‹¤ì œ null ê°’ë§Œ ê²€ì‚¬ (None, numpy.nan ë“±)
                    null_rows = df_reordered[df_reordered[col_name].isna()]
                    if not null_rows.empty:
                        null_violations.append(
                            {
                                "column": col_name,
                                "count": len(null_rows),
                                "sample_rows": null_rows.head(3).to_dict("records"),
                            }
                        )

            if null_violations:
                logger.warning(f"NOT NULL ì œì•½ì¡°ê±´ ìœ„ë°˜ ë°œê²¬: {null_violations}")
                # null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œë¡œ ì²˜ë¦¬
                for col_name in not_null_columns:
                    if col_name in df_reordered.columns:
                        df_reordered[col_name] = df_reordered[col_name].fillna("")
                        logger.info(
                            f"ì»¬ëŸ¼ {col_name}ì˜ null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤."
                        )

            # ì¼ë°˜ì ì¸ ë°ì´í„° íƒ€ì… ë³€í™˜
            for col in df_reordered.columns:
                # ì‹¤ì œ null ê°’ë§Œ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜ ("NA" ë¬¸ìì—´ì€ ë³´ì¡´)
                df_reordered[col] = df_reordered[col].fillna("")

                # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                if df_reordered[col].dtype in ["int64", "float64"]:
                    df_reordered[col] = df_reordered[col].astype(str)

                # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
                df_reordered[col] = df_reordered[col].astype(str)

            # ìˆ«ì íƒ€ì… ì»¬ëŸ¼ì˜ ë¹ˆ ë¬¸ìì—´ì„ NULLë¡œ ë³€í™˜
            # PostgreSQLì—ì„œ ë¹ˆ ë¬¸ìì—´ì„ ìˆ«ì íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì˜¤ë¥˜ ë°©ì§€
            logger.info(f"=== ì»¬ëŸ¼ íƒ€ì… ê²€ì‚¬ ì‹œì‘ ===")
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]
                logger.info(f"ì»¬ëŸ¼ {col_name}: íƒ€ì…={col_type}")

                # priority ì»¬ëŸ¼ íŠ¹ë³„ ë¡œê¹…
                if col_name == "priority":
                    logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ ë°œê²¬! íƒ€ì…: {col_type}, BIGINT í¬í•¨ ì—¬ë¶€: {col_type in ['BIGINT', 'INTEGER']}")
                    logger.info(f"ë°ì´í„°í”„ë ˆì„ì— ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€: {col_name in df_reordered.columns}")
                    if col_name in df_reordered.columns:
                        sample_values = df_reordered[col_name].head(5).tolist()
                        logger.info(f"PRIORITY ì»¬ëŸ¼ ìƒ˜í”Œ ê°’: {sample_values}")

                if col_name in df_reordered.columns and col_type in ["DOUBLE PRECISION", "BIGINT", "INTEGER", "NUMERIC", "REAL"]:
                    # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'nan' ë¬¸ìì—´ì„ None(NULL)ë¡œ ë³€í™˜
                    df_reordered[col_name] = df_reordered[col_name].replace({
                        "": None,
                        " ": None,
                        "nan": None,
                        "None": None,
                        "NULL": None,
                        "null": None
                    })

                    # BIGINT/INTEGER ì»¬ëŸ¼ì˜ ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                    logger.info(f"ì»¬ëŸ¼ {col_name} íƒ€ì… ê²€ì‚¬: {col_type} (BIGINT/INTEGER í¬í•¨ ì—¬ë¶€: {col_type in ['BIGINT', 'INTEGER']})")
                    if col_type in ["BIGINT", "INTEGER"]:
                        logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ ì†Œìˆ˜ì  ê°’ ë³€í™˜ ì‹œì‘")
                        try:
                            # ë” ê°•ë ¥í•œ ì†Œìˆ˜ì  ê°’ ë³€í™˜ ë¡œì§
                            def convert_decimal_to_int(x):
                                if pd.isna(x) or x is None:
                                    return None
                                if isinstance(x, str):
                                    x = x.strip()
                                    if not x or x in ["", "nan", "None", "NULL", "null"]:
                                        return None
                                    if "." in x:
                                        try:
                                            # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                                            float_val = float(x)
                                            int_val = int(float_val)
                                            return str(int_val)
                                        except (ValueError, TypeError):
                                            logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{x}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                            return x
                                elif isinstance(x, (int, float)):
                                    return str(int(x))
                                return str(x) if x is not None else None

                            # ë³€í™˜ ì „í›„ ê°’ í™•ì¸
                            before_values = df_reordered[col_name].tolist()
                            df_reordered[col_name] = df_reordered[col_name].apply(convert_decimal_to_int)
                            after_values = df_reordered[col_name].tolist()

                            # ë³€í™˜ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                            changed_count = sum(1 for b, a in zip(before_values, after_values) if b != a)
                            if changed_count > 0:
                                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ {changed_count}ê°œ ì†Œìˆ˜ì  ê°’ì´ ì •ìˆ˜ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                # ë³€í™˜ëœ ê°’ ìƒ˜í”Œ ë¡œê¹…
                                for i, (b, a) in enumerate(zip(before_values, after_values)):
                                    if b != a:
                                        logger.info(f"ì»¬ëŸ¼ {col_name} í–‰ {i}: '{b}' â†’ '{a}'")
                                        if i >= 2:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…
                                            break
                            else:
                                logger.info(f"ì •ìˆ˜ íƒ€ì… ì»¬ëŸ¼ {col_name}ì— ë³€í™˜í•  ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")

                            # ë³€í™˜ í›„ ì¶”ê°€ ê²€ì¦: ì—¬ì „íˆ ì†Œìˆ˜ì ì´ ìˆëŠ” ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                            remaining_decimals = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                            if remaining_decimals > 0:
                                logger.warning(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {remaining_decimals}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
                                # ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤ì„ ë¡œê¹…
                                problem_values = df_reordered[col_name][df_reordered[col_name].astype(str).str.contains(r'\.', na=False)]
                                logger.warning(f"ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤: {problem_values.head(5).tolist()}")

                                # ê°•ì œë¡œ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                                logger.info(f"ì»¬ëŸ¼ {col_name}ì˜ ë‚¨ì€ ì†Œìˆ˜ì  ê°’ë“¤ì„ ê°•ì œ ë³€í™˜í•©ë‹ˆë‹¤.")
                                def force_convert_decimal(x):
                                    if pd.isna(x) or x is None:
                                        return None
                                    if isinstance(x, str) and "." in x:
                                        try:
                                            return str(int(float(x)))
                                        except (ValueError, TypeError):
                                            logger.error(f"ê°•ì œ ë³€í™˜ ì‹¤íŒ¨: '{x}' â†’ NULLë¡œ ì„¤ì •")
                                            return None
                                    return x

                                df_reordered[col_name] = df_reordered[col_name].apply(force_convert_decimal)

                                # ìµœì¢… ê²€ì¦
                                final_decimals = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                                if final_decimals > 0:
                                    logger.error(f"ì»¬ëŸ¼ {col_name}ì— ì—¬ì „íˆ {final_decimals}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                                else:
                                    logger.info(f"ì»¬ëŸ¼ {col_name}ì˜ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ì†Œìˆ˜ì  ê°’ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                    # ì¶”ê°€ë¡œ pandasì˜ NaN ê°’ë„ Noneìœ¼ë¡œ ë³€í™˜
                    df_reordered[col_name] = df_reordered[col_name].replace({pd.NA: None, pd.NaT: None})
                    logger.info(f"ìˆ«ì íƒ€ì… ì»¬ëŸ¼ {col_name}ì˜ ë¹ˆ ë¬¸ìì—´ê³¼ NaNì„ NULLë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")

            logger.info(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ: {len(df_reordered)}í–‰")

            # ğŸš¨ CSV ì €ì¥ ì „ ìµœì¢… PRIORITY ì»¬ëŸ¼ ê²€ì¦
            logger.info("=== ğŸš¨ CSV ì €ì¥ ì „ ìµœì¢… PRIORITY ì»¬ëŸ¼ ê²€ì¦ ===")
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]

                if col_name == "priority" and col_type in ["BIGINT", "INTEGER"]:
                    if col_name in df_reordered.columns:
                        # ìµœì¢… ì†Œìˆ˜ì  ê°’ ê²€ì‚¬
                        final_decimal_count = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                        if final_decimal_count > 0:
                            logger.error(f"ğŸš¨ ğŸš¨ ğŸš¨ CSV ì €ì¥ ì „ PRIORITY ì»¬ëŸ¼ì— ì—¬ì „íˆ {final_decimal_count}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                            # ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤ì„ ê°•ì œë¡œ ë³€í™˜
                            problem_values = df_reordered[col_name][df_reordered[col_name].astype(str).str.contains(r'\.', na=False)]
                            logger.error(f"ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤: {problem_values.head(10).tolist()}")

                            # ê°•ì œ ë³€í™˜
                            def force_convert_final(x):
                                if pd.isna(x) or x is None:
                                    return None
                                if isinstance(x, str) and "." in x:
                                    try:
                                        return str(int(float(x)))
                                    except (ValueError, TypeError):
                                        logger.error(f"ìµœì¢… ê°•ì œ ë³€í™˜ ì‹¤íŒ¨: '{x}' â†’ NULLë¡œ ì„¤ì •")
                                        return None
                                return x

                            df_reordered[col_name] = df_reordered[col_name].apply(force_convert_final)

                            # ìµœì¢… ê²€ì¦
                            final_check = df_reordered[col_name].astype(str).str.contains(r'\.', na=False).sum()
                            if final_check == 0:
                                logger.info("âœ… ìµœì¢… ê°•ì œ ë³€í™˜ í›„ ëª¨ë“  ì†Œìˆ˜ì  ê°’ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            else:
                                logger.error(f"ğŸš¨ ğŸš¨ ğŸš¨ ìµœì¢… ê°•ì œ ë³€í™˜ í›„ì—ë„ {final_check}ê°œì˜ ì†Œìˆ˜ì  ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤!")
                        else:
                            logger.info("âœ… PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ì´ ì—†ìŠµë‹ˆë‹¤. CSV ì €ì¥ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

            # ë°ì´í„° ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            logger.info("ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3í–‰):")
            for i, row in df_reordered.head(3).iterrows():
                logger.info(f"í–‰ {i}: {dict(row)}")

            # INSERT ì‹¤í–‰ - íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ batch_size ì‚¬ìš©
            total_inserted = 0

            for i in range(0, len(df_reordered), batch_size):
                batch_df = df_reordered.iloc[i : i + batch_size]

                # None ê°’ì„ ì œëŒ€ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°ì´í„° ë³€í™˜
                batch_data = []
                for _, row in batch_df.iterrows():
                    # None ê°’ì„ PostgreSQLì˜ NULLë¡œ ë³€í™˜
                    row_data = []
                    for col_idx, value in enumerate(row):
                        # ì»¬ëŸ¼ëª…ê³¼ íƒ€ì… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        col_name = temp_columns[col_idx]
                        col_type = None
                        for col in source_schema["columns"]:
                            if col["name"] == col_name:
                                col_type = col["type"]
                                break

                        # priority ì»¬ëŸ¼ íŠ¹ë³„ ë¡œê¹… (INSERT ë‹¨ê³„)
                        if col_name == "priority":
                            logger.info(f"ğŸš¨ INSERT ë‹¨ê³„ - PRIORITY ì»¬ëŸ¼: ê°’='{value}', íƒ€ì…='{col_type}'")
                            if isinstance(value, str) and "." in value:
                                logger.info(f"ğŸš¨ PRIORITY ì»¬ëŸ¼ì— ì†Œìˆ˜ì  ê°’ ë°œê²¬: '{value}'")

                        # None ê°’ ì²˜ë¦¬
                        if value is None or value == "None" or value == "null" or value == "NULL":
                            row_data.append(None)
                        elif value == "" or value == " " or value == "nan":
                            row_data.append(None)
                        # BIGINT/INTEGER ì»¬ëŸ¼ì˜ ì†Œìˆ˜ì  ê°’ ì²˜ë¦¬
                        elif col_type in ["BIGINT", "INTEGER"] and isinstance(value, str) and "." in value:
                            try:
                                # ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                                int_value = int(float(value))
                                row_data.append(str(int_value))
                                logger.debug(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ '{int_value}'ë¡œ ë³€í™˜")
                            except (ValueError, TypeError):
                                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê°’ ì‚¬ìš©
                                row_data.append(value)
                                logger.warning(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: BIGINT/INTEGER ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ ê²€ì¦
                        elif col_type in ["BIGINT", "INTEGER"] and isinstance(value, str):
                            # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ì„ NULLë¡œ ë³€í™˜
                            if not value.strip():
                                row_data.append(None)
                            # ì†Œìˆ˜ì ì´ ìˆëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ê³  ë³€í™˜
                            elif "." in value:
                                try:
                                    int_value = int(float(value))
                                    row_data.append(str(int_value))
                                    logger.debug(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ '{int_value}'ë¡œ ë³€í™˜ (ì¶”ê°€ ê²€ì¦)")
                                except (ValueError, TypeError):
                                    logger.error(f"ì»¬ëŸ¼ {col_name}ì˜ ê°’ '{value}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. NULLë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                                    row_data.append(None)
                            else:
                                row_data.append(value)
                        else:
                            row_data.append(value)
                    batch_data.append(tuple(row_data))

                insert_query = f"""
                    INSERT INTO {temp_table} ({', '.join(temp_columns)})
                    VALUES ({', '.join(['%s'] * len(temp_columns))})
                """

                cursor.executemany(insert_query, batch_data)
                total_inserted += len(batch_data)

                if i % 10000 == 0:
                    logger.info(f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")

            logger.info(
                f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}"
            )
            return total_inserted

        except Exception as e:
            logger.error(f"CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise Exception(f"CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    def _execute_merge_in_session(
        self,
        cursor,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
    ) -> dict:
        """í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ MERGE ì‘ì—… ì‹¤í–‰"""
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            cursor.execute(all_columns_query)
            non_pk_columns = cursor.fetchall()
            non_pk_column_names = [col[0] for col in non_pk_columns]

            # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not non_pk_column_names:
                logger.warning(
                    "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                )
                # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if sync_mode == "full_sync":
                    merge_sql = f"""
                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};
                    """
                else:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # MERGE ì‹¤í–‰
                start_time = pd.Timestamp.now()
                cursor.execute(merge_sql)
                end_time = pd.Timestamp.now()

                # ê²°ê³¼ í™•ì¸
                cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                source_count = cursor.fetchone()[0]

                cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                target_count = cursor.fetchone()[0]

                merge_result = {
                    "source_count": source_count,
                    "target_count": target_count,
                    "sync_mode": sync_mode,
                    "execution_time": (end_time - start_time).total_seconds(),
                    "status": "success",
                    "message": (
                        f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                        f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                    ),
                }

                logger.info(merge_result["message"])
                return merge_result

            # MERGE ì¿¼ë¦¬ ìƒì„±
            if sync_mode == "full_sync":
                # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                if non_pk_column_names:
                    merge_sql = f"""
                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì…
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table};
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};
                    """
            else:
                # ì¦ë¶„ ë™ê¸°í™”: UPSERT (INSERT ... ON CONFLICT)
                if non_pk_column_names:
                    update_set_clause = ", ".join(
                        [f"{col} = EXCLUDED.{col}" for col in non_pk_column_names]
                    )

                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO UPDATE SET {update_set_clause};
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

            # MERGE ì‹¤í–‰
            start_time = pd.Timestamp.now()
            cursor.execute(merge_sql)
            end_time = pd.Timestamp.now()

            # ê²°ê³¼ í™•ì¸
            cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
            source_count = cursor.fetchone()[0]

            cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
            target_count = cursor.fetchone()[0]

            merge_result = {
                "source_count": source_count,
                "target_count": target_count,
                "sync_mode": sync_mode,
                "execution_time": (end_time - start_time).total_seconds(),
                "status": "success",
                "message": (
                    f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                ),
            }

            logger.info(merge_result["message"])
            return merge_result

        except Exception as e:
            logger.error(
                f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e!s}"
            )
            raise

    def _build_where_clause(
        self,
        custom_where: str | None = None,
        where_clause: str | None = None,
        sync_mode: str = "full_sync",
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        target_table: str | None = None,
    ) -> str | None:
        """
        ìµœì¢… WHERE ì¡°ê±´ êµ¬ì„±

        Args:
            custom_where: ì»¤ìŠ¤í…€ WHERE ì¡°ê±´
            where_clause: ê¸°ë³¸ WHERE ì ˆ ì¡°ê±´
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…

        Returns:
            êµ¬ì„±ëœ WHERE ì¡°ê±´ ë¬¸ìì—´
        """
        conditions = []

        # 1. custom_whereê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš©
        if custom_where:
            conditions.append(f"({custom_where})")
            logger.info(f"ì»¤ìŠ¤í…€ WHERE ì¡°ê±´ ì¶”ê°€: {custom_where}")

        # 2. ì¦ë¶„ ë™ê¸°í™” ëª¨ë“œì¼ ë•Œ incremental_field ì¡°ê±´ ì¶”ê°€
        if sync_mode == "incremental_sync" and incremental_field and target_table:
            incremental_condition = self._build_incremental_condition(
                incremental_field, incremental_field_type, target_table
            )
            if incremental_condition:
                conditions.append(f"({incremental_condition})")
                logger.info(f"ì¦ë¶„ ì¡°ê±´ ì¶”ê°€: {incremental_condition}")

        # 3. ê¸°ë³¸ where_clauseê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if where_clause:
            conditions.append(f"({where_clause})")
            logger.info(f"ê¸°ë³¸ WHERE ì¡°ê±´ ì¶”ê°€: {where_clause}")

        # 4. ëª¨ë“  ì¡°ê±´ì„ ANDë¡œ ê²°í•©
        if conditions:
            final_where = " AND ".join(conditions)
            logger.info(f"ìµœì¢… WHERE ì¡°ê±´ êµ¬ì„±: {final_where}")
            return final_where
        else:
            logger.info("WHERE ì¡°ê±´ ì—†ìŒ - ì „ì²´ ë°ì´í„° ì²˜ë¦¬")
            return None

    def _build_incremental_condition(
        self,
        incremental_field: str,
        incremental_field_type: str | None,
        target_table: str,
    ) -> str | None:
        """
        ì¦ë¶„ ë™ê¸°í™”ë¥¼ ìœ„í•œ WHERE ì¡°ê±´ êµ¬ì„±

        Args:
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…

        Returns:
            ì¦ë¶„ ì¡°ê±´ ë¬¸ìì—´
        """
        try:
            # íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¡°íšŒ
            last_update_query = f"""
                SELECT MAX({incremental_field})
                FROM {target_table}
                WHERE {incremental_field} IS NOT NULL
            """

            last_update_result = self.target_hook.get_first(last_update_query)
            last_update_time = last_update_result[0] if last_update_result and last_update_result[0] else None

            if not last_update_time:
                logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” {target_table}ì— ì¦ë¶„ í•„ë“œ {incremental_field} ë°ì´í„°ê°€ ì—†ìŒ - ì „ì²´ ë°ì´í„° ì²˜ë¦¬")
                return None

            # í•„ë“œ íƒ€ì…ì— ë”°ë¥¸ ì¡°ê±´ êµ¬ì„±
            if incremental_field_type == "yyyymmdd":
                # YYYYMMDD í˜•ì‹ (ì˜ˆ: 20250812) - ì´ìƒ(>=) ì¡°ê±´ ì‚¬ìš©
                # ë‚ ì§œ í˜•ì‹ì—ì„œëŠ” ì •í™•í•œ ì‹œì ì„ ì¡ê¸° ì–´ë ¤ìš°ë¯€ë¡œ >= ì‚¬ìš©
                condition = f"{incremental_field} >= '{last_update_time}'"
                logger.info(f"YYYYMMDD í˜•ì‹ ê°ì§€ - ì´ìƒ(>=) ì¡°ê±´ ì‚¬ìš©: {condition}")
            elif incremental_field_type == "timestamp":
                # TIMESTAMP í˜•ì‹ - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš© (ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì •ë°€ë„)
                condition = f"{incremental_field} > '{last_update_time}'"
            elif incremental_field_type == "date":
                # DATE í˜•ì‹ - ì´ìƒ(>=) ì¡°ê±´ ì‚¬ìš© (ë‚ ì§œ ë‹¨ìœ„)
                condition = f"{incremental_field} >= '{last_update_time}'"
            elif incremental_field_type == "datetime":
                # DATETIME í˜•ì‹ - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš© (ì´ˆ ë‹¨ìœ„ ì •ë°€ë„)
                condition = f"{incremental_field} > '{last_update_time}'"
            elif incremental_field_type == "integer":
                # ì •ìˆ˜í˜• (ì˜ˆ: Unix timestamp) - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš©
                condition = f"{incremental_field} > {last_update_time}"
            else:
                # ê¸°ë³¸ê°’: ë¬¸ìì—´ ë¹„êµ - ì´ˆê³¼(>) ì¡°ê±´ ì‚¬ìš©
                condition = f"{incremental_field} > '{last_update_time}'"

            logger.info(f"ì¦ë¶„ ì¡°ê±´ êµ¬ì„±: {condition} (ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update_time})")
            return condition

        except Exception as e:
            logger.warning(f"ì¦ë¶„ ì¡°ê±´ êµ¬ì„± ì‹¤íŒ¨: {e!s} - ì¦ë¶„ ì¡°ê±´ ì—†ì´ ì§„í–‰")
            return None

    def copy_table_data(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        batch_size: int = 10000,
        custom_where: str | None = None,
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
    ) -> dict[str, Any]:
        """
        í…Œì´ë¸” ë°ì´í„° ë³µì‚¬ (ê¸°ë³¸ ë©”ì„œë“œ)
        """
        return self._copy_table_data_internal(
            source_table=source_table,
            target_table=target_table,
            primary_keys=primary_keys,
            sync_mode=sync_mode,
            batch_size=batch_size,
            custom_where=custom_where,
            incremental_field=incremental_field,
            incremental_field_type=incremental_field_type,
        )

    def copy_table_data_with_custom_sql(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        batch_size: int = 10000,
        custom_where: str | None = None,
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        count_sql: str | None = None,
        select_sql: str | None = None,
    ) -> dict[str, Any]:
        """
        ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ë°ì´í„° ë³µì‚¬

        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            custom_where: ì»¤ìŠ¤í…€ WHERE ì¡°ê±´
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            count_sql: ì‚¬ìš©ì ì •ì˜ COUNT SQL
            select_sql: ì‚¬ìš©ì ì •ì˜ SELECT SQL

        Returns:
            ë³µì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì‚¬ìš©ì ì •ì˜ SQLì´ ì œê³µëœ ê²½ìš° ì´ë¥¼ ì‚¬ìš©
        if count_sql and select_sql:
            return self._copy_table_data_with_custom_sql_internal(
                source_table=source_table,
                target_table=target_table,
                primary_keys=primary_keys,
                sync_mode=sync_mode,
                batch_size=batch_size,
                custom_where=custom_where,
                incremental_field=incremental_field,
                incremental_field_type=incremental_field_type,
                count_sql=count_sql,
                select_sql=select_sql,
            )
        else:
            # ê¸°ë³¸ ë©”ì„œë“œ ì‚¬ìš©
            return self._copy_table_data_internal(
                source_table=source_table,
                target_table=target_table,
                primary_keys=primary_keys,
                sync_mode=sync_mode,
                batch_size=batch_size,
                custom_where=custom_where,
                incremental_field=incremental_field,
                incremental_field_type=incremental_field_type,
            )

    def _copy_table_data_internal(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        batch_size: int,
        custom_where: str | None,
        incremental_field: str | None,
        incremental_field_type: str | None,
    ) -> dict[str, Any]:
        """
        í…Œì´ë¸” ë°ì´í„° ë³µì‚¬ (ë‚´ë¶€ ë©”ì„œë“œ)
        """
        csv_path = None
        temp_table = None

        try:
            start_time = pd.Timestamp.now()

            # WHERE ì¡°ê±´ êµ¬ì„± (custom_where ìš°ì„ , where_clauseëŠ” ê¸°ë³¸ê°’)
            final_where_clause = self._build_where_clause(
                custom_where=custom_where,
                where_clause=None,
                sync_mode=sync_mode,
                incremental_field=incremental_field,
                incremental_field_type=incremental_field_type,
                target_table=target_table
            )

            logger.info(f"ìµœì¢… WHERE ì¡°ê±´: {final_where_clause}")

            # 1. ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹œì‘: {source_table}")
            source_schema = self.db_ops.get_table_schema(source_table)

            if not source_schema or not source_schema.get("columns"):
                raise Exception(
                    f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            # 2. CSVë¡œ ë‚´ë³´ë‚´ê¸° (ì†ŒìŠ¤ DBì—ì„œ - ë‹¤ë¥¸ ì„¸ì…˜)
            csv_path = os.path.join(
                self.temp_dir, f"temp_{os.path.basename(source_table)}.csv"
            )
            logger.info(f"CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘: {source_table} -> {csv_path}")
            exported_rows = self.export_to_csv(
                source_table, csv_path, final_where_clause, batch_size, incremental_field
            )

            if exported_rows == 0:
                return {
                    "status": "warning",
                    "message": f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì— ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. WHERE: {final_where_clause}",
                    "execution_time": 0,
                }

            # 3. í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±, ë°ì´í„° ì‚½ì…, MERGE ì‘ì—… ìˆ˜í–‰ (ì„¸ì…˜ ê²©ë¦¬ ë¬¸ì œ í•´ê²°)
            logger.info(
                f"íƒ€ê²Ÿ DBì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±, ë°ì´í„° ì‚½ì…, MERGE ì‘ì—… ì‹œì‘: {target_table}"
            )

            # íƒ€ê²Ÿ DBì—ì„œ í•˜ë‚˜ì˜ ì—°ê²°ë¡œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰
            with self.target_hook.get_conn() as target_conn:
                with target_conn.cursor() as cursor:
                    # 3-1. ì„ì‹œ í…Œì´ë¸” ìƒì„± (ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜)
                    temp_table = self._create_temp_table_in_session(
                        cursor, target_table, source_schema
                    )
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # 3-2. CSV ë°ì´í„° ì‚½ì…
                    imported_rows = self._import_csv_in_session(
                        cursor, temp_table, csv_path, source_schema, batch_size
                    )
                    logger.info(
                        f"CSV ë°ì´í„° ì‚½ì… ì™„ë£Œ: {temp_table}, í–‰ ìˆ˜: {imported_rows}"
                    )

                    # 3-3. MERGE ì‘ì—… ì‹¤í–‰ (ê°™ì€ ì—°ê²°ì—ì„œ)
                    merge_result = self._execute_merge_in_session(
                        cursor, temp_table, target_table, primary_keys, sync_mode
                    )
                    logger.info(f"MERGE ì‘ì—… ì™„ë£Œ: {temp_table} -> {target_table}")

                # ëª¨ë“  ì‘ì—…ì´ ì„±ê³µí•˜ë©´ ì»¤ë°‹
                target_conn.commit()
                logger.info("ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì»¤ë°‹ë˜ì—ˆìŠµë‹ˆë‹¤.")

            end_time = pd.Timestamp.now()
            total_time = (end_time - start_time).total_seconds()

            # 5. ê²°ê³¼ ì •ë¦¬
            final_result = {
                "status": "success",
                "source_table": source_table,
                "target_table": target_table,
                "exported_rows": exported_rows,
                "imported_rows": imported_rows,
                "merge_result": merge_result,
                "total_execution_time": total_time,
                "where_clause_used": final_where_clause,
                "sync_mode": sync_mode,
                "message": (
                    f"í…Œì´ë¸” ë³µì‚¬ ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"WHERE: {final_where_clause}, "
                    f"ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ"
                ),
            }

            logger.info(final_result["message"])
            return final_result

        except Exception as e:
            error_result = {
                "status": "error",
                "source_table": source_table,
                "target_table": target_table,
                "error": str(e),
                "message": (
                    f"í…Œì´ë¸” ë³µì‚¬ ì‹¤íŒ¨: {source_table} -> {target_table}, "
                    f"ì˜¤ë¥˜: {e!s}"
                ),
            }

            logger.error(error_result["message"])
            return error_result

        finally:
            # 6. ì •ë¦¬ ì‘ì—…
            if csv_path:
                self.cleanup_temp_files(csv_path)

            if temp_table:
                try:
                    # ì„ì‹œ í…Œì´ë¸”ì€ ìë™ìœ¼ë¡œ ì‚­ì œë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ
                    self.target_hook.run(f"DROP TABLE IF EXISTS {temp_table}")
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ì •ë¦¬ ì™„ë£Œ: {temp_table}")
                except Exception as e:
                    logger.warning(f"ì„ì‹œ í…Œì´ë¸” ì •ë¦¬ ì‹¤íŒ¨: {temp_table}, ì˜¤ë¥˜: {e!s}")

    def _copy_table_data_with_custom_sql_internal(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        batch_size: int,
        custom_where: str | None,
        incremental_field: str | None,
        incremental_field_type: str | None,
        count_sql: str,
        select_sql: str,
    ) -> dict[str, Any]:
        """
        ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ë°ì´í„° ë³µì‚¬ (ë‚´ë¶€ ë©”ì„œë“œ)
        """
        csv_path = None
        start_time = time.time()

        try:
            # 1ë‹¨ê³„: ë°ì´í„° ë‚´ë³´ë‚´ê¸° (ì‚¬ìš©ì ì •ì˜ SQL ì‚¬ìš©)
            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹œì‘: {source_table}")

            # ì „ì²´ í–‰ ìˆ˜ ì¡°íšŒ (ì‚¬ìš©ì ì •ì˜ COUNT SQL ì‚¬ìš©)
            if count_sql:
                total_count = self.source_hook.get_first(count_sql)[0]
                logger.info(f"ì‚¬ìš©ì ì •ì˜ COUNT SQLë¡œ ì¡°íšŒëœ í–‰ ìˆ˜: {total_count}")
            else:
                total_count = self.db_ops.get_table_row_count(source_table, where_clause=custom_where)

            if total_count == 0:
                logger.warning(f"í…Œì´ë¸” {source_table}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": 0,
                    "message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # CSV íŒŒì¼ ê²½ë¡œ ìƒì„±
            csv_path = f"/tmp/temp_{source_table.replace('.', '_')}.csv"

            # ì‚¬ìš©ì ì •ì˜ SELECT SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            if select_sql:
                logger.info(f"ì‚¬ìš©ì ì •ì˜ SELECT SQL ì‚¬ìš©: {select_sql}")
                exported_rows = self._export_with_custom_sql(select_sql, csv_path, batch_size)
            else:
                # ê¸°ë³¸ ë‚´ë³´ë‚´ê¸° ì‚¬ìš©
                exported_rows = self.export_to_csv(
                    source_table, csv_path, custom_where, batch_size, incremental_field
                )

            if exported_rows == 0:
                logger.warning(f"ë‚´ë³´ë‚¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": time.time() - start_time,
                    "message": "ë‚´ë³´ë‚¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # 2ë‹¨ê³„: ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            logger.info(f"íƒ€ê²Ÿ DBì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„±, ë°ì´í„° ì‚½ì…, MERGE ì‘ì—… ì‹œì‘: {target_table}")
            imported_rows = self._import_and_merge_data(
                csv_path, target_table, primary_keys, sync_mode
            )

            # 3ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬
            total_time = time.time() - start_time

            result = {
                "status": "success",
                "exported_rows": exported_rows,
                "imported_rows": imported_rows,
                "total_execution_time": total_time,
                "message": f"ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {exported_rows}í–‰ ë‚´ë³´ë‚´ê¸°, {imported_rows}í–‰ ê°€ì ¸ì˜¤ê¸°"
            }

            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {result}")
            return result

        except Exception as e:
            total_time = time.time() - start_time
            error_msg = f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)

            result = {
                "status": "error",
                "exported_rows": 0,
                "imported_rows": 0,
                "total_execution_time": total_time,
                "error": error_msg
            }

            return result

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if csv_path and os.path.exists(csv_path):
                try:
                    os.remove(csv_path)
                    logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {csv_path}")
                except Exception as e:
                    logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def _export_with_custom_sql(self, select_sql: str, csv_path: str, batch_size: int, source_schema: dict[str, Any] = None) -> int:
        """
        ì‚¬ìš©ì ì •ì˜ SELECT SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            select_sql: SELECT SQL ì¿¼ë¦¬
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì»¬ëŸ¼ëª… ì¶”ì¶œìš©)
        """
        try:
            # ì‚¬ìš©ì ì •ì˜ SQLë¡œ ë°ì´í„° ì¶”ì¶œ
            all_data = []
            offset = 0

            while True:
                # LIMITì™€ OFFSETì„ ì¶”ê°€í•œ SQL ìƒì„±
                paginated_sql = f"{select_sql} LIMIT {batch_size} OFFSET {offset}"
                logger.info(f"í˜ì´ì§€ë„¤ì´ì…˜ SQL ì‹¤í–‰: OFFSET {offset}")

                batch_data = self.source_hook.get_records(paginated_sql)

                if not batch_data:
                    break

                all_data.extend(batch_data)
                offset += batch_size

                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {len(all_data)}í–‰ ìˆ˜ì§‘")

                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if len(batch_data) < batch_size:
                    break

            if not all_data:
                logger.warning("ì‚¬ìš©ì ì •ì˜ SQLë¡œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # DataFrameìœ¼ë¡œ ë³€í™˜ (ì»¬ëŸ¼ëª… ëª…ì‹œ)
            if source_schema and source_schema.get("columns"):
                # ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
                column_names = [col["name"] for col in source_schema["columns"]]
            else:
                # ê¸°ë³¸ ì»¬ëŸ¼ëª… ì‚¬ìš©
                column_names = [f"col_{i}" for i in range(len(all_data[0]) if all_data else 0)]

            df = pd.DataFrame(all_data, columns=column_names)

            # CSVë¡œ ì €ì¥ (ì»¬ëŸ¼ëª… í¬í•¨)
            df.to_csv(csv_path, index=False, header=True)
            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLë¡œ CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {csv_path}, í–‰ ìˆ˜: {len(all_data)}")

            return len(all_data)

        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì •ì˜ SQLë¡œ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _import_and_merge_data(self, csv_path: str, target_table: str, primary_keys: list[str], sync_mode: str) -> int:
        """
        ì„ì‹œ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  MERGE ì‘ì—… ìˆ˜í–‰ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            # 1ë‹¨ê³„: íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            target_schema = self.db_ops.get_table_schema(target_table)

            if not target_schema or not target_schema.get("columns"):
                raise Exception(f"íƒ€ê²Ÿ í…Œì´ë¸” {target_table}ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2ë‹¨ê³„: ì„ì‹œ í…Œì´ë¸” ìƒì„± (ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            temp_table = self.create_temp_table(target_table, target_schema)

            # 3ë‹¨ê³„: CSVì—ì„œ ì„ì‹œ í…Œì´ë¸”ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            imported_rows = self.import_from_csv(csv_path, temp_table, target_schema)

            if imported_rows == 0:
                logger.warning(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return 0

            # 4ë‹¨ê³„: MERGE ì‘ì—… ìˆ˜í–‰ (temp_tableì„ ì†ŒìŠ¤ë¡œ ì‚¬ìš©)
            merge_result = self.execute_merge_operation(
                temp_table, target_table, primary_keys, sync_mode
            )

            if merge_result["status"] != "success":
                logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {merge_result['message']}")
                return 0

            return imported_rows

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE ì‘ì—… ì‹¤íŒ¨: {e}")
            return 0

    def create_target_table_and_verify_schema(
        self, target_table: str, source_schema: dict[str, Any]
    ) -> dict[str, Any]:
        """
        íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„±ê³¼ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ í•œ ë²ˆì— ì²˜ë¦¬

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´

        Returns:
            ê²€ì¦ëœ íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
        """
        try:
            logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘: {target_table}")

            # DatabaseOperationsì˜ ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
            verified_schema = self.db_ops.create_table_and_verify_schema(
                target_table, source_schema
            )

            logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ: {target_table}")
            return verified_schema

        except Exception as e:
            logger.error(f"íƒ€ê²Ÿ í…Œì´ë¸” ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {target_table}, ì˜¤ë¥˜: {e}")
            raise

    def copy_data_with_custom_sql(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "incremental_sync",
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        custom_where: str | None = None,
        batch_size: int = 10000,
        verified_target_schema: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """
        ê²€ì¦ëœ íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ SQLë¡œ ë°ì´í„° ë³µì‚¬

        Args:
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            primary_keys: ê¸°ë³¸í‚¤ ëª©ë¡
            sync_mode: ë™ê¸°í™” ëª¨ë“œ
            incremental_field: ì¦ë¶„ í•„ë“œëª…
            incremental_field_type: ì¦ë¶„ í•„ë“œ íƒ€ì…
            custom_where: ì‚¬ìš©ì ì •ì˜ WHERE ì¡°ê±´
            batch_size: ë°°ì¹˜ í¬ê¸°
            verified_target_schema: ê²€ì¦ëœ íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ

        Returns:
            ë³µì‚¬ ê²°ê³¼
        """
        try:
            start_time = time.time()

            # 1ë‹¨ê³„: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            source_schema = self.db_ops.get_table_schema(source_table)

            # 2ë‹¨ê³„: ì‚¬ìš©ì ì •ì˜ SQL ìƒì„±
            count_sql, select_sql = self._build_custom_sql_queries(
                source_table, source_schema, incremental_field,
                incremental_field_type, custom_where
            )

            # 3ë‹¨ê³„: ë°ì´í„° ê°œìˆ˜ í™•ì¸
            row_count = self._get_source_row_count(count_sql)

            if row_count == 0:
                logger.info(f"ì†ŒìŠ¤ í…Œì´ë¸” {source_table}ì— ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "success",
                    "exported_rows": 0,
                    "imported_rows": 0,
                    "total_execution_time": time.time() - start_time,
                    "message": "ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŒ"
                }

            # 4ë‹¨ê³„: CSV íŒŒì¼ë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
            csv_path = f"/tmp/temp_{source_table.replace('.', '_')}.csv"
            exported_rows = self._export_with_custom_sql(select_sql, csv_path, batch_size, source_schema)

            if exported_rows == 0:
                raise Exception("ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨")

            # 5ë‹¨ê³„: ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE (ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            if verified_target_schema:
                # ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
                imported_rows = self._import_and_merge_with_verified_schema(
                    csv_path, target_table, primary_keys, sync_mode, verified_target_schema
                )
            else:
                # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                imported_rows = self._import_and_merge_data(
                    csv_path, target_table, primary_keys, sync_mode
                )

            total_time = time.time() - start_time

            result = {
                "status": "success",
                "exported_rows": exported_rows,
                "imported_rows": imported_rows,
                "total_execution_time": total_time,
                "message": f"ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {exported_rows}í–‰ ë‚´ë³´ë‚´ê¸°, {imported_rows}í–‰ ê°€ì ¸ì˜¤ê¸°"
            }

            logger.info(f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: {result}")
            return result

        except Exception as e:
            total_time = time.time() - start_time
            error_msg = f"ì‚¬ìš©ì ì •ì˜ SQLì„ ì‚¬ìš©í•œ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)

            result = {
                "status": "error",
                "exported_rows": 0,
                "imported_rows": 0,
                "total_execution_time": total_time,
                "error": error_msg
            }

            return result

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if 'csv_path' in locals() and csv_path and os.path.exists(csv_path):
                try:
                    os.remove(csv_path)
                    logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {csv_path}")
                except Exception as e:
                    logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def _build_custom_sql_queries(
        self,
        source_table: str,
        source_schema: dict[str, Any],
        incremental_field: str | None = None,
        incremental_field_type: str | None = None,
        custom_where: str | None = None
    ) -> tuple[str, str]:
        """ì‚¬ìš©ì ì •ì˜ SQL ì¿¼ë¦¬ ìƒì„±"""
        try:
            # ì›ë³¸ ì»¬ëŸ¼ëª… ì‚¬ìš© (ë³€í™˜ ì—†ì´)
            column_names = [col["name"] for col in source_schema["columns"]]
            transformed_columns = [f'"{col_name}"' for col_name in column_names]

            # ê¸°ë³¸ WHERE ì¡°ê±´
            where_conditions = []
            if custom_where:
                where_conditions.append(custom_where)
            # custom_whereê°€ ì´ë¯¸ ì¦ë¶„ ì¡°ê±´ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            elif incremental_field and incremental_field_type:
                # custom_whereê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¦ë¶„ ë™ê¸°í™” ì¡°ê±´ ì¶”ê°€
                if incremental_field_type == "yyyymmdd":
                    where_conditions.append(f"{incremental_field} >= '20250812'")
                # ë‹¤ë¥¸ ì¦ë¶„ í•„ë“œ íƒ€ì…ë“¤ë„ ì¶”ê°€ ê°€ëŠ¥

            where_clause = " AND ".join(where_conditions) if where_conditions else ""
            where_sql = f" WHERE {where_clause}" if where_clause else ""

            # COUNT ì¿¼ë¦¬
            count_sql = f'SELECT COUNT(*) FROM {source_table}{where_sql}'

            # SELECT ì¿¼ë¦¬
            select_sql = f'SELECT {", ".join(transformed_columns)} FROM {source_table}{where_sql} ORDER BY {incremental_field or "1"}'

            return count_sql, select_sql

        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì •ì˜ SQL ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _apply_column_transformation(self, column_name: str, data_type: str) -> str:
        """ì»¬ëŸ¼ë³„ ë°ì´í„° ë³€í™˜ ì ìš©"""
        # ê°„ë‹¨í•œ ë³€í™˜ ë¡œì§ (í•„ìš”ì— ë”°ë¼ í™•ì¥ ê°€ëŠ¥)
        if data_type.lower() in ["bigint", "integer", "smallint"]:
            return f"""
                CASE
                    WHEN "{column_name}" IS NULL THEN NULL
                    WHEN "{column_name}"::TEXT = '' THEN NULL
                    WHEN "{column_name}"::TEXT ~ '^[0-9]+\.[0]+$' THEN CAST("{column_name}" AS BIGINT)::TEXT
                    ELSE "{column_name}"::TEXT
                END AS "{column_name}"
            """
        elif data_type.lower() in ["text", "character varying", "varchar"]:
            return f"""
                CASE
                    WHEN "{column_name}" IS NULL THEN NULL
                    WHEN "{column_name}" = '' THEN NULL
                    WHEN "{column_name}" ~ '^[0-9]+$' THEN CAST("{column_name}" AS BIGINT)::TEXT
                    WHEN "{column_name}" ~ '^[0-9]{{8}}$' AND "{column_name}" ~ '^(19|20)[0-9]{{6}}$'
                        THEN TO_CHAR(TO_DATE("{column_name}", 'YYYYMMDD'), 'YYYY-MM-DD')
                    ELSE "{column_name}"
                END AS "{column_name}"
            """
        else:
            return f'"{column_name}"'

    def _get_source_row_count(self, count_sql: str) -> int:
        """ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ì¡°ê±´ì— ë§ëŠ” í–‰ ìˆ˜ ì¡°íšŒ"""
        try:
            result = self.source_hook.get_first(count_sql)
            return result[0] if result else 0
        except Exception as e:
            logger.error(f"ì†ŒìŠ¤ í…Œì´ë¸” í–‰ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0

    def _import_and_merge_with_verified_schema(
        self,
        csv_path: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str,
        verified_schema: dict[str, Any]
    ) -> int:
        """ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE"""
        try:
            # í•˜ë‚˜ì˜ ì—°ê²°ì—ì„œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # 1ë‹¨ê³„: ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    imported_rows, temp_table = self._create_temp_table_and_import_csv_in_connection(
                        cursor, target_table, verified_schema, csv_path
                    )

                    if imported_rows == 0:
                        logger.warning(f"ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        return 0

                    # 2ë‹¨ê³„: ê°™ì€ ì—°ê²°ì—ì„œ MERGE ì‘ì—… ìˆ˜í–‰
                    merge_result = self._execute_merge_operation_with_cursor(
                        cursor, temp_table, target_table, primary_keys, sync_mode, verified_schema
                    )

                    if merge_result["status"] != "success":
                        logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {merge_result['message']}")
                        return 0

                    # ì»¤ë°‹
                    conn.commit()
                    return imported_rows

        except Exception as e:
            logger.error(f"ê²€ì¦ëœ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° MERGE ì‹¤íŒ¨: {e}")
            return 0

    def _create_temp_table_and_import_csv(
        self,
        target_table: str,
        source_schema: dict[str, Any],
        csv_path: str,
        batch_size: int = 1000
    ) -> tuple[int, str]:
        """
        ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸°ë¥¼ í•˜ë‚˜ì˜ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬

        Args:
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª…
            source_schema: ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ê°€ì ¸ì˜¨ í–‰ ìˆ˜
        """
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_path, encoding="utf-8")

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„±
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]
                col_type = self._convert_to_postgres_type(col_type, col.get('max_length'))
                nullable = "" if col["nullable"] else " NOT NULL"
                column_definitions.append(f"{col_name} {col_type}{nullable}")

            # ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL
            create_temp_table_sql = f"""
                CREATE TEMP TABLE {temp_table} (
                    {', '.join(column_definitions)}
                ) ON COMMIT DROP
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_temp_table_sql}")

            # ê°™ì€ ì—°ê²°ì—ì„œ ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    # 1. ì„ì‹œ í…Œì´ë¸” ìƒì„±
                    cursor.execute(create_temp_table_sql)
                    logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

                    # 2. ì„ì‹œ í…Œì´ë¸”ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    cursor.execute(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{temp_table}')")
                    exists = cursor.fetchone()[0]
                    if not exists:
                        raise Exception(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                    # 3. CSV ë°ì´í„°ë¥¼ ì„ì‹œ í…Œì´ë¸”ë¡œ ê°€ì ¸ì˜¤ê¸°
                    # ì»¬ëŸ¼ëª… ë§¤í•‘
                    if source_schema and source_schema.get("columns"):
                        schema_columns = [col["name"] for col in source_schema["columns"]]
                        available_columns = [col for col in schema_columns if col in df.columns]

                        if not available_columns:
                            logger.warning("CSVì™€ ìŠ¤í‚¤ë§ˆ ê°„ì— ê³µí†µ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            return 0

                        df_reordered = df[available_columns]
                        temp_columns = available_columns
                    else:
                        temp_columns = list(df.columns)
                        df_reordered = df

                    # ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ì •ë¦¬
                    for col in df_reordered.columns:
                        # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                        df_reordered[col] = df_reordered[col].where(pd.notna(df_reordered[col]), None)

                        # ìŠ¤í‚¤ë§ˆì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ì˜ íƒ€ì… ì •ë³´ ì°¾ê¸°
                        col_schema = next((c for c in source_schema["columns"] if c["name"] == col), None)
                        col_type = col_schema["type"].lower() if col_schema else "text"

                        # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                        if df_reordered[col].dtype in ["int64", "float64"]:
                            df_reordered[col] = df_reordered[col].astype(str)

                        # ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œ 'nan', 'NaN' ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                        if df_reordered[col].dtype == "object":
                            df_reordered[col] = df_reordered[col].replace(['nan', 'NaN', 'None', ''], None)

                        # BIGINT ì»¬ëŸ¼ì˜ ê²½ìš° ë¶€ë™ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ì˜ˆ: "1.0" -> "1")
                        if col_type in ["bigint", "integer", "int"] and df_reordered[col].dtype == "object":
                            df_reordered[col] = df_reordered[col].apply(
                                lambda x: str(int(float(x))) if x and isinstance(x, str) and x.replace('.', '').replace('-', '').isdigit() and float(x).is_integer() else x
                            )

                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ INSERT ì‹¤í–‰
                    total_inserted = 0
                    for i in range(0, len(df_reordered), batch_size):
                        batch_df = df_reordered.iloc[i : i + batch_size]
                        batch_data = [tuple(row) for row in batch_df.values]

                        insert_query = f"""
                            INSERT INTO {temp_table} ({', '.join(temp_columns)})
                            VALUES ({', '.join(['%s'] * len(temp_columns))})
                        """

                        cursor.executemany(insert_query, batch_data)
                        total_inserted += len(batch_data)

                        if i % 10000 == 0:
                            logger.info(f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")

                    # ì»¤ë°‹
                    conn.commit()

                    logger.info(f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}")
                    return total_inserted, temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _create_temp_table_and_import_csv_in_connection(
        self,
        cursor,
        target_table: str,
        source_schema: dict[str, Any],
        csv_path: str,
        batch_size: int = 1000
    ) -> tuple[int, str]:
        """
        ê¸°ì¡´ ì»¤ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ì‹œ í…Œì´ë¸” ìƒì„±ê³¼ CSV ê°€ì ¸ì˜¤ê¸°ë¥¼ ìˆ˜í–‰
        """
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_path, encoding="utf-8")

            if df.empty:
                logger.warning(f"CSV íŒŒì¼ {csv_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0, ""

            # ì„ì‹œ í…Œì´ë¸”ëª… ìƒì„±
            temp_table = (
                f"temp_{target_table.replace('.', '_')}_"
                f"{int(pd.Timestamp.now().timestamp())}"
            )

            # ì»¬ëŸ¼ ì •ì˜ ìƒì„±
            column_definitions = []
            for col in source_schema["columns"]:
                col_name = col["name"]
                col_type = col["type"]
                col_type = self._convert_to_postgres_type(col_type, col.get('max_length'))
                nullable = "" if col["nullable"] else " NOT NULL"
                column_definitions.append(f"{col_name} {col_type}{nullable}")

            # ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL
            create_temp_table_sql = f"""
                CREATE TEMP TABLE {temp_table} (
                    {', '.join(column_definitions)}
                ) ON COMMIT DROP
            """

            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± SQL: {create_temp_table_sql}")

            # 1. ì„ì‹œ í…Œì´ë¸” ìƒì„±
            cursor.execute(create_temp_table_sql)
            logger.info(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")

            # 2. ì„ì‹œ í…Œì´ë¸”ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            cursor.execute(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{temp_table}')")
            exists = cursor.fetchone()[0]
            if not exists:
                raise Exception(f"ì„ì‹œ í…Œì´ë¸” {temp_table}ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # 3. CSV ë°ì´í„°ë¥¼ ì„ì‹œ í…Œì´ë¸”ë¡œ ê°€ì ¸ì˜¤ê¸°
            # ì»¬ëŸ¼ëª… ë§¤í•‘
            if source_schema and source_schema.get("columns"):
                schema_columns = [col["name"] for col in source_schema["columns"]]
                available_columns = [col for col in schema_columns if col in df.columns]

                if not available_columns:
                    logger.warning("CSVì™€ ìŠ¤í‚¤ë§ˆ ê°„ì— ê³µí†µ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return 0, ""

                df_reordered = df[available_columns]
                temp_columns = available_columns
            else:
                temp_columns = list(df.columns)
                df_reordered = df

            # ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ì •ë¦¬
            for col in df_reordered.columns:
                # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                df_reordered[col] = df_reordered[col].where(pd.notna(df_reordered[col]), None)

                # ìŠ¤í‚¤ë§ˆì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ì˜ íƒ€ì… ì •ë³´ ì°¾ê¸°
                col_schema = next((c for c in source_schema["columns"] if c["name"] == col), None)
                col_type = col_schema["type"].lower() if col_schema else "text"

                # ìˆ«ì ì»¬ëŸ¼ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                if df_reordered[col].dtype in ["int64", "float64"]:
                    df_reordered[col] = df_reordered[col].astype(str)

                # ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œ 'nan', 'NaN' ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                if df_reordered[col].dtype == "object":
                    df_reordered[col] = df_reordered[col].replace(['nan', 'NaN', 'None', ''], None)

                # BIGINT ì»¬ëŸ¼ì˜ ê²½ìš° ë¶€ë™ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ì˜ˆ: "1.0" -> "1")
                if col_type in ["bigint", "integer", "int"] and df_reordered[col].dtype == "object":
                    df_reordered[col] = df_reordered[col].apply(
                        lambda x: str(int(float(x))) if x and isinstance(x, str) and x.replace('.', '').replace('-', '').isdigit() and float(x).is_integer() else x
                    )

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ INSERT ì‹¤í–‰
            total_inserted = 0
            for i in range(0, len(df_reordered), batch_size):
                batch_df = df_reordered.iloc[i : i + batch_size]
                batch_data = [tuple(row) for row in batch_df.values]

                insert_query = f"""
                    INSERT INTO {temp_table} ({', '.join(temp_columns)})
                    VALUES ({', '.join(['%s'] * len(temp_columns))})
                """

                cursor.executemany(insert_query, batch_data)
                total_inserted += len(batch_data)

                if i % 10000 == 0:
                    logger.info(f"INSERT ì§„í–‰ë¥ : {total_inserted}/{len(df_reordered)}")

            logger.info(f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {csv_path} -> {temp_table}, í–‰ ìˆ˜: {total_inserted}")
            return total_inserted, temp_table

        except Exception as e:
            logger.error(f"ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° CSV ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise

    def _execute_merge_operation_same_connection(
        self,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "full_sync",
        verified_schema: dict[str, Any] = None
    ) -> dict[str, Any]:
        """
        ê°™ì€ ì—°ê²°ì—ì„œ MERGE ì‘ì—… ì‹¤í–‰ (ì„ì‹œ í…Œì´ë¸”ê³¼ í•¨ê»˜ ì‚¬ìš©)
        """
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            # ê°™ì€ ì—°ê²°ì—ì„œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            with self.target_hook.get_conn() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(all_columns_query)
                    non_pk_columns = cursor.fetchall()
                    non_pk_column_names = [col[0] for col in non_pk_columns]

                    # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                    if not non_pk_column_names:
                        logger.warning(
                            "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                        )
                        # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                        if sync_mode == "full_sync":
                            merge_sql = f"""
                                BEGIN;

                                -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                                DELETE FROM {target_table}
                                WHERE ({pk_columns}) IN (
                                    SELECT {pk_columns} FROM {source_table}
                                );

                                -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table};

                                COMMIT;
                            """
                        else:
                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO NOTHING;
                            """

                        # MERGE ì‹¤í–‰
                        start_time = pd.Timestamp.now()
                        cursor.execute(merge_sql)
                        conn.commit()
                        end_time = pd.Timestamp.now()

                        # ê²°ê³¼ í™•ì¸
                        cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                        source_count = cursor.fetchone()[0]
                        cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                        target_count = cursor.fetchone()[0]

                        merge_result = {
                            "source_count": source_count,
                            "target_count": target_count,
                            "sync_mode": sync_mode,
                            "execution_time": (end_time - start_time).total_seconds(),
                            "status": "success",
                            "message": (
                                f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                                f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                            ),
                        }

                        logger.info(merge_result["message"])
                        return merge_result

                    # MERGE ì¿¼ë¦¬ ìƒì„±
                    if sync_mode == "full_sync":
                        # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                        if non_pk_column_names:
                            merge_sql = f"""
                                BEGIN;

                                -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                                DELETE FROM {target_table}
                                WHERE ({pk_columns}) IN (
                                    SELECT {pk_columns} FROM {source_table}
                                );

                                -- ìƒˆ ë°ì´í„° ì‚½ì…
                                INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                                SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                                FROM {source_table};

                                COMMIT;
                            """
                        else:
                            # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                            merge_sql = f"""
                                BEGIN;

                                -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                                DELETE FROM {target_table}
                                WHERE ({pk_columns}) IN (
                                    SELECT {pk_columns} FROM {source_table}
                                );

                                -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table};

                                COMMIT;
                            """
                    else:
                        # ì¦ë¶„ ë™ê¸°í™”: UPSERT (INSERT ... ON CONFLICT)
                        if non_pk_column_names:
                            update_set_clause = ", ".join(
                                [f"{col} = EXCLUDED.{col}" for col in non_pk_column_names]
                            )

                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                                SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO UPDATE SET {update_set_clause};
                            """
                        else:
                            # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                            merge_sql = f"""
                                INSERT INTO {target_table} ({pk_columns})
                                SELECT {pk_columns}
                                FROM {source_table}
                                ON CONFLICT ({pk_columns})
                                DO NOTHING;
                            """

                    # MERGE ì‹¤í–‰
                    start_time = pd.Timestamp.now()
                    cursor.execute(merge_sql)
                    conn.commit()
                    end_time = pd.Timestamp.now()

                    # ê²°ê³¼ í™•ì¸
                    cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                    source_count = cursor.fetchone()[0]
                    cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                    target_count = cursor.fetchone()[0]

                    merge_result = {
                        "source_count": source_count,
                        "target_count": target_count,
                        "sync_mode": sync_mode,
                        "execution_time": (end_time - start_time).total_seconds(),
                        "status": "success",
                        "message": (
                            f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                            f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                        ),
                    }

                    logger.info(merge_result["message"])
                    return merge_result

        except Exception as e:
            logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "message": str(e),
                "source_count": 0,
                "target_count": 0,
                "sync_mode": sync_mode,
                "execution_time": 0
            }

    def _execute_merge_operation_with_cursor(
        self,
        cursor,
        source_table: str,
        target_table: str,
        primary_keys: list[str],
        sync_mode: str = "full_sync",
        verified_schema: dict[str, Any] = None
    ) -> dict[str, Any]:
        """
        ê¸°ì¡´ ì»¤ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ MERGE ì‘ì—… ì‹¤í–‰
        """
        try:
            # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
            if "." in target_table:
                schema, table = target_table.split(".", 1)
            else:
                schema = "public"
                table = target_table

            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
            pk_columns = ", ".join(primary_keys)

            # ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
            constraint_name = f"pk_{table}_{'_'.join(primary_keys)}"
            check_constraint_sql = f"""
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.table_constraints
                    WHERE constraint_name = '{constraint_name}'
                    AND table_schema = '{schema}'
                    AND table_name = '{table}'
                )
            """
            cursor.execute(check_constraint_sql)
            constraint_exists = cursor.fetchone()[0]

            if not constraint_exists:
                logger.info(f"ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„±: {constraint_name}")
                create_constraint_sql = f"""
                    ALTER TABLE {target_table}
                    ADD CONSTRAINT {constraint_name}
                    PRIMARY KEY ({pk_columns})
                """
                cursor.execute(create_constraint_sql)
                logger.info(f"ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„± ì™„ë£Œ: {constraint_name}")

            # ëª¨ë“  ì»¬ëŸ¼ ì¡°íšŒ (ê¸°ë³¸í‚¤ ì œì™¸)
            all_columns_query = f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = '{schema}' AND table_name = '{table}'
                AND column_name NOT IN ({', '.join([f"'{pk}'" for pk in primary_keys])})
                ORDER BY ordinal_position
            """

            cursor.execute(all_columns_query)
            non_pk_columns = cursor.fetchall()
            non_pk_column_names = [col[0] for col in non_pk_columns]

            # non_pk_column_namesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not non_pk_column_names:
                logger.warning(
                    "ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ì—¬ MERGEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
                )
                # ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if sync_mode == "full_sync":
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

                # MERGE ì‹¤í–‰
                start_time = pd.Timestamp.now()
                cursor.execute(merge_sql)
                end_time = pd.Timestamp.now()

                # ê²°ê³¼ í™•ì¸
                cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
                source_count = cursor.fetchone()[0]
                cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                target_count = cursor.fetchone()[0]

                merge_result = {
                    "source_count": source_count,
                    "target_count": target_count,
                    "sync_mode": sync_mode,
                    "execution_time": (end_time - start_time).total_seconds(),
                    "status": "success",
                    "message": (
                        f"MERGE ì™„ë£Œ (ê¸°ë³¸í‚¤ë§Œ): {source_table} -> {target_table}, "
                        f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                    ),
                }

                logger.info(merge_result["message"])
                return merge_result

            # MERGE ì¿¼ë¦¬ ìƒì„±
            if sync_mode == "full_sync":
                # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                if non_pk_column_names:
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì…
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table};

                        COMMIT;
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        BEGIN;

                        -- ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                        DELETE FROM {target_table}
                        WHERE ({pk_columns}) IN (
                            SELECT {pk_columns} FROM {source_table}
                        );

                        -- ìƒˆ ë°ì´í„° ì‚½ì… (ê¸°ë³¸í‚¤ë§Œ)
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table};

                        COMMIT;
                    """
            else:
                # ì¦ë¶„ ë™ê¸°í™”: UPSERT (INSERT ... ON CONFLICT)
                if non_pk_column_names:
                    update_set_clause = ", ".join(
                        [f"{col} = EXCLUDED.{col}" for col in non_pk_column_names]
                    )

                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns}, {', '.join(non_pk_column_names)})
                        SELECT {pk_columns}, {', '.join(non_pk_column_names)}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO UPDATE SET {update_set_clause};
                    """
                else:
                    # ë¹„ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸í‚¤ë§Œ ì‚¬ìš©
                    merge_sql = f"""
                        INSERT INTO {target_table} ({pk_columns})
                        SELECT {pk_columns}
                        FROM {source_table}
                        ON CONFLICT ({pk_columns})
                        DO NOTHING;
                    """

            # MERGE ì‹¤í–‰
            start_time = pd.Timestamp.now()
            cursor.execute(merge_sql)
            end_time = pd.Timestamp.now()

            # ê²°ê³¼ í™•ì¸
            cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
            source_count = cursor.fetchone()[0]
            cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
            target_count = cursor.fetchone()[0]

            merge_result = {
                "source_count": source_count,
                "target_count": target_count,
                "sync_mode": sync_mode,
                "execution_time": (end_time - start_time).total_seconds(),
                "status": "success",
                "message": (
                    f"MERGE ì™„ë£Œ: {source_table} -> {target_table}, "
                    f"ì†ŒìŠ¤: {source_count}í–‰, íƒ€ê²Ÿ: {target_count}í–‰"
                ),
            }

            logger.info(merge_result["message"])
            return merge_result

        except Exception as e:
            logger.error(f"MERGE ì‘ì—… ì‹¤íŒ¨: {source_table} -> {target_table}, ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "message": str(e),
                "source_count": 0,
                "target_count": 0,
                "sync_mode": sync_mode,
                "execution_time": 0
            }
