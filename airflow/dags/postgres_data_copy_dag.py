"""
PostgreSQL Multi-Table Data Copy DAG
ì—¬ëŸ¬ PostgreSQL í…Œì´ë¸”ì„ ìˆœì°¨ì ìœ¼ë¡œ ë³µì‚¬í•˜ëŠ” DAG

ì´ DAGëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì†ŒìŠ¤ PostgreSQLì—ì„œ ë°ì´í„° ì¶”ì¶œ (ë™ê¸°í™” ëª¨ë“œë³„)
2. ë°ì´í„° ë³€í™˜ ë° ê²€ì¦
3. íƒ€ê²Ÿ PostgreSQLì— ë°ì´í„° ë¡œë“œ
4. ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬

ë™ê¸°í™” ëª¨ë“œ:
- incremental_sync: ì¦ë¶„ ë™ê¸°í™” (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)
- full_sync: ì „ì²´ ë™ê¸°í™” (ì†ŒìŠ¤ì— ì—†ëŠ” ë°ì´í„° ì‚­ì œ)

ì°¸ê³ : https://github.com/apache/airflow/tree/providers-postgres/6.2.3/providers/postgres/tests/system/postgres
"""

import logging
import os
import subprocess
import time
from datetime import datetime, timedelta
from typing import Any

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "data_team",
    "depends_on_past": False,
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email": ["admin@example.com"],
}

# DAG ì •ì˜
dag = DAG(
    "postgres_multi_table_copy",
    default_args=default_args,
    description="Copy data from multiple PostgreSQL tables sequentially and create dbt snapshots",
    schedule_interval="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["postgres", "data-copy", "etl", "multi-table", "dbt-snapshot"],
    max_active_runs=1,
)

# ì—°ê²° ID ì„¤ì •
SOURCE_CONN_ID = "fs2_postgres"
TARGET_CONN_ID = "postgres_default"

# dbt í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
DBT_PROJECT_PATH = "/opt/airflow/dbt"

# ì—¬ëŸ¬ í…Œì´ë¸” ì„¤ì • (ìˆœì°¨ ì²˜ë¦¬)
TABLES_CONFIG = [
    {
        "source": "fds_íŒ©ì…‹.ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°",
        "target": "raw_data.ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°",
        "primary_key": ["ì¸í¬ë§¥ìŠ¤ì½”ë“œ", "íŒ©ì…‹ê±°ë˜ì†Œ", "gts_exnm", "í‹°ì»¤"],
        "sync_mode": "full_sync",  # 'incremental_sync' ë˜ëŠ” 'full_sync'
        "batch_size": 10000,
    }
    # {
    #     'source': "fds_íŒ©ì…‹.ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°",
    #     'target': "raw_data.ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°",
    #     'primary_key': ['ì¢…ëª©ì½”ë“œ'],
    #     'incremental_field': 'ì—…ë°ì´íŠ¸_ì‹œê°„',
    #     'incremental_field_type': 'timestamp',  # 'timestamp', 'yyyymmdd', 'date', 'datetime'
    #     'sync_mode': 'incremental_sync',  # 'incremental_sync' ë˜ëŠ” 'full_sync'
    #     'batch_size': 10000,
    #     'custom_where': 'ì‹œê°€ì´ì•¡ > 1000000000'  # ë¹„ì¦ˆë‹ˆìŠ¤ ì¡°ê±´ë§Œ
    # },
    # {
    #     'source': "fds_íŒ©ì…‹.ê±°ë˜ë‚´ì—­",
    #     'target': "raw_data.ê±°ë˜ë‚´ì—­",
    #     'primary_key': ['ê±°ë˜ë²ˆí˜¸'],
    #     'incremental_field': 'ê±°ë˜ì¼ì‹œ',
    #     'incremental_field_type': 'datetime',
    #     'sync_mode': 'incremental_sync',
    #     'batch_size': 5000,
    #     'custom_where': 'ê±°ë˜ìƒíƒœ = \'ì™„ë£Œ\''
    # },
    # {
    #     'source': "fds_íŒ©ì…‹.ê¸°ì¤€ì •ë³´",
    #     'target': "raw_data.ê¸°ì¤€ì •ë³´",
    #     'primary_key': ['ê¸°ì¤€ì¼ì', 'ì¢…ëª©ì½”ë“œ'],
    #     'incremental_field': 'ê¸°ì¤€ì¼ì',
    #     'incremental_field_type': 'date',
    #     'sync_mode': 'full_sync',
    #     'batch_size': 1000,
    #     'custom_where': 'ìœ íš¨ì—¬ë¶€ = \'Y\''
    # }
]


def get_common_filters() -> list[str]:
    """ê³µí†µìœ¼ë¡œ ì ìš©í•  ê¸°ë³¸ í•„í„° ë°˜í™˜"""
    return [
        "deleted_at IS NULL",  # ì‚­ì œë˜ì§€ ì•Šì€ ë°ì´í„°
        "status != 'deleted'",  # ì‚­ì œ ìƒíƒœê°€ ì•„ë‹Œ ë°ì´í„°
        "is_active = true",  # í™œì„± ë°ì´í„°
    ]


def get_table_specific_filters(table_config: dict, hook: PostgresHook) -> list[str]:
    """
    í…Œì´ë¸”ë³„ë¡œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ í•„í„° ì ìš©

    Args:
        table_config: í…Œì´ë¸” ì„¤ì •
        hook: PostgreSQL ì—°ê²° í›…

    Returns:
        ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì— ëŒ€í•œ í•„í„° ëª©ë¡
    """
    try:
        # ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        source_schema, source_table = table_config["source"].split(".")
        columns_sql = """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
        """

        columns = hook.get_records(
            columns_sql, parameters=(source_schema, source_table)
        )
        existing_columns = {col[0].lower() for col in columns}

        # ê³µí†µ í•„í„° ì¤‘ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ ì ìš©
        common_filters = get_common_filters()
        applicable_filters = []

        for filter_condition in common_filters:
            # ì»¬ëŸ¼ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
            if (
                ("deleted_at" in filter_condition and "deleted_at" in existing_columns)
                or ("status" in filter_condition and "status" in existing_columns)
                or ("is_active" in filter_condition and "is_active" in existing_columns)
            ):
                applicable_filters.append(filter_condition)

        return applicable_filters

    except Exception as e:
        logger.warning(
            f"Could not get table-specific filters for {table_config['source']}: {e!s}"
        )
        # ì—ëŸ¬ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í•„í„° ì—†ì´ ì§„í–‰)
        return []


def generate_temp_table_name(source_table: str, target_table: str) -> str:
    """
    ì†ŒìŠ¤ í…Œì´ë¸”ê³¼ íƒ€ê²Ÿ í…Œì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ temp í…Œì´ë¸” ì´ë¦„ ìë™ ìƒì„±

    Args:
        source_table: "schema.table" í˜•ì‹ì˜ ì†ŒìŠ¤ í…Œì´ë¸”
        target_table: "schema.table" í˜•ì‹ì˜ íƒ€ê²Ÿ í…Œì´ë¸”

    Returns:
        temp í…Œì´ë¸” ì´ë¦„ (ì˜ˆ: "temp_ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°")
    """
    # íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ í…Œì´ë¸”ëª…ë§Œ ì¶”ì¶œ
    target_schema, target_table_name = target_table.split(".")

    # temp_ ì ‘ë‘ì‚¬ ì¶”ê°€
    temp_table_name = f"temp_{target_table_name}"

    return temp_table_name


def get_table_config_with_temp(table_config: dict) -> dict:
    """
    í…Œì´ë¸” ì„¤ì •ì— temp í…Œì´ë¸” ì •ë³´ ì¶”ê°€

    Args:
        table_config: ê¸°ë³¸ í…Œì´ë¸” ì„¤ì •

    Returns:
        temp í…Œì´ë¸” ì •ë³´ê°€ ì¶”ê°€ëœ ì„¤ì •
    """
    temp_table_name = generate_temp_table_name(
        table_config["source"], table_config["target"]
    )

    # ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
    target_schema, target_table_name = table_config["target"].split(".")

    enhanced_config = table_config.copy()
    enhanced_config.update(
        {
            "temp_table": temp_table_name,
            "temp_table_full": f"{target_schema}.{temp_table_name}",
            "target_schema": target_schema,
            "target_table_name": target_table_name,
        }
    )

    # temp_table_fullì„ ì›ë³¸ table_configì—ë„ ì¶”ê°€ (ë‹¤ë¥¸ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©)
    table_config["temp_table_full"] = enhanced_config["temp_table_full"]

    return enhanced_config


def build_incremental_filter(
    field_name: str, field_type: str, last_update_time: Any
) -> str:
    """
    í•„ë“œ íƒ€ì…ì— ë”°ë¼ ì¦ë¶„ í•„í„° ìë™ ìƒì„±

    Args:
        field_name: ì¦ë¶„ í•„ë“œëª…
        field_type: í•„ë“œ íƒ€ì… ('timestamp', 'yyyymmdd', 'date', 'datetime')
        last_update_time: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„

    Returns:
        ì¦ë¶„ í•„í„° SQL ì¡°ê±´
    """
    if field_type == "timestamp":
        return f"{field_name} >= %(last_update_time)s"

    elif field_type == "yyyymmdd":
        # YYYYMMDD í˜•ì‹ (ì˜ˆ: 20240101)
        if isinstance(last_update_time, str):
            # ë¬¸ìì—´ì„ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_time = (
                last_update_time.replace("-", "").replace(":", "").replace(" ", "")[:8]
            )
        else:
            formatted_time = last_update_time.strftime("%Y%m%d")
        return f"{field_name} >= {formatted_time}"

    elif field_type == "date":
        # DATE í˜•ì‹ (ì˜ˆ: 2024-01-01)
        if isinstance(last_update_time, str):
            formatted_time = last_update_time[:10]  # YYYY-MM-DD ë¶€ë¶„ë§Œ
        else:
            formatted_time = last_update_time.strftime("%Y-%m-%d")
        return f"{field_name} >= '{formatted_time}'"

    elif field_type == "datetime":
        # DATETIME í˜•ì‹ (ì˜ˆ: 2024-01-01 00:00:00)
        if isinstance(last_update_time, str):
            formatted_time = last_update_time[:19]  # YYYY-MM-DD HH:MM:SS ë¶€ë¶„ë§Œ
        else:
            formatted_time = last_update_time.strftime("%Y-%m-%d %H:%M:%S")
        return f"{field_name} >= '{formatted_time}'"

    else:
        raise ValueError(f"Unsupported field type: {field_type}")


def build_dynamic_sql(table_config: dict, sql_type: str, **kwargs) -> str:
    """
    í…Œì´ë¸” ì„¤ì •ì— ë”°ë¼ ë™ì ìœ¼ë¡œ SQL ìƒì„±
    """
    # ğŸš¨ PRIORITY ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ - ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜
    if table_config['source'] == 'm23.edi_690':
        logger.info("ğŸš¨ m23.edi_690 í…Œì´ë¸” ê°ì§€! PRIORITY ì»¬ëŸ¼ ì†Œìˆ˜ì  ê°’ ë³€í™˜ SQL ìƒì„±")
        base_sql = f"""
            SELECT
                eventcd, eventid, optionid, serialid, scexhid, sedolid, actflag, changed, created,
                secid, issid, isin, uscode, issuername, cntryofincorp, sectycd, securitydesc,
                parvalue, pvcurrency, statusflag, primaryexchgcd, sedol, sedolcurrency, defunct,
                sedolregcntry, exchgcntry, exchgcd, mic, micseg, localcode, liststatus, issuedate,
                date1type, date1, date2type, date2, date3type, date3, date4type, date4, date5type,
                date5, date6type, date6, date7type, date7, date8type, date8, date9type, date9,
                date10type, date10, date11type, date11, date12type, date12, paytype,
                CASE
                    WHEN priority IS NULL THEN NULL
                    WHEN priority = '' THEN NULL
                    WHEN priority ~ '^[0-9]+\.[0-9]+$' THEN CAST(CAST(priority AS NUMERIC) AS BIGINT)::TEXT
                    ELSE priority
                END AS priority,
                defaultopt, outurnsecid, outurnisin, ratioold, rationew, fractions, currency,
                rate1type, rate1, rate2type, rate2, field1name, field1, field2name, field2,
                field3name, field3, field4name, field4, field5name, field5, field6name, field6,
                field7name, field7, field8name, field8, field9name, field9, field10name, field10,
                field11name, field11, field12name, field12, field13name, field13, field14name,
                field14, field15name, field15, field16name, field16, field17name, field17,
                field18name, field18, field19name, field19, field20name, field20, field21name,
                field21, field22name, field22, uptodate
            FROM {table_config['source']}
        """
    else:
        base_sql = f"SELECT * FROM {table_config['source']}"

    where_conditions = []

    if sql_type == "select_incremental":
        # ì¦ë¶„ ë™ê¸°í™” ì‹œì—ë§Œ ì¦ë¶„ í•„í„° ì¶”ê°€
        if table_config["sync_mode"] == "incremental_sync":
            incremental_filter = build_incremental_filter(
                table_config["incremental_field"],
                table_config["incremental_field_type"],
                kwargs.get("last_update_time"),
            )
            where_conditions.append(incremental_filter)

    # í…Œì´ë¸”ë³„ ê³µí†µ í•„í„° ì ìš© (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ)
    try:
        hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)
        table_filters = get_table_specific_filters(table_config, hook)
        where_conditions.extend(table_filters)
    except Exception as e:
        logger.warning(f"Could not apply table-specific filters: {e!s}")
        # ì—ëŸ¬ ì‹œ ê¸°ë³¸ ê³µí†µ í•„í„° ì ìš©
        common_filters = get_common_filters()
        where_conditions.extend(common_filters)

    # ì»¤ìŠ¤í…€ WHERE ì¡°ê±´ ì¶”ê°€
    if table_config.get("custom_where"):
        where_conditions.append(table_config["custom_where"])

    # WHERE ì ˆ ì¶”ê°€
    if where_conditions:
        base_sql += " WHERE " + " AND ".join(where_conditions)

    # ORDER BY ì¶”ê°€ (ì¦ë¶„ í•„ë“œ ê¸°ì¤€)
    if table_config.get("incremental_field"):
        base_sql += f" ORDER BY {table_config['incremental_field']} DESC"

    return base_sql


def get_default_time_by_type(field_type: str) -> Any:
    """í•„ë“œ íƒ€ì…ë³„ ê¸°ë³¸ ì‹œê°„ ë°˜í™˜"""
    if field_type == "yyyymmdd":
        return "19000101"
    elif field_type == "date":
        return "1900-01-01"
    elif field_type == "datetime":
        return "1900-01-01 00:00:00"
    else:  # timestamp
        return datetime(1900, 1, 1)


def get_last_update_time(table_config: dict, **context) -> Any:
    """
    í…Œì´ë¸”ë³„ë¡œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ìë™ ê°ì§€
    full_sync ëª¨ë“œì¼ ë•ŒëŠ” None ë°˜í™˜
    """
    # full_sync ëª¨ë“œì¼ ë•ŒëŠ” ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ
    if table_config.get("sync_mode") == "full_sync":
        logger.info(
            f"Full sync mode for {table_config['source']}, skipping last update time check"
        )
        return None

    # incremental_sync ëª¨ë“œì¼ ë•Œë§Œ incremental_fieldì™€ incremental_field_typeì´ í•„ìš”
    if not table_config.get("incremental_field") or not table_config.get(
        "incremental_field_type"
    ):
        logger.warning(
            f"Missing incremental_field or incremental_field_type for {table_config['source']} in incremental_sync mode"
        )
        return None

    try:
        target_hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)
        incremental_field = table_config["incremental_field"]

        # íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¡°íšŒ
        last_update_sql = f"""
            SELECT MAX({incremental_field})
            FROM {table_config['target']}
            WHERE {incremental_field} IS NOT NULL
        """

        result = target_hook.get_first(last_update_sql)
        last_update_time = result[0] if result and result[0] else None

        if last_update_time:
            logger.info(
                f"Last update time for {table_config['source']}: {last_update_time}"
            )
            return last_update_time
        else:
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            field_type = table_config["incremental_field_type"]
            return get_default_time_by_type(field_type)

    except Exception as e:
        logger.warning(
            f"Could not get last update time for {table_config['source']}: {e!s}"
        )
        # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return get_default_time_by_type(table_config["incremental_field_type"])


def parse_table_name(full_table_name: str) -> tuple[str, str]:
    """
    ì „ì²´ í…Œì´ë¸”ëª…ì—ì„œ ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª…ì„ ë¶„ë¦¬

    Args:
        full_table_name: "schema.table" í˜•ì‹ì˜ í…Œì´ë¸”ëª…

    Returns:
        (schema_name, table_name) íŠœí”Œ

    Raises:
        ValueError: í…Œì´ë¸”ëª… í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš°
    """
    if "." not in full_table_name:
        raise ValueError(
            f"Invalid table name format: {full_table_name}. Expected 'schema.table'"
        )

    parts = full_table_name.split(".")
    if len(parts) != 2:
        raise ValueError(
            f"Invalid table name format: {full_table_name}. Expected 'schema.table'"
        )

    return parts[0], parts[1]


def check_source_connection(table_config: dict, **context) -> dict[str, Any]:
    """ì†ŒìŠ¤ PostgreSQL ì—°ê²° ìƒíƒœ í™•ì¸ (í…Œì´ë¸”ë³„)"""
    try:
        logger.info(f"Checking source connection for table: {table_config['source']}")

        hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)
        conn = hook.get_conn()
        cursor = conn.cursor()

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        cursor.execute("SELECT 1")
        result = cursor.fetchone()
        logger.info(
            f"Source database connection successful for {table_config['source']}"
        )

        return {"status": "success", "table": table_config["source"]}

    except Exception as e:
        error_msg = f"Source connection failed for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def check_target_connection(table_config: dict, **context) -> dict[str, Any]:
    """íƒ€ê²Ÿ PostgreSQL ì—°ê²° ìƒíƒœ í™•ì¸ (í…Œì´ë¸”ë³„)"""
    try:
        logger.info(f"Checking target connection for table: {table_config['target']}")

        hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)
        conn = hook.get_conn()
        cursor = conn.cursor()

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        cursor.execute("SELECT 1")
        result = cursor.fetchone()
        logger.info(
            f"Target database connection successful for {table_config['target']}"
        )

        return {"status": "success", "table": table_config["target"]}

    except Exception as e:
        error_msg = f"Target connection failed for {table_config['target']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def get_source_data_count(table_config: dict, **context) -> dict[str, Any]:
    """ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ ì¡°íšŒ (í…Œì´ë¸”ë³„)"""
    try:
        logger.info(f"Getting data count for table: {table_config['source']}")

        hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)

        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
        last_update_time = get_last_update_time(table_config, **context)

        # ë™ê¸°í™” ëª¨ë“œì— ë”°ë¼ SQL ìƒì„±
        if table_config["sync_mode"] == "incremental_sync":
            count_sql = build_dynamic_sql(
                table_config, "select_incremental", last_update_time=last_update_time
            )
        else:
            count_sql = build_dynamic_sql(table_config, "select_full")

        # COUNT(*) ì¿¼ë¦¬ë¡œ ë³€í™˜
        count_sql = count_sql.replace("SELECT *", "SELECT COUNT(*)")

        logger.info(f"Count SQL: {count_sql}")

        result = hook.get_first(count_sql)
        total_count = result[0] if result else 0

        logger.info(
            f"Total records to process for {table_config['source']}: {total_count}"
        )

        # target_table_nameì„ ì•ˆì „í•˜ê²Œ ìƒì„±
        target_schema, target_table_name = table_config["target"].split(".")

        # XComì— ì €ì¥
        context["task_instance"].xcom_push(
            key=f"source_data_count_{target_table_name}", value=total_count
        )

        return {"status": "success", "total_count": total_count, "sql_used": count_sql}

    except Exception as e:
        error_msg = f"Failed to get data count for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def create_temp_table(table_config: dict, **context) -> str:
    """ì„ì‹œ í…Œì´ë¸” ìƒì„± (í…Œì´ë¸”ë³„)"""
    try:
        enhanced_config = get_table_config_with_temp(table_config)
        temp_table_full = enhanced_config["temp_table_full"]

        logger.info(f"Creating temporary table: {temp_table_full}")

        hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸° (ì†ŒìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ)
        source_schema, source_table = table_config["source"].split(".")
        source_hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)

        columns_sql = """
            SELECT column_name, data_type AS type, is_nullable AS nullable, column_default AS default
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """

        columns = source_hook.get_records(
            columns_sql, parameters=(source_schema, source_table)
        )

        logger.info(
            f"Found {len(columns)} columns in source table: {[col[0] for col in columns]}"
        )

        if not columns:
            raise Exception(f"No columns found for table {table_config['source']}")

        # ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
        table_exists_sql = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = '{enhanced_config['target_schema']}'
                AND table_name = '{enhanced_config['temp_table']}'
            )
        """
        table_exists = hook.get_first(table_exists_sql)[0]

        if table_exists:
            logger.info(
                f"Table {temp_table_full} already exists, checking column structure..."
            )

            # ê¸°ì¡´ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            existing_columns_sql = f"""
                SELECT column_name, data_type AS type, is_nullable AS nullable, column_default AS default
                FROM information_schema.columns
                WHERE table_schema = '{enhanced_config['target_schema']}'
                AND table_name = '{enhanced_config['temp_table']}'
                ORDER BY ordinal_position
            """
            existing_columns = hook.get_records(existing_columns_sql)
            existing_column_names = [col[0] for col in existing_columns]
            expected_column_names = [col[0] for col in columns]

            logger.info(f"Existing columns: {existing_column_names}")
            logger.info(f"Expected columns: {expected_column_names}")

            # ì»¬ëŸ¼ êµ¬ì¡°ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            if existing_column_names == expected_column_names:
                logger.info(
                    f"Column structure matches, using existing table {temp_table_full}"
                )
                return f"Using existing table {temp_table_full} with {len(existing_columns)} columns"
            else:
                logger.info(
                    f"Column structure mismatch, dropping and recreating table {temp_table_full}"
                )
                hook.run(f"DROP TABLE IF EXISTS {temp_table_full}")

        # CREATE TABLE êµ¬ë¬¸ ìƒì„±
        create_sql = f"CREATE TABLE {temp_table_full} ("
        column_definitions = []

        for col in columns:
            col_name, col_type, nullable, col_default = col
            nullable_clause = "NULL" if nullable == "YES" else "NOT NULL"
            default = f" DEFAULT {col_default}" if col_default else ""
            column_definitions.append(
                f"{col_name} {col_type} {nullable_clause}{default}"
            )

        create_sql += ", ".join(column_definitions) + ")"

        logger.info(f"CREATE TABLE SQL: {create_sql}")

        hook.run(create_sql)
        logger.info(
            f"Temporary table {temp_table_full} created successfully with {len(columns)} columns"
        )

        # ìƒì„±ëœ í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ ìˆ˜ í™•ì¸
        verify_sql = f"""
            SELECT COUNT(*) as column_count
            FROM information_schema.columns
            WHERE table_schema = '{enhanced_config['target_schema']}'
            AND table_name = '{enhanced_config['temp_table']}'
        """
        actual_columns = hook.get_first(verify_sql)[0]
        logger.info(f"Verified: Temporary table actually has {actual_columns} columns")

        if actual_columns != len(columns):
            raise Exception(
                f"Column count mismatch: expected {len(columns)}, but table has {actual_columns}"
            )

        return (
            f"Temporary table {temp_table_full} created with {actual_columns} columns"
        )

    except Exception as e:
        error_msg = (
            f"Failed to create temporary table for {table_config['source']}: {e!s}"
        )
        logger.error(error_msg)
        raise Exception(error_msg)


def ensure_target_table_exists(table_config: dict, **context) -> str:
    """
    íƒ€ê²Ÿ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ì†ŒìŠ¤ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
    """
    try:
        logger.info(f"Ensuring target table exists: {table_config['target']}")

        target_hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)
        source_hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)

        # íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª… ë¶„ë¦¬
        target_schema, target_table = table_config["target"].split(".")

        # 1ë‹¨ê³„: ìŠ¤í‚¤ë§ˆê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
        schema_exists_sql = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.schemata
                WHERE schema_name = '{target_schema}'
            )
        """
        schema_exists = target_hook.get_first(schema_exists_sql)[0]

        if not schema_exists:
            logger.info(f"Schema {target_schema} does not exist, creating it...")
            create_schema_sql = f"CREATE SCHEMA IF NOT EXISTS {target_schema}"
            target_hook.run(create_schema_sql)
            logger.info(f"Schema {target_schema} created successfully")
        else:
            logger.info(f"Schema {target_schema} already exists")

        # 2ë‹¨ê³„: íƒ€ê²Ÿ í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        check_sql = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = '{target_schema}'
                AND table_name = '{target_table}'
            )
        """

        logger.info(f"Checking table existence with SQL: {check_sql}")
        table_exists = target_hook.get_first(check_sql)[0]
        logger.info(f"Table existence check result: {table_exists}")

        if table_exists:
            logger.info(f"Target table {table_config['target']} already exists")
            # ì‹¤ì œë¡œ í…Œì´ë¸”ì´ ì •ë§ ì¡´ì¬í•˜ëŠ”ì§€ í•œ ë²ˆ ë” í™•ì¸
            verify_sql = f"SELECT COUNT(*) FROM {table_config['target']} LIMIT 1"
            try:
                row_count = target_hook.get_first(verify_sql)[0]
                logger.info(f"Table verification successful: {row_count} rows found")
                return f"Target table {table_config['target']} already exists and verified"
            except Exception as verify_error:
                logger.warning(f"Table verification failed: {verify_error}, will recreate table")
                # í…Œì´ë¸”ì´ ì‹¤ì œë¡œëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìƒì„± ì§„í–‰
                pass

        # ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (max_length í¬í•¨)
        source_schema, source_table = table_config["source"].split(".")
        columns_sql = """
            SELECT column_name, data_type AS type, is_nullable AS nullable, column_default AS default, character_maximum_length AS max_length
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """

        columns = source_hook.get_records(
            columns_sql, parameters=(source_schema, source_table)
        )

        if not columns:
            raise Exception(
                f"No columns found in source table {table_config['source']}"
            )

        # CREATE TABLE êµ¬ë¬¸ ìƒì„± - DataCopyEngineê³¼ ë™ì¼í•œ íƒ€ì… ë§¤í•‘ ì‚¬ìš©
        create_sql = f"CREATE TABLE {table_config['target']} ("
        column_definitions = []

        for col in columns:
            col_name, col_type, nullable, col_default, max_length = col  # max_length ì¶”ê°€
            is_nullable = nullable == "YES"

            # DataCopyEngineê³¼ ë™ì¼í•œ íƒ€ì… ë§¤í•‘ ë¡œì§ ì‚¬ìš©
            if "char" in col_type.lower() or "text" in col_type.lower():
                if max_length and max_length > 0:
                    pg_type = f"VARCHAR({max_length})"
                else:
                    pg_type = "TEXT"
            elif "int" in col_type.lower():
                if "bigint" in col_type.lower():
                    pg_type = "BIGINT"
                elif "smallint" in col_type.lower():
                    pg_type = "SMALLINT"
                else:
                    pg_type = "INTEGER"
            elif "decimal" in col_type.lower() or "numeric" in col_type.lower():
                pg_type = "NUMERIC"
            elif "float" in col_type.lower() or "double" in col_type.lower():
                pg_type = "DOUBLE PRECISION"
            elif "real" in col_type.lower():
                pg_type = "REAL"
            elif "date" in col_type.lower():
                pg_type = "DATE"
            elif "time" in col_type.lower():
                if "timestamp" in col_type.lower():
                    pg_type = "TIMESTAMP"
                else:
                    pg_type = "TIME"
            elif "bool" in col_type.lower():
                pg_type = "BOOLEAN"
            elif "json" in col_type.lower():
                pg_type = "JSONB"
            elif "uuid" in col_type.lower():
                pg_type = "UUID"
            else:
                pg_type = "TEXT"  # ê¸°ë³¸ê°’

            nullable_clause = "NOT NULL" if not is_nullable else ""
            default = f" DEFAULT {col_default}" if col_default else ""
            column_definitions.append(
                f"{col_name} {pg_type} {nullable_clause}{default}"
            )

        create_sql += ", ".join(column_definitions) + ")"

        logger.info(f"CREATE TABLE SQL for target: {create_sql}")

        target_hook.run(create_sql)
        logger.info(
            f"Target table {table_config['target']} created successfully with {len(columns)} columns"
        )

        return (
            f"Target table {table_config['target']} created with {len(columns)} columns"
        )

    except Exception as e:
        error_msg = (
            f"Failed to ensure target table exists for {table_config['source']}: {e!s}"
        )
        logger.error(error_msg)
        raise Exception(error_msg)


def log_table_structure(table_config: dict, **context) -> str:
    """
    ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ êµ¬ì¡°ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹… ì§€ì›
    """
    try:
        logger.info(f"Logging table structure for debugging: {table_config['source']}")

        source_hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)
        target_hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # ì†ŒìŠ¤ í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´
        source_schema, source_table = table_config["source"].split(".")
        source_columns_sql = """
            SELECT column_name, data_type AS type
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """

        source_columns = source_hook.get_records(
            source_columns_sql, parameters=(source_schema, source_table)
        )
        source_column_names = [col[0] for col in source_columns]

        logger.info(
            f"Source table {table_config['source']} columns: {source_column_names}"
        )

        # íƒ€ê²Ÿ í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´
        target_schema, target_table = table_config["target"].split(".")
        target_columns_sql = """
            SELECT column_name, data_type AS type
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """

        target_columns = target_hook.get_records(
            target_columns_sql, parameters=(target_schema, target_table)
        )
        target_column_names = [col[0] for col in target_columns]

        logger.info(
            f"Target table {table_config['target']} columns: {target_column_names}"
        )

        # ì»¬ëŸ¼ ì°¨ì´ì  ë¶„ì„
        missing_in_target = set(source_column_names) - set(target_column_names)
        extra_in_target = set(target_column_names) - set(source_column_names)

        if missing_in_target:
            logger.warning(f"Columns missing in target: {list(missing_in_target)}")
        if extra_in_target:
            logger.warning(f"Extra columns in target: {list(extra_in_target)}")

        return f"Table structure logged. Source: {len(source_columns)} columns, Target: {len(target_columns)} columns"

    except Exception as e:
        logger.warning(
            f"Could not log table structure for {table_config['source']}: {e!s}"
        )
        return f"Failed to log table structure: {e!s}"


def copy_data_with_dynamic_sql(table_config: dict, **context) -> dict[str, Any]:
    """
    ë™ì  SQLì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë³µì‚¬
    """
    try:
        enhanced_config = get_table_config_with_temp(table_config)
        temp_table_full = enhanced_config["temp_table_full"]

        logger.info(
            f"Starting data copy with dynamic SQL from {table_config['source']} to {temp_table_full}"
        )

        source_hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)
        target_hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
        last_update_time = get_last_update_time(table_config, **context)

        # ë™ê¸°í™” ëª¨ë“œì— ë”°ë¼ SQL ìƒì„±
        if table_config["sync_mode"] == "incremental_sync":
            select_sql = build_dynamic_sql(
                table_config, "select_incremental", last_update_time=last_update_time
            )
        else:
            select_sql = build_dynamic_sql(table_config, "select_full")

        logger.info(f"Generated SQL: {select_sql}")

        # 1ë‹¨ê³„: ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œí•˜ì—¬ CSVë¡œ ë‚´ë³´ë‚´ê¸° (psql \copy ì‚¬ìš©)
        logger.info("Step 1: Exporting data from source to CSV using psql \\copy")
        csv_filename = f"/tmp/source_data_{enhanced_config['target_table_name']}.csv"

        # ì†ŒìŠ¤ ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        source_conn = source_hook.get_conn()
        source_info = source_conn.get_dsn_parameters()

        # ê°„ë‹¨í•œ psql \copy ëª…ë ¹ì–´ë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        export_cmd = f"psql -h {source_info.get('host', 'localhost')} -p {source_info.get('port', '15432')} -U {source_info.get('user')} -d {source_info.get('dbname')} -c \"\\copy ({select_sql}) TO '{csv_filename}' WITH CSV HEADER\""

        logger.info(f"Executing export command: {export_cmd}")
        result = subprocess.run(export_cmd, shell=True, capture_output=True, text=True)

        if result.returncode != 0:
            raise Exception(f"Export failed: {result.stderr}")

        logger.info("Data exported to CSV successfully")

        # 2ë‹¨ê³„: CSVì—ì„œ íƒ€ê²Ÿìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (psql \copy ì‚¬ìš©)
        logger.info("Step 2: Importing data from CSV to target using psql \\copy")

        # íƒ€ê²Ÿ ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë¹„ë°€ë²ˆí˜¸ í¬í•¨)
        target_conn = target_hook.get_conn()
        target_info = target_conn.get_dsn_parameters()

        # PostgresHookì—ì„œ ì§ì ‘ ë¹„ë°€ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
        target_password = target_hook.get_connection(
            target_hook.postgres_conn_id
        ).password

        logger.info(
            f"Target connection info - Host: {target_info.get('host')}, Port: {target_info.get('port')}, User: {target_info.get('user')}, DB: {target_info.get('dbname')}, Password: {'***' if target_password else 'None'}"
        )

        # ê°„ë‹¨í•œ psql \copy ëª…ë ¹ì–´ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë¹„ë°€ë²ˆí˜¸ í¬í•¨)
        if target_password:
            # ë¹„ë°€ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ PGPASSWORD í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
            import_cmd = f"psql -h {target_info.get('host', 'localhost')} -p {target_info.get('port', '15432')} -U {target_info.get('user')} -d {target_info.get('dbname')} -c \"\\copy {temp_table_full} FROM '{csv_filename}' WITH CSV HEADER\""
        else:
            # ë¹„ë°€ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª…ë ¹ì–´
            import_cmd = f"psql -h {target_info.get('host', 'localhost')} -p {target_info.get('port', '15432')} -U {target_info.get('user')} -d {target_info.get('dbname')} -c \"\\copy {temp_table_full} FROM '{csv_filename}' WITH CSV HEADER\""

        logger.info(f"Executing import command: {import_cmd}")

        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        env = os.environ.copy()
        if target_password:
            env["PGPASSWORD"] = target_password
            logger.info("PGPASSWORD environment variable set")
        else:
            logger.warning("No password found for target connection")

        result = subprocess.run(
            import_cmd, shell=True, capture_output=True, text=True, env=env
        )

        if result.returncode != 0:
            raise Exception(f"Import failed: {result.stderr}")

        logger.info("Data imported from CSV successfully")

        # 3ë‹¨ê³„: ì¤‘ë³µ ì œê±° (Primary Key ê¸°ì¤€)
        logger.info("Step 3: Removing duplicates based on primary key")

        # Primary Key ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë¬¸ìì—´ ìƒì„±
        if isinstance(table_config["primary_key"], list):
            pk_columns = ", ".join(table_config["primary_key"])
        else:
            pk_columns = table_config["primary_key"]

        # ë” íš¨ìœ¨ì ì¸ ì¤‘ë³µ ì œê±° ë°©ë²• (ROW_NUMBER ì‚¬ìš©)
        dedup_sql = f"""
            DELETE FROM {temp_table_full}
            WHERE ctid IN (
                SELECT ctid
                FROM (
                    SELECT ctid,
                           ROW_NUMBER() OVER (
                               PARTITION BY {pk_columns}
                               ORDER BY ctid
                           ) as rn
                    FROM {temp_table_full}
                ) t
                WHERE t.rn > 1
            )
        """

        target_hook.run(dedup_sql)
        logger.info(f"Removed duplicates from {temp_table_full}")

        # 4ë‹¨ê³„: ë³µì‚¬ëœ ë ˆì½”ë“œ ìˆ˜ í™•ì¸ (ì¤‘ë³µ ì œê±° í›„)
        copied_count = target_hook.get_first(f"SELECT COUNT(*) FROM {temp_table_full}")[
            0
        ]

        context["task_instance"].xcom_push(
            key=f'copied_records_count_{enhanced_config["target_table_name"]}',
            value=copied_count,
        )

        logger.info(f"Data copy completed. Total records copied: {copied_count}")

        return {
            "status": "success",
            "copied_count": copied_count,
            "sql_used": select_sql,
        }

    except Exception as e:
        error_msg = f"Data copy failed for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def prepare_merge_parameters(table_config: dict, **context) -> dict[str, Any]:
    """
    MERGE ì‘ì—…ì„ ìœ„í•œ ë™ì  íŒŒë¼ë¯¸í„° ì¤€ë¹„ (í…Œì´ë¸”ë³„)
    """
    try:
        logger.info(f"Preparing MERGE parameters for table: {table_config['source']}")

        source_hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)
        target_hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # 1ë‹¨ê³„: ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ëª¨ë“  ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        schema_name, table_name = parse_table_name(table_config["source"])

        columns_sql = """
            SELECT column_name, data_type AS type
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """

        columns = source_hook.get_records(
            columns_sql, parameters=(schema_name, table_name)
        )
        all_columns = [col[0] for col in columns]

        # 2ë‹¨ê³„: ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼ ëª©ë¡ ìƒì„± (ì œì™¸ ì»¬ëŸ¼ ì œì™¸)
        exclude_columns = ["created_at"]

        # incremental_sync ëª¨ë“œì¼ ë•Œë§Œ incremental_field ì œì™¸
        if table_config.get("sync_mode") == "incremental_sync" and table_config.get(
            "incremental_field"
        ):
            exclude_columns.append(table_config["incremental_field"])

        # primary_keyê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(table_config["primary_key"], list):
            exclude_columns.extend(table_config["primary_key"])
        else:
            exclude_columns.append(table_config["primary_key"])

        update_columns = [col for col in all_columns if col not in exclude_columns]

        # 3ë‹¨ê³„: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
        last_update_time = get_last_update_time(table_config, **context)

        # enhanced_configì—ì„œ temp_table_full ê°€ì ¸ì˜¤ê¸°
        enhanced_config = get_table_config_with_temp(table_config)
        temp_table_full = enhanced_config["temp_table_full"]

        # 4ë‹¨ê³„: MERGE íŒŒë¼ë¯¸í„° êµ¬ì„±
        merge_params = {
            "target_table": table_config["target"],
            "temp_table": temp_table_full,
            "primary_key": table_config["primary_key"],
            "incremental_field": table_config.get(
                "incremental_field"
            ),  # Noneì¼ ìˆ˜ ìˆìŒ
            "update_columns": update_columns,
            "all_columns": all_columns,
            "last_update_time": last_update_time,
        }

        logger.info(f"MERGE parameters prepared: {merge_params}")

        # target_table_nameì„ ì•ˆì „í•˜ê²Œ ìƒì„±
        target_schema, target_table_name = table_config["target"].split(".")

        # 5ë‹¨ê³„: XComì— íŒŒë¼ë¯¸í„° ì €ì¥
        context["task_instance"].xcom_push(
            key=f"merge_parameters_{target_table_name}", value=merge_params
        )

        return merge_params

    except Exception as e:
        error_msg = (
            f"Failed to prepare MERGE parameters for {table_config['source']}: {e!s}"
        )
        logger.error(error_msg)
        raise Exception(error_msg)


def execute_full_sync_merge(table_config: dict, merge_params: dict, **context) -> str:
    """ì „ì²´ ë™ê¸°í™” MERGE ì‹¤í–‰ (SQL íŒŒì¼ ì‚¬ìš©)"""
    try:
        hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # enhanced_configì—ì„œ temp_table_full ê°€ì ¸ì˜¤ê¸°
        enhanced_config = get_table_config_with_temp(table_config)
        temp_table_full = enhanced_config["temp_table_full"]

        # SQL íŒŒì¼ì—ì„œ MERGE SQL ì½ê¸°
        import os

        sql_file_path = os.path.join(
            os.path.dirname(__file__), "sql", "full_sync_merge.sql"
        )

        with open(sql_file_path) as f:
            merge_sql = f.read()

        # SQL íŒŒë¼ë¯¸í„° ì¹˜í™˜
        merge_sql = merge_sql.replace("TARGET_TABLE", table_config["target"])
        merge_sql = merge_sql.replace("TEMP_TABLE", temp_table_full)

        # primary_keyê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(table_config["primary_key"], list):
            primary_key_condition = " AND ".join(
                [f"target.{pk} = source.{pk}" for pk in table_config["primary_key"]]
            )
        else:
            primary_key_condition = f"target.{table_config['primary_key']} = source.{table_config['primary_key']}"
        merge_sql = merge_sql.replace("PRIMARY_KEY_CONDITION", primary_key_condition)

        # full_sync ëª¨ë“œì—ì„œëŠ” incremental_field ê´€ë ¨ ì¹˜í™˜ ê±´ë„ˆë›°ê¸°
        # ì»¬ëŸ¼ ì •ë³´ ì¹˜í™˜
        update_columns = merge_params.get("update_columns", [])
        all_columns = merge_params.get("all_columns", [])

        # ì—…ë°ì´íŠ¸ ì»¬ëŸ¼ ì¹˜í™˜
        update_set_clause = ", ".join(
            [f"{col} = source.{col}" for col in update_columns]
        )

        # updated_at ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì¡°ê±´ë¶€ë¡œ ì¶”ê°€
        target_schema, target_table = table_config["target"].split(".")
        updated_at_exists_sql = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.columns
                WHERE table_schema = '{target_schema}'
                AND table_name = '{target_table}'
                AND column_name = 'updated_at'
            )
        """
        updated_at_exists = hook.get_first(updated_at_exists_sql)[0]

        if updated_at_exists:
            update_set_clause += ", updated_at = CURRENT_TIMESTAMP"
            logger.info(
                f"updated_at column exists in {table_config['target']}, adding to UPDATE SET"
            )
        else:
            logger.info(
                f"updated_at column does not exist in {table_config['target']}, skipping"
            )

        merge_sql = merge_sql.replace("UPDATE_COLUMNS", update_set_clause)

        # INSERT ì»¬ëŸ¼ ì¹˜í™˜
        insert_columns = ", ".join(all_columns)
        insert_values = ", ".join([f"source.{col}" for col in all_columns])
        merge_sql = merge_sql.replace("ALL_COLUMNS", insert_columns)
        merge_sql = merge_sql.replace("INSERT_VALUES", insert_values)

        # PRIMARY_KEY ì¹˜í™˜ (ê²°ê³¼ í™•ì¸ìš©)
        if isinstance(table_config["primary_key"], list):
            primary_key_str = table_config["primary_key"][0]  # ì²« ë²ˆì§¸ PKë§Œ ì‚¬ìš©
        else:
            primary_key_str = table_config["primary_key"]
        merge_sql = merge_sql.replace("PRIMARY_KEY", primary_key_str)

        logger.info(f"Executing full sync MERGE SQL for {table_config['source']}")

        # MERGE ì‹¤í–‰
        hook.run(merge_sql)

        logger.info(f"Full sync MERGE completed for {table_config['source']}")
        return "Full sync MERGE completed"

    except Exception as e:
        error_msg = f"Full sync MERGE failed for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def execute_incremental_merge(table_config: dict, merge_params: dict, **context) -> str:
    """ì¦ë¶„ ë™ê¸°í™” MERGE ì‹¤í–‰ (SQL íŒŒì¼ ì‚¬ìš©)"""
    try:
        hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # enhanced_configì—ì„œ temp_table_full ê°€ì ¸ì˜¤ê¸°
        enhanced_config = get_table_config_with_temp(table_config)
        temp_table_full = enhanced_config["temp_table_full"]

        # SQL íŒŒì¼ì—ì„œ MERGE SQL ì½ê¸°
        import os

        sql_file_path = os.path.join(
            os.path.dirname(__file__), "sql", "incremental_merge.sql"
        )

        with open(sql_file_path) as file:
            merge_sql = file.read()

        # SQL íŒŒë¼ë¯¸í„° ì¹˜í™˜
        merge_sql = merge_sql.replace("TARGET_TABLE", table_config["target"])
        merge_sql = merge_sql.replace("TEMP_TABLE", temp_table_full)

        # primary_keyê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(table_config["primary_key"], list):
            primary_key_condition = " AND ".join(
                [f"target.{pk} = source.{pk}" for pk in table_config["primary_key"]]
            )
        else:
            primary_key_condition = f"target.{table_config['primary_key']} = source.{table_config['primary_key']}"
        merge_sql = merge_sql.replace("PRIMARY_KEY_CONDITION", primary_key_condition)

        # incremental_fieldê°€ ìˆì„ ë•Œë§Œ ì¹˜í™˜, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì¹˜í™˜
        incremental_field = table_config.get("incremental_field", "")
        merge_sql = merge_sql.replace("INCREMENTAL_FIELD", incremental_field)

        # ì»¬ëŸ¼ ì •ë³´ ì¹˜í™˜
        update_columns = merge_params.get("update_columns", [])
        all_columns = merge_params.get("all_columns", [])

        # ì—…ë°ì´íŠ¸ ì»¬ëŸ¼ ì¹˜í™˜
        update_set_clause = ", ".join(
            [f"{col} = source.{col}" for col in update_columns]
        )
        merge_sql = merge_sql.replace("UPDATE_COLUMNS", update_set_clause)

        # INSERT ì»¬ëŸ¼ ì¹˜í™˜
        insert_columns = ", ".join(all_columns)
        insert_values = ", ".join([f"source.{col}" for col in all_columns])
        merge_sql = merge_sql.replace("ALL_COLUMNS", insert_columns)
        merge_sql = merge_sql.replace("INSERT_VALUES", insert_values)

        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¹˜í™˜
        last_update_time = merge_params.get("last_update_time", "1900-01-01")
        if isinstance(last_update_time, datetime):
            last_update_time = last_update_time.strftime("%Y-%m-%d %H:%M:%S")
        merge_sql = merge_sql.replace("LAST_UPDATE_TIME", str(last_update_time))

        # PRIMARY_KEY ì¹˜í™˜ (ê²°ê³¼ í™•ì¸ìš©)
        if isinstance(table_config["primary_key"], list):
            primary_key_str = table_config["primary_key"][0]  # ì²« ë²ˆì§¸ PKë§Œ ì‚¬ìš©
        else:
            primary_key_str = table_config["primary_key"]
        merge_sql = merge_sql.replace("PRIMARY_KEY", primary_key_str)

        logger.info(f"Executing incremental MERGE SQL for {table_config['source']}")

        # MERGE ì‹¤í–‰
        hook.run(merge_sql)

        logger.info(f"Incremental MERGE completed for {table_config['source']}")
        return "Incremental MERGE completed"

    except Exception as e:
        error_msg = f"Incremental MERGE failed for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def validate_data_integrity(table_config: dict, **context) -> dict[str, Any]:
    """
    MERGE í›„ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ (í…Œì´ë¸”ë³„)
    """
    try:
        logger.info(
            f"Starting MERGE data integrity validation for {table_config['source']}"
        )

        source_hook = PostgresHook(postgres_conn_id=SOURCE_CONN_ID)
        target_hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # target_table_nameì„ ì•ˆì „í•˜ê²Œ ìƒì„±
        target_schema, target_table_name = table_config["target"].split(".")

        # MERGE íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        merge_params = context["task_instance"].xcom_pull(
            key=f"merge_parameters_{target_table_name}", task_ids="process_all_tables"
        )

        if not merge_params:
            raise Exception("MERGE parameters not found")

        # 1ë‹¨ê³„: ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì˜ ë ˆì½”ë“œ ìˆ˜ ë¹„êµ
        source_count = source_hook.get_first(
            f"SELECT COUNT(*) FROM {table_config['source']}"
        )[0]
        target_count = target_hook.get_first(
            f"SELECT COUNT(*) FROM {table_config['target']}"
        )[0]

        logger.info(f"Source count: {source_count}, Target count: {target_count}")

        # 2ë‹¨ê³„: MERGE ê²°ê³¼ ê²€ì¦ (PK ê¸°ì¤€)
        primary_key = merge_params["primary_key"]
        incremental_field = merge_params["incremental_field"]

        # incremental_fieldê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ê²€ì¦
        if not incremental_field:
            logger.info(
                f"No incremental field for {table_config['source']}, performing full data validation"
            )
            latest_source_count = source_count
            latest_target_count = target_count
        else:
            # ì†ŒìŠ¤ì—ì„œ ìµœì‹  ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ì¦
            latest_source_sql = f"""
                SELECT COUNT(*) FROM {table_config['source']}
                WHERE {incremental_field} >= %s
            """
            latest_source_count = source_hook.get_first(
                latest_source_sql, parameters=(merge_params["last_update_time"],)
            )[0]

            # íƒ€ê²Ÿì—ì„œ ìµœì‹  ë°ì´í„° ìˆ˜ í™•ì¸
            latest_target_sql = f"""
                SELECT COUNT(*) FROM {table_config['target']}
                WHERE {incremental_field} >= %s
            """
            latest_target_count = target_hook.get_first(
                latest_target_sql, parameters=(merge_params["last_update_time"],)
            )[0]

        logger.info(
            f"Latest source count: {latest_source_count}, Latest target count: {latest_target_count}"
        )

        # 3ë‹¨ê³„: ìƒ˜í”Œ ë°ì´í„° ê²€ì¦ (PK ê¸°ì¤€)
        logger.info("Performing sample data validation by primary key")

        # primary_keyê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(primary_key, list):
            pk_columns = ", ".join(primary_key)
            pk_where_conditions = " AND ".join([f"{pk} = %s" for pk in primary_key])
        else:
            pk_columns = primary_key
            pk_where_conditions = f"{primary_key} = %s"

        # incremental_fieldê°€ ì—†ìœ¼ë©´ PKë§Œìœ¼ë¡œ ê²€ì¦
        if not incremental_field:
            sample_sql = f"""
                SELECT {pk_columns}
                FROM {table_config['source']}
                LIMIT 10
            """
            source_sample = source_hook.get_records(sample_sql)

            # íƒ€ê²Ÿì—ì„œ ë™ì¼í•œ PKë¡œ ê²€ì¦ (ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸)
            for source_row in source_sample:
                pk_values = (
                    source_row[: len(primary_key)]
                    if isinstance(primary_key, list)
                    else [source_row[0]]
                )

                target_sql = f"""
                    SELECT {pk_columns}
                    FROM {table_config['target']}
                    WHERE {pk_where_conditions}
                """
                target_row = target_hook.get_first(target_sql, parameters=pk_values)

                if not target_row:
                    pk_display = (
                        ", ".join(map(str, pk_values))
                        if isinstance(primary_key, list)
                        else str(pk_values[0])
                    )
                    error_msg = f"Data not found in target for PK {pk_display}"
                    logger.error(error_msg)
                    raise Exception(error_msg)
        else:
            # incremental_fieldê°€ ìˆìœ¼ë©´ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê²€ì¦
            sample_sql = f"""
                SELECT {pk_columns}, {incremental_field}
                FROM {table_config['source']}
                ORDER BY {incremental_field} DESC
                LIMIT 10
            """
            source_sample = source_hook.get_records(sample_sql)

            # íƒ€ê²Ÿì—ì„œ ë™ì¼í•œ PKë¡œ ê²€ì¦
            for source_row in source_sample:
                if isinstance(primary_key, list):
                    pk_values = source_row[: len(primary_key)]
                    source_time = source_row[len(primary_key)]
                else:
                    pk_values = [source_row[0]]
                    source_time = source_row[1]

                target_sql = f"""
                    SELECT {pk_columns}, {incremental_field}
                    FROM {table_config['target']}
                    WHERE {pk_where_conditions}
                """
                target_row = target_hook.get_first(target_sql, parameters=pk_values)

                if not target_row or target_row[len(primary_key)] != source_time:
                    pk_display = (
                        ", ".join(map(str, pk_values))
                        if isinstance(primary_key, list)
                        else str(pk_values[0])
                    )
                    error_msg = f"Data mismatch for PK {pk_display}: source={source_time}, target={target_row[len(primary_key)] if target_row else 'None'}"
                    logger.error(error_msg)
                    raise Exception(error_msg)

        logger.info("Sample data validation passed")

        context["task_instance"].xcom_push(
            key=f"validation_status_{target_table_name}", value="passed"
        )

        return {
            "status": "success",
            "source_count": source_count,
            "target_count": target_count,
            "latest_source_count": latest_source_count,
            "latest_target_count": latest_target_count,
            "validation": "passed",
            "sync_mode": table_config["sync_mode"],
        }

    except Exception as e:
        error_msg = f"MERGE data validation failed for {table_config['source']}: {e!s}"
        logger.error(error_msg)

        context["task_instance"].xcom_push(
            key=f"validation_status_{target_table_name}", value="failed"
        )
        raise Exception(error_msg)


def cleanup_temp_table(table_config: dict, **context) -> str:
    """
    ì„ì‹œ í…Œì´ë¸” ì •ë¦¬ ë° MERGE ê²°ê³¼ ìš”ì•½ (í…Œì´ë¸”ë³„)
    """
    try:
        enhanced_config = get_table_config_with_temp(table_config)
        temp_table_full = enhanced_config["temp_table_full"]

        logger.info(f"Cleaning up temporary table: {temp_table_full}")

        hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # target_table_nameì„ ì•ˆì „í•˜ê²Œ ìƒì„±
        target_schema, target_table_name = table_config["target"].split(".")

        # MERGE ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        merge_params = context["task_instance"].xcom_pull(
            key=f"merge_parameters_{target_table_name}", task_ids="process_all_tables"
        )

        # MERGE ê²°ê³¼ ìš”ì•½
        if merge_params:
            logger.info(f"MERGE completed with parameters: {merge_params}")

            # ìµœì¢… ê²°ê³¼ í™•ì¸
            final_count_sql = f"SELECT COUNT(*) FROM {table_config['target']}"
            final_count = hook.get_first(final_count_sql)[0]

            context["task_instance"].xcom_push(
                key=f"final_record_count_{target_table_name}", value=final_count
            )

            logger.info(f"Final record count in target table: {final_count}")

        # ì„ì‹œ í…Œì´ë¸”ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ìŠ¤í‚¤ë§ˆ í¬í•¨)
        temp_exists_sql = """
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = %s AND table_name = %s
            )
        """

        temp_exists = hook.get_first(
            temp_exists_sql,
            parameters=(
                enhanced_config["target_schema"],
                enhanced_config["temp_table"],
            ),
        )[0]

        if temp_exists:
            hook.run(f"DROP TABLE IF EXISTS {temp_table_full}")
            logger.info(f"Temporary table {temp_table_full} dropped")
            return f"Temporary table {temp_table_full} cleaned up. MERGE completed successfully."
        else:
            logger.info("No temporary table to clean up")
            return "No temporary table to clean up. MERGE completed successfully."

    except Exception as e:
        error_msg = f"Failed to cleanup temp table for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def run_dbt_snapshot(table_config: dict, **context) -> dict[str, Any]:
    """
    dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰

    Args:
        table_config: í…Œì´ë¸” ì„¤ì •
        **context: Airflow ì»¨í…ìŠ¤íŠ¸

    Returns:
        ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼
    """
    try:
        logger.info(f"Running dbt snapshot for table: {table_config['source']}")

        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì €ì¥
        original_cwd = os.getcwd()

        try:
            # dbt í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
            if not os.path.exists(DBT_PROJECT_PATH):
                raise Exception(
                    f"dbt project directory does not exist: {DBT_PROJECT_PATH}"
                )

            os.chdir(DBT_PROJECT_PATH)
            logger.info(f"Changed to dbt project directory: {DBT_PROJECT_PATH}")

            # dbtê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            dbt_version_cmd = ["/home/airflow/.local/bin/dbt", "--version"]
            version_result = subprocess.run(
                dbt_version_cmd, capture_output=True, text=True, env=os.environ.copy()
            )

            if version_result.returncode != 0:
                logger.warning(
                    "dbt not found in default location, trying alternative locations"
                )
                # dbtê°€ ê¸°ë³¸ ê²½ë¡œì— ì—†ëŠ” ê²½ìš° ëŒ€ì•ˆ ê²½ë¡œ ì‹œë„
                possible_dbt_paths = [
                    "/usr/local/bin/dbt",
                    "/opt/airflow/.local/bin/dbt",
                    "/home/airflow/.local/bin/dbt",
                    "/home/jskim947/.local/bin/dbt",
                ]

                dbt_found = False
                for dbt_path in possible_dbt_paths:
                    if os.path.exists(dbt_path) and os.access(dbt_path, os.X_OK):
                        logger.info(f"Found executable dbt at: {dbt_path}")
                        dbt_cmd = dbt_path
                        dbt_found = True
                        break

                if not dbt_found:
                    # ë§ˆì§€ë§‰ ì‹œë„: which ëª…ë ¹ì–´ë¡œ dbt ì°¾ê¸°
                    try:
                        which_result = subprocess.run(
                            ["which", "dbt"], capture_output=True, text=True
                        )
                        if which_result.returncode == 0:
                            dbt_cmd = which_result.stdout.strip()
                            logger.info(f"Found dbt using 'which' command: {dbt_cmd}")
                            dbt_found = True
                    except Exception as e:
                        logger.warning(f"Failed to use 'which' command: {e}")

                if not dbt_found:
                    raise Exception("dbt executable not found in any expected location")
            else:
                dbt_cmd = "/home/airflow/.local/bin/dbt"
                logger.info(f"dbt version: {version_result.stdout.strip()}")

            # dbt ì‹¤í–‰ íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œì§€ ìµœì¢… í™•ì¸
            if not os.path.exists(dbt_cmd):
                raise Exception(
                    f"dbt executable not found at specified path: {dbt_cmd}"
                )

            if not os.access(dbt_cmd, os.X_OK):
                raise Exception(f"dbt executable is not executable at path: {dbt_cmd}")

            logger.info(f"Using dbt executable: {dbt_cmd}")

            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            env = os.environ.copy()
            env.update(
                {
                    "DBT_PROFILES_DIR": DBT_PROJECT_PATH,
                    "DBT_PROJECT_DIR": DBT_PROJECT_PATH,
                    "PYTHONPATH": f"{DBT_PROJECT_PATH}:{env.get('PYTHONPATH', '')}",
                }
            )

            logger.info(f"DBT_PROFILES_DIR: {env.get('DBT_PROFILES_DIR')}")
            logger.info(f"DBT_PROJECT_DIR: {env.get('DBT_PROJECT_DIR')}")
            logger.info(f"Current working directory: {os.getcwd()}")
            logger.info(f"dbt project path: {DBT_PROJECT_PATH}")
            logger.info(f"dbt executable: {dbt_cmd}")

            # dbt deps ì‹¤í–‰ (ì˜ì¡´ì„± ì„¤ì¹˜)
            logger.info("Running dbt deps...")
            deps_cmd = [dbt_cmd, "deps"]
            logger.info(f"Executing dbt deps command: {' '.join(deps_cmd)}")
            deps_result = subprocess.run(
                deps_cmd, capture_output=True, text=True, env=env, cwd=DBT_PROJECT_PATH
            )

            if deps_result.returncode != 0:
                logger.warning(f"dbt deps failed: {deps_result.stderr}")
                logger.warning(f"dbt deps stdout: {deps_result.stdout}")
            else:
                logger.info("dbt deps completed successfully")

            # dbt debug ì‹¤í–‰ (í”„ë¡œì íŠ¸ ì„¤ì • í™•ì¸)
            logger.info("Running dbt debug...")
            debug_cmd = [dbt_cmd, "debug"]
            logger.info(f"Executing dbt debug command: {' '.join(debug_cmd)}")
            debug_result = subprocess.run(
                debug_cmd, capture_output=True, text=True, env=env, cwd=DBT_PROJECT_PATH
            )

            if debug_result.returncode != 0:
                logger.warning(f"dbt debug failed: {debug_result.stderr}")
                logger.warning(f"dbt debug output: {debug_result.stdout}")
            else:
                logger.info("dbt debug completed successfully")

            # dbt parse ì‹¤í–‰ (í”„ë¡œì íŠ¸ íŒŒì‹± í…ŒìŠ¤íŠ¸)
            logger.info("Running dbt parse...")
            parse_cmd = [dbt_cmd, "parse"]
            logger.info(f"Executing dbt parse command: {' '.join(parse_cmd)}")
            parse_result = subprocess.run(
                parse_cmd, capture_output=True, text=True, env=env, cwd=DBT_PROJECT_PATH
            )

            if parse_result.returncode != 0:
                logger.error(f"dbt parse failed: {parse_result.stderr}")
                logger.error(f"dbt parse output: {parse_result.stdout}")
                raise Exception(f"dbt project parsing failed: {parse_result.stderr}")
            else:
                logger.info("dbt parse completed successfully")

            # dbt run ì‹¤í–‰ (ëª¨ë¸ ì‹¤í–‰)
            logger.info("Running dbt run...")
            run_cmd = [dbt_cmd, "run"]
            logger.info(f"Executing dbt run command: {' '.join(run_cmd)}")
            run_result = subprocess.run(
                run_cmd, capture_output=True, text=True, env=env, cwd=DBT_PROJECT_PATH
            )

            if run_result.returncode != 0:
                logger.warning(f"dbt run failed: {run_result.stderr}")
                logger.warning(f"dbt run output: {run_result.stdout}")
            else:
                logger.info("dbt run completed successfully")

            # dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
            logger.info("Running dbt snapshot...")
            snapshot_cmd = [dbt_cmd, "snapshot"]
            logger.info(f"Executing dbt snapshot command: {' '.join(snapshot_cmd)}")
            snapshot_result = subprocess.run(
                snapshot_cmd,
                capture_output=True,
                text=True,
                env=env,
                cwd=DBT_PROJECT_PATH,
            )

            if snapshot_result.returncode != 0:
                error_msg = f"dbt snapshot failed: {snapshot_result.stderr}"
                logger.error(error_msg)
                logger.error(f"Command output: {snapshot_result.stdout}")
                logger.error(f"Command executed: {' '.join(snapshot_cmd)}")
                logger.error(f"Working directory: {os.getcwd()}")
                logger.error(f"dbt executable: {dbt_cmd}")
                logger.error(f"dbt project path: {DBT_PROJECT_PATH}")
                logger.error(
                    f"Environment variables: DBT_PROFILES_DIR={env.get('DBT_PROFILES_DIR')}, DBT_PROJECT_DIR={env.get('DBT_PROJECT_DIR')}"
                )

                # ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
                try:
                    # dbt ì‹¤í–‰ íŒŒì¼ ìƒíƒœ í™•ì¸
                    if os.path.exists(dbt_cmd):
                        logger.error(f"dbt executable exists at: {dbt_cmd}")
                        logger.error(
                            f"dbt executable permissions: {oct(os.stat(dbt_cmd).st_mode)[-3:]}"
                        )
                    else:
                        logger.error(f"dbt executable does not exist at: {dbt_cmd}")

                    # dbt í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
                    if os.path.exists(DBT_PROJECT_PATH):
                        logger.error(
                            f"dbt project directory exists at: {DBT_PROJECT_PATH}"
                        )
                        logger.error(
                            f"dbt project directory contents: {os.listdir(DBT_PROJECT_PATH)}"
                        )
                    else:
                        logger.error(
                            f"dbt project directory does not exist at: {DBT_PROJECT_PATH}"
                        )

                except Exception as debug_e:
                    logger.error(f"Error during debug info collection: {debug_e}")

                raise Exception(error_msg)

            logger.info(
                f"dbt snapshot completed successfully: {snapshot_result.stdout}"
            )

            # ìŠ¤ëƒ…ìƒ· ê²°ê³¼ë¥¼ XComì— ì €ì¥
            target_schema, target_table_name = table_config["target"].split(".")
            context["task_instance"].xcom_push(
                key=f"dbt_snapshot_result_{target_table_name}",
                value={
                    "status": "success",
                    "stdout": snapshot_result.stdout,
                    "stderr": snapshot_result.stderr,
                    "return_code": snapshot_result.returncode,
                },
            )

            return {
                "status": "success",
                "stdout": snapshot_result.stdout,
                "stderr": snapshot_result.stderr,
                "return_code": snapshot_result.returncode,
            }

        finally:
            # ì›ë˜ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ë³µì›
            os.chdir(original_cwd)
            logger.info(f"Restored original working directory: {original_cwd}")

    except Exception as e:
        error_msg = f"Failed to run dbt snapshot for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def validate_snapshot_data(table_config: dict, **context) -> dict[str, Any]:
    """
    dbt ìŠ¤ëƒ…ìƒ· ë°ì´í„° ê²€ì¦

    Args:
        table_config: í…Œì´ë¸” ì„¤ì •
        **context: Airflow ì»¨í…ìŠ¤íŠ¸

    Returns:
        ìŠ¤ëƒ…ìƒ· ê²€ì¦ ê²°ê³¼
    """
    try:
        logger.info(f"Validating dbt snapshot data for table: {table_config['source']}")

        target_hook = PostgresHook(postgres_conn_id=TARGET_CONN_ID)

        # ìŠ¤ëƒ…ìƒ· í…Œì´ë¸”ëª… ìƒì„± - í…Œì´ë¸” ì„¤ì •ì—ì„œ ë™ì ìœ¼ë¡œ ìƒì„±
        target_schema, target_table_name = table_config["target"].split(".")
        snapshot_table_name = f"{target_table_name}_snapshot"
        snapshot_table = f"snapshots.{snapshot_table_name}"

        # ìŠ¤ëƒ…ìƒ· í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        snapshot_exists_sql = """
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = 'snapshots'
                AND table_name = %s
            )
        """

        snapshot_exists = target_hook.get_first(
            snapshot_exists_sql, parameters=(snapshot_table_name,)
        )[0]

        if not snapshot_exists:
            raise Exception(f"Snapshot table {snapshot_table} does not exist")

        # ìŠ¤ëƒ…ìƒ· ë°ì´í„° ìˆ˜ í™•ì¸
        snapshot_count = target_hook.get_first(
            f"SELECT COUNT(*) FROM {snapshot_table}"
        )[0]

        # ì›ë³¸ í…Œì´ë¸”ê³¼ ë¹„êµ
        original_count = target_hook.get_first(
            f"SELECT COUNT(*) FROM {table_config['target']}"
        )[0]

        logger.info(
            f"Original table count: {original_count}, Snapshot count: {snapshot_count}"
        )

        # ìŠ¤ëƒ…ìƒ· ì»¬ëŸ¼ í™•ì¸
        snapshot_columns_sql = """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = 'snapshots'
            AND table_name = 'ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„°_snapshot'
            ORDER BY ordinal_position
        """

        snapshot_columns = target_hook.get_records(snapshot_columns_sql)
        snapshot_column_names = [col[0] for col in snapshot_columns]

        # dbt ìŠ¤ëƒ…ìƒ· í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ["dbt_valid_from", "dbt_valid_to", "dbt_scd_id"]
        missing_columns = [
            col for col in required_columns if col not in snapshot_column_names
        ]

        if missing_columns:
            raise Exception(f"Missing required snapshot columns: {missing_columns}")

        # ìŠ¤ëƒ…ìƒ· ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        # 1. ìœ íš¨í•œ ë ˆì½”ë“œ ìˆ˜ í™•ì¸ (dbt_valid_to IS NULL)
        valid_records = target_hook.get_first(
            f"SELECT COUNT(*) FROM {snapshot_table} WHERE dbt_valid_to IS NULL"
        )[0]

        # 2. íˆìŠ¤í† ë¦¬ ë ˆì½”ë“œ ìˆ˜ í™•ì¸ (dbt_valid_to IS NOT NULL)
        history_records = target_hook.get_first(
            f"SELECT COUNT(*) FROM {snapshot_table} WHERE dbt_valid_to IS NOT NULL"
        )[0]

        logger.info(
            f"Valid records: {valid_records}, History records: {history_records}"
        )

        # ê²€ì¦ ê²°ê³¼ë¥¼ XComì— ì €ì¥
        target_schema, target_table_name = table_config["target"].split(".")
        validation_result = {
            "status": "success",
            "snapshot_table": snapshot_table,
            "snapshot_count": snapshot_count,
            "original_count": original_count,
            "valid_records": valid_records,
            "history_records": history_records,
            "snapshot_columns": snapshot_column_names,
        }

        context["task_instance"].xcom_push(
            key=f"snapshot_validation_result_{target_table_name}",
            value=validation_result,
        )

        return validation_result

    except Exception as e:
        error_msg = f"Snapshot validation failed for {table_config['source']}: {e!s}"
        logger.error(error_msg)
        raise Exception(error_msg)


def log_snapshot_results(table_config: dict, **context) -> str:
    """
    dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…

    Args:
        table_config: í…Œì´ë¸” ì„¤ì •
        **context: Airflow ì»¨í…ìŠ¤íŠ¸

    Returns:
        ë¡œê¹… ê²°ê³¼ ë©”ì‹œì§€
    """
    try:
        target_schema, target_table_name = table_config["target"].split(".")

        # dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        snapshot_result = context["task_instance"].xcom_pull(
            key=f"dbt_snapshot_result_{target_table_name}", task_ids="run_dbt_snapshot"
        )

        # ìŠ¤ëƒ…ìƒ· ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        validation_result = context["task_instance"].xcom_pull(
            key=f"snapshot_validation_result_{target_table_name}",
            task_ids="validate_snapshot_data",
        )

        if snapshot_result and validation_result:
            logger.info(
                f"dbt Snapshot completed successfully for {table_config['source']}"
            )
            logger.info(f"Snapshot result: {snapshot_result}")
            logger.info(f"Validation result: {validation_result}")

            return (
                f"dbt Snapshot completed successfully for {table_config['source']}. "
                f"Snapshot table: {validation_result['snapshot_table']}, "
                f"Total records: {validation_result['snapshot_count']}, "
                f"Valid records: {validation_result['valid_records']}, "
                f"History records: {validation_result['history_records']}"
            )
        else:
            error_msg = "Snapshot or validation results not found"
            logger.error(error_msg)
            return error_msg

    except Exception as e:
        error_msg = (
            f"Failed to log snapshot results for {table_config['source']}: {e!s}"
        )
        logger.error(error_msg)
        return error_msg


def process_all_tables(**context) -> str:
    """ëª¨ë“  í…Œì´ë¸”ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
    results = []

    for i, table_config in enumerate(TABLES_CONFIG):
        logger.info(
            f"Processing table {i+1}/{len(TABLES_CONFIG)}: {table_config['source']}"
        )

        try:
            # 1. ì†ŒìŠ¤ ì—°ê²° í™•ì¸
            check_source_connection(table_config, **context)

            # 2. íƒ€ê²Ÿ ì—°ê²° í™•ì¸
            check_target_connection(table_config, **context)

            # 3. íƒ€ê²Ÿ í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            ensure_target_table_exists(table_config, **context)

            # 4. í…Œì´ë¸” êµ¬ì¡° ë¡œê¹… (ë””ë²„ê¹…ìš©)
            log_table_structure(table_config, **context)

            # 5. ë°ì´í„° ì¹´ìš´íŠ¸
            count_result = get_source_data_count(table_config, **context)

            # 6. ì„ì‹œ í…Œì´ë¸” ìƒì„±
            create_temp_table(table_config, **context)

            # 7. ë°ì´í„° ë³µì‚¬
            copy_result = copy_data_with_dynamic_sql(table_config, **context)

            # 8. MERGE íŒŒë¼ë¯¸í„° ì¤€ë¹„
            merge_params = prepare_merge_parameters(table_config, **context)

            # 9. MERGE ì‹¤í–‰ (ë™ê¸°í™” ëª¨ë“œì— ë”°ë¼)
            if table_config["sync_mode"] == "full_sync":
                merge_result = execute_full_sync_merge(
                    table_config, merge_params, **context
                )
            else:
                merge_result = execute_incremental_merge(
                    table_config, merge_params, **context
                )

            # 10. ë°ì´í„° ê²€ì¦
            validation_result = validate_data_integrity(table_config, **context)

            # 11. ì„ì‹œ í…Œì´ë¸” ì •ë¦¬
            cleanup_result = cleanup_temp_table(table_config, **context)

            # 12. dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
            dbt_snapshot_result = run_dbt_snapshot(table_config, **context)

            # 13. dbt ìŠ¤ëƒ…ìƒ· ë°ì´í„° ê²€ì¦
            snapshot_validation_result = validate_snapshot_data(table_config, **context)

            # 14. dbt ìŠ¤ëƒ…ìƒ· ê²°ê³¼ ë¡œê¹…
            dbt_log_result = log_snapshot_results(table_config, **context)

            table_result = {
                "table": table_config["source"],
                "status": "success",
                "records_processed": count_result.get("total_count", 0),
                "sync_mode": table_config["sync_mode"],
                "sync_result": merge_result,
                "dbt_snapshot_result": dbt_snapshot_result,
                "snapshot_validation_result": snapshot_validation_result,
                "dbt_log_result": dbt_log_result,
            }

        except Exception as e:
            logger.error(f"Failed to process table {table_config['source']}: {e!s}")
            table_result = {
                "table": table_config["source"],
                "status": "failed",
                "error": str(e),
            }

        results.append(table_result)

        # ë‹¤ìŒ í…Œì´ë¸” ì²˜ë¦¬ ì „ ì ì‹œ ëŒ€ê¸°
        time.sleep(2)

    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    success_count = len([r for r in results if r["status"] == "success"])
    failed_count = len([r for r in results if r["status"] == "failed"])

    summary = f"Processing completed. Success: {success_count}, Failed: {failed_count}"
    logger.info(summary)

    # XComì— ê²°ê³¼ ì €ì¥
    context["task_instance"].xcom_push(key="processing_results", value=results)

    # ì‹¤íŒ¨í•œ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒí•˜ì—¬ Airflowì—ì„œ ì‹¤íŒ¨ë¡œ í‘œì‹œ
    if failed_count > 0:
        failed_tables = [r["table"] for r in results if r["status"] == "failed"]
        failed_errors = [
            f"{r['table']}: {r['error']}" for r in results if r["status"] == "failed"
        ]
        error_msg = f"Failed to process {failed_count} table(s): {', '.join(failed_tables)}. Errors: {'; '.join(failed_errors)}"
        logger.error(error_msg)
        raise Exception(error_msg)

    return summary


def send_completion_notification(**context) -> str:
    """ì‘ì—… ì™„ë£Œ ì•Œë¦¼ ì „ì†¡ (ì„±ê³µ/ì‹¤íŒ¨ ëª¨ë‘ í¬í•¨)"""
    try:
        # ì²˜ë¦¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        results = context["task_instance"].xcom_pull(
            key="processing_results", task_ids="process_all_tables"
        )

        if not results:
            return "No processing results found"

        # ì„±ê³µí•œ í…Œì´ë¸”ë“¤
        success_tables = [r for r in results if r["status"] == "success"]
        failed_tables = [r for r in results if r["status"] == "failed"]

        # dbt ìŠ¤ëƒ…ìƒ· ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        target_schema, target_table_name = TABLES_CONFIG[0]["target"].split(".")
        snapshot_result = context["task_instance"].xcom_pull(
            key=f"dbt_snapshot_result_{target_table_name}", task_ids="run_dbt_snapshot"
        )
        snapshot_validation = context["task_instance"].xcom_pull(
            key=f"snapshot_validation_result_{target_table_name}",
            task_ids="validate_snapshot_data",
        )

        # ì•Œë¦¼ ë©”ì‹œì§€ êµ¬ì„±
        message = f"""
        PostgreSQL Multi-Table Copy DAG ì‹¤í–‰ ì™„ë£Œ

        ì´ ì²˜ë¦¬ í…Œì´ë¸”: {len(results)}ê°œ
        ì„±ê³µ: {len(success_tables)}ê°œ
        ì‹¤íŒ¨: {len(failed_tables)}ê°œ

        ì„±ê³µí•œ í…Œì´ë¸”:
        {chr(10).join([f"- {r['table']} ({r['sync_mode']}): {r['records_processed']}ê±´" for r in success_tables])}
        """

        if failed_tables:
            message += f"""

        ì‹¤íŒ¨í•œ í…Œì´ë¸”:
        {chr(10).join([f"- {r['table']}: {r['error']}" for r in failed_tables])}
        """

        # dbt ìŠ¤ëƒ…ìƒ· ê²°ê³¼ ì¶”ê°€
        if snapshot_result and snapshot_validation:
            message += f"""

        dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼:
        - ìŠ¤ëƒ…ìƒ· í…Œì´ë¸”: {snapshot_validation['snapshot_table']}
        - ì´ ë ˆì½”ë“œ ìˆ˜: {snapshot_validation['snapshot_count']}ê±´
        - ìœ íš¨ ë ˆì½”ë“œ: {snapshot_validation['valid_records']}ê±´
        - íˆìŠ¤í† ë¦¬ ë ˆì½”ë“œ: {snapshot_validation['history_records']}ê±´
        - ìƒíƒœ: ì„±ê³µ
        """
        else:
            message += """

        dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼:
        - ìƒíƒœ: ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ
        """

        logger.info(message)

        # ì‹¤íŒ¨í•œ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ê²½ê³  ë¡œê·¸ë¡œ í‘œì‹œ
        if failed_tables:
            logger.warning(
                f"Some tables failed to process: {', '.join([r['table'] for r in failed_tables])}"
            )

        return message

    except Exception as e:
        error_msg = f"Failed to send success notification: {e!s}"
        logger.error(error_msg)
        return error_msg


# DAG íƒœìŠ¤í¬ ì •ì˜

# ë‹¨ì¼ íƒœìŠ¤í¬ë¡œ ëª¨ë“  í…Œì´ë¸” ì²˜ë¦¬
process_all_tables_task = PythonOperator(
    task_id="process_all_tables",
    python_callable=process_all_tables,
    dag=dag,
    doc_md="""
    ëª¨ë“  í…Œì´ë¸”ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    - ì†ŒìŠ¤/íƒ€ê²Ÿ ì—°ê²° í™•ì¸
    - ë°ì´í„° ë³µì‚¬
    - ë™ê¸°í™” ëª¨ë“œì— ë”°ë¥¸ MERGE ì‹¤í–‰
    - ë°ì´í„° ê²€ì¦
    - ì„ì‹œ í…Œì´ë¸” ì •ë¦¬
    - dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰
    - ìŠ¤ëƒ…ìƒ· ë°ì´í„° ê²€ì¦
    """,
)

# dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ íƒœìŠ¤í¬ (ì „ì²´ í…Œì´ë¸” ì²˜ë¦¬ í›„)
run_dbt_snapshot_task = PythonOperator(
    task_id="run_dbt_snapshot",
    python_callable=run_dbt_snapshot,
    op_kwargs={"table_config": TABLES_CONFIG[0]},  # ì²« ë²ˆì§¸ í…Œì´ë¸”ì— ëŒ€í•´ì„œë§Œ
    dag=dag,
    doc_md="""
    dbt ìŠ¤ëƒ…ìƒ·ì„ ì‹¤í–‰í•˜ì—¬ ë°ì´í„° ë³€í™”ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.

    - ì¸í¬ë§¥ìŠ¤ì¢…ëª©ë§ˆìŠ¤í„° í…Œì´ë¸”ì˜ ë³€ê²½ì‚¬í•­ ìº¡ì²˜
    - SCD2 íŒ¨í„´ìœ¼ë¡œ ë°ì´í„° íˆìŠ¤í† ë¦¬ ë³´ì¡´
    - ë³€ê²½ëœ ë ˆì½”ë“œì˜ ìœ íš¨ê¸°ê°„ ê´€ë¦¬
    """,
)

# dbt ìŠ¤ëƒ…ìƒ· ë°ì´í„° ê²€ì¦ íƒœìŠ¤í¬
validate_snapshot_task = PythonOperator(
    task_id="validate_snapshot_data",
    python_callable=validate_snapshot_data,
    op_kwargs={"table_config": TABLES_CONFIG[0]},  # ì²« ë²ˆì§¸ í…Œì´ë¸”ì— ëŒ€í•´ì„œë§Œ
    dag=dag,
    doc_md="""
    dbt ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì˜ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.

    - ìŠ¤ëƒ…ìƒ· í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    - í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    - ë°ì´í„° ìˆ˜ëŸ‰ ê²€ì¦
    - ìœ íš¨/íˆìŠ¤í† ë¦¬ ë ˆì½”ë“œ ë¶„ë¥˜
    """,
)

# dbt ìŠ¤ëƒ…ìƒ· ê²°ê³¼ ë¡œê¹… íƒœìŠ¤í¬
log_snapshot_task = PythonOperator(
    task_id="log_snapshot_results",
    python_callable=log_snapshot_results,
    op_kwargs={"table_config": TABLES_CONFIG[0]},  # ì²« ë²ˆì§¸ í…Œì´ë¸”ì— ëŒ€í•´ì„œë§Œ
    dag=dag,
    doc_md="""
    dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.

    - ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
    - ê²€ì¦ ê²°ê³¼ ìš”ì•½
    - ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
    """,
)

# ì™„ë£Œ ì•Œë¦¼
completion_notification_task = PythonOperator(
    task_id="send_completion_notification",
    python_callable=send_completion_notification,
    dag=dag,
    doc_md="""
    ì‘ì—… ì™„ë£Œ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.

    - ì²˜ë¦¬ëœ í…Œì´ë¸” ìˆ˜ ìš”ì•½
    - ì„±ê³µ/ì‹¤íŒ¨ í˜„í™©
    - ê° í…Œì´ë¸”ë³„ ì²˜ë¦¬ ê²°ê³¼
    - dbt ìŠ¤ëƒ…ìƒ· ì‹¤í–‰ ê²°ê³¼
    """,
)

# Task ì˜ì¡´ì„± ì„¤ì • (dbt ìŠ¤ëƒ…ìƒ· í¬í•¨)
(
    process_all_tables_task
    >> run_dbt_snapshot_task
    >> validate_snapshot_task
    >> log_snapshot_task
    >> completion_notification_task
)
