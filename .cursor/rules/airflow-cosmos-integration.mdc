# Airflow + Astronomer Cosmos 통합 가이드

## Cosmos DAG 구조 및 설정

### 기본 Cosmos DAG 설정
[dbt_processing_dag.py](mdc:airflow/dags/dbt_processing_dag.py)에서 Cosmos DAG 생성:

```python
from cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig

# 프로젝트 설정
project_config = ProjectConfig(
    dbt_project_path="/opt/airflow/dbt",
    dbt_manifest_path="/opt/airflow/dbt/target/manifest.json"
)

# 프로필 설정
profile_config = ProfileConfig(
    profile_name="postgres_data_copy",
    target_name="dev",
    profiles_yml_path="/opt/airflow/dbt/profiles.yml"
)

# 실행 설정
execution_config = ExecutionConfig(
    dbt_executable_path="/home/airflow/.local/bin/dbt"
)

# Cosmos DAG 생성
dbt_snapshot_dag = DbtDag(
    project_config=project_config,
    profile_config=profile_config,
    execution_config=execution_config,
    dag_id="dbt_snapshot_dag",
    schedule_interval="@daily"
)
```

## Airflow 설정 최적화

### DagBag Import Timeout 설정
`airflow.cfg` 또는 환경 변수에서 설정:

```ini
[core]
dagbag_import_timeout = 300  # 기본값 30초에서 증가
```

또는 환경 변수:
```bash
AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
```

### DAG 로딩 성능 최적화
```ini
[core]
dag_file_processor_timeout = 600
dagbag_import_timeout = 300
dagbag_import_timeout_seconds = 300
```

## PostgreSQL 데이터 복사 DAG

### 메인 DAG 구조
[postgres_data_copy_dag.py](mdc:airflow/dags/postgres_data_copy_dag.py)의 주요 구성:

```python
# 연결 ID 설정
SOURCE_CONN_ID = "fs2_postgres"      # 소스 데이터베이스
TARGET_CONN_ID = "postgres_default"   # 타겟 데이터베이스

# 테이블 설정
TABLES_CONFIG = [
    {
        'source': "fds_팩셋.인포맥스종목마스터",
        'target': "raw_data.인포맥스종목마스터",
        'primary_key': ['인포맥스코드','팩셋거래소','gts_exnm','티커'],
        'sync_mode': 'full_sync',
        'batch_size': 10000
    }
]
```

### DBT 스냅샷 실행 함수
```python
def run_dbt_snapshot(table_config: dict, **context) -> Dict[str, Any]:
    """
    dbt 스냅샷 실행

    실행 순서:
    1. dbt deps - 패키지 의존성 설치
    2. dbt parse - 프로젝트 파싱
    3. dbt run - 모델 실행
    4. dbt snapshot - 스냅샷 생성
    """
```

## 데이터베이스 연결 설정

### 소스 데이터베이스 (fs2_postgres)
- **Host**: 190.1.100.102
- **Port**: 5432
- **Database**: infomax
- **Schema**: fds_팩셋

### 타겟 데이터베이스 (postgres_default)
- **Host**: 10.150.2.150
- **Port**: 15432
- **Database**: airflow
- **Schema**: raw_data

### 스냅샷 스키마
- **Schema**: snapshots
- **테이블**: 인포맥스종목마스터_snapshot

## DAG 실행 순서 및 의존성

### 1단계: 데이터 복사
```python
# PostgreSQL 데이터 복사 작업
process_all_tables_task = PythonOperator(
    task_id='process_all_tables',
    python_callable=process_all_tables,
    provide_context=True
)
```

### 2단계: DBT 스냅샷
```python
# dbt 스냅샷 실행
run_dbt_snapshot_task = PythonOperator(
    task_id='run_dbt_snapshot',
    python_callable=run_dbt_snapshot,
    provide_context=True
)
```

### 3단계: 검증 및 완료
```python
# 데이터 무결성 검증
validate_snapshot_task = PythonOperator(
    task_id='validate_snapshot',
    python_callable=validate_snapshot,
    provide_context=True
)

# 완료 알림
completion_notification_task = PythonOperator(
    task_id='completion_notification',
    python_callable=completion_notification,
    provide_context=True
)
```

## 오류 처리 및 로깅

### 일반적인 오류 유형
1. **Connection Error**: 데이터베이스 연결 실패
2. **Parsing Error**: dbt 프로젝트 파싱 실패
3. **Import Timeout**: DAG 로딩 시간 초과
4. **Data Validation Error**: 데이터 무결성 검증 실패

### 오류 처리 전략
```python
try:
    # dbt 명령어 실행
    result = subprocess.run(
        dbt_cmd,
        capture_output=True,
        text=True,
        env=env,
        cwd=DBT_PROJECT_PATH
    )

    if result.returncode != 0:
        error_msg = f"dbt command failed: {result.stderr}"
        logger.error(error_msg)
        raise Exception(error_msg)

except Exception as e:
    logger.error(f"Unexpected error: {str(e)}")
    raise
```

## 성능 최적화

### 배치 처리 설정
```python
TABLES_CONFIG = [
    {
        'batch_size': 10000,  # 배치 크기 조정
        'sync_mode': 'full_sync',  # 또는 'incremental_sync'
        'custom_where': '시가총액 > 1000000000'  # 조건부 필터링
    }
]
```

### 병렬 처리 고려사항
```python
# max_active_runs 설정
dag = DAG(
    "postgres_multi_table_copy",
    max_active_runs=1,  # 동시 실행 제한
    concurrency=2       # 태스크 동시성
)
```

## 모니터링 및 알림

### XCom을 통한 결과 공유
```python
# DBT 실행 결과를 XCom에 저장
context['task_instance'].xcom_push(
    key=f'dbt_snapshot_result_{target_table_name}',
    value={
        'status': 'success',
        'stdout': result.stdout,
        'stderr': result.stderr,
        'return_code': result.returncode
    }
)
```

### 이메일 알림 설정
```python
default_args = {
    "email_on_failure": True,
    "email_on_retry": False,
    "email": ["admin@example.com"],
    "retries": 2,
    "retry_delay": timedelta(minutes=5)
}
```

## 배포 및 운영

### Docker 환경 설정
```yaml
# docker-compose.yml
services:
  airflow:
    environment:
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT=600
    volumes:
      - ./airflow/dbt:/opt/airflow/dbt
      - ./airflow/dags:/opt/airflow/dags
```

### 환경별 설정 분리
```python
# 환경별 설정
ENVIRONMENT = os.getenv('ENVIRONMENT', 'dev')

if ENVIRONMENT == 'prod':
    DBT_PROJECT_PATH = "/opt/airflow/dbt"
    EMAIL_RECIPIENTS = ["prod-team@company.com"]
else:
    DBT_PROJECT_PATH = "/opt/airflow/dbt"
    EMAIL_RECIPIENTS = ["dev-team@company.com"]
```

## 문제 해결 체크리스트

### DAG 로딩 문제
- [ ] `core.dagbag_import_timeout` 설정 확인
- [ ] dbt 프로젝트 경로 정확성 확인
- [ ] 필요한 패키지 설치 상태 확인

### DBT 실행 문제
- [ ] `dbt deps` 실행 성공 여부
- [ ] `dbt parse` 파싱 오류 확인
- [ ] `profiles.yml` 연결 정보 검증

### 데이터 복사 문제
- [ ] 소스/타겟 데이터베이스 연결 상태
- [ ] 테이블 권한 확인
- [ ] 네트워크 방화벽 설정 확인
description:
globs:
alwaysApply: false
---
