# Data Copy Engine ì²­í¬ ë°©ì‹ ì „í™˜ ê³„íšì„œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### **ëª©í‘œ**
ê¸°ì¡´ì˜ ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì‹ì—ì„œ ì§„ì§œ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ `data_copy_engine.py`ë¥¼ ì „í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëŒ€í­ ì¤„ì´ê³  ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ì²˜ë¦¬ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.

### **í˜„ì¬ ë¬¸ì œì **
- ë©”ëª¨ë¦¬ì— ì „ì²´ ë°ì´í„° ëˆ„ì  í›„ í•œ ë²ˆì— CSV ì €ì¥
- 596ë§Œ í–‰ í…Œì´ë¸” ì²˜ë¦¬ ì‹œ ì•½ 400-600MB ë©”ëª¨ë¦¬ ì‚¬ìš©
- PostgreSQL MVCC ì¶©ëŒë¡œ ì¸í•œ ì•ˆì •ì„± ë¬¸ì œ
- ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì§€ì† ì¦ê°€
- **ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ ì—°ê²° ëŠê¹€ ìœ„í—˜**
- **ì²­í¬ ì‹¤íŒ¨ ì‹œ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥ ë¶€ì¡±**

### **ê°œì„  ëª©í‘œ**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 90% ì´ìƒ ê°ì†Œ (500MB â†’ 50MB ì´í•˜)
- ì²˜ë¦¬ ì•ˆì •ì„±: PostgreSQL ì¶©ëŒ 95% ê°ì†Œ
- í™•ì¥ì„±: í…Œì´ë¸” í¬ê¸°ì— ê´€ê³„ì—†ì´ ì¼ì •í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©
- **ì„¸ì…˜ ì•ˆì •ì„±: 99% ì´ìƒ ì—°ê²° ìœ ì§€**
- **ë°ì´í„° ë¬´ê²°ì„±: 100% ë³´ì¥**

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ë³€ê²½ ê³„íš

### **ê¸°ì¡´ êµ¬ì¡°**
```
ì†ŒìŠ¤ DB â†’ ë°°ì¹˜ë³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° â†’ ë©”ëª¨ë¦¬ ëˆ„ì  â†’ DataFrame ë³€í™˜ â†’ CSV ì €ì¥ â†’ íƒ€ê²Ÿ DB
```

### **ìƒˆë¡œìš´ êµ¬ì¡°**
```
ì†ŒìŠ¤ DB â†’ ë°°ì¹˜ë³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° â†’ ì¦‰ì‹œ CSV ì¶”ê°€ â†’ ë©”ëª¨ë¦¬ í•´ì œ â†’ íƒ€ê²Ÿ DB
```

### **ê°œì„ ëœ ì•ˆì „ êµ¬ì¡°**
```
ì†ŒìŠ¤ DB â†’ ì„¸ì…˜ ê´€ë¦¬ â†’ íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬ â†’ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ â†’ CSV ì¶”ê°€ â†’ ë©”ëª¨ë¦¬ í•´ì œ â†’ íƒ€ê²Ÿ DB
```

## ğŸ”§ êµ¬í˜„ ì„¸ë¶€ ê³„íš

### **1ë‹¨ê³„: export_to_csv ë©”ì„œë“œ ì „ë©´ ì¬ì‘ì„±**

#### **1.1 ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½**
```python
def export_to_csv(
    self,
    table_name: str,
    csv_path: str,
    where_clause: str | None = None,
    batch_size: int = 10000,
    order_by_field: str | None = None,
    chunk_mode: bool = True,  # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°
    enable_checkpoint: bool = True,  # ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”
    max_retries: int = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
) -> int:
```

#### **1.2 ê°œì„ ëœ ì²­í¬ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„**
```python
def _export_to_csv_chunked(self, table_name, csv_path, where_clause, batch_size, order_by_field, enable_checkpoint=True, max_retries=3):
    """
    ì„¸ì…˜ ê´€ë¦¬ì™€ íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ì  ì—†ìŒ)
    """
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬ ì‹œë„
    start_offset, total_exported = 0, 0
    if enable_checkpoint:
        start_offset, total_exported = self._resume_from_checkpoint(table_name, csv_path)
    
    session_refresh_interval = 50  # 50ê°œ ì²­í¬ë§ˆë‹¤ ì„¸ì…˜ ê°±ì‹ 
    chunk_count = 0
    
    with open(csv_path, 'a' if start_offset > 0 else 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # í—¤ë”ëŠ” ì²˜ìŒì—ë§Œ ì“°ê¸°
        if start_offset == 0:
            columns = self._get_table_columns(table_name)
            writer.writerow(columns)
        
        offset = start_offset
        
        while offset < total_count:
            try:
                # ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ê°±ì‹ 
                if chunk_count % session_refresh_interval == 0:
                    self._refresh_database_session()
                    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ê°±ì‹  (ì²­í¬ {chunk_count})")
                
                # íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬
                batch_result = self._process_single_chunk_with_transaction(
                    table_name, where_clause, batch_size, offset, order_by_field, writer, max_retries
                )
                
                if batch_result['success']:
                    total_exported += batch_result['rows_processed']
                    offset += batch_size
                    chunk_count += 1
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                    if enable_checkpoint:
                        self._save_checkpoint(table_name, offset, total_exported, csv_path)
                    
                    # ì§„í–‰ë¥  ë¡œê¹…
                    logger.info(f"ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ : {total_exported}/{total_count}")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                    self._check_memory_usage()
                else:
                    # ì²­í¬ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬
                    error_action = self._handle_chunk_error(batch_result['error'], offset, batch_size)
                    if error_action == 'skip':
                        offset += batch_size
                        logger.warning(f"ì²­í¬ ê±´ë„ˆë›°ê¸° (offset {offset})")
                    elif error_action == 'retry':
                        continue  # ì¬ì‹œë„
                    else:
                        raise Exception(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {batch_result['error']}")
                
            except (psycopg2.OperationalError, psycopg2.InterfaceError) as db_error:
                # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ ì‹œ ì„¸ì…˜ ì¬ìƒì„±
                logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {db_error}")
                self._refresh_database_session()
                
                # ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„
                continue
                
            except Exception as chunk_error:
                # ê¸°íƒ€ ì˜¤ë¥˜ ì‹œ ë¡œê¹… ë° ì¬ì‹œë„
                logger.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜ (offset {offset}): {chunk_error}")
                error_action = self._handle_chunk_error(chunk_error, offset, batch_size)
                if error_action == 'skip':
                    offset += batch_size
                elif error_action == 'retry':
                    continue
                else:
                    raise chunk_error
    
    # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
    if enable_checkpoint:
        self._cleanup_checkpoint(csv_path)
    
    return total_exported
```

### **2ë‹¨ê³„: ì„¸ì…˜ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**

#### **2.1 ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ê°±ì‹ **
```python
def _refresh_database_session(self):
    """
    ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ê°±ì‹ 
    """
    try:
        # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ í™•ì¸
        if hasattr(self, 'db_ops') and hasattr(self.db_ops, 'connection'):
            # ì—°ê²° ìƒíƒœ í™•ì¸
            if self.db_ops.connection.closed:
                logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ëŠì–´ì§, ì¬ì—°ê²° ì‹œë„")
                self.db_ops.reconnect()
            else:
                # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ì„¸ì…˜ ìƒíƒœ í™•ì¸
                self.db_ops.execute_query("SELECT 1")
                
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        # ê°•ì œ ì¬ì—°ê²°
        self.db_ops.reconnect()

def _check_session_health(self):
    """
    ì„¸ì…˜ ìƒíƒœ ì¢…í•© ì ê²€
    """
    try:
        # ì—°ê²° ìƒíƒœ í™•ì¸
        if self.db_ops.connection.closed:
            return False
        
        # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ í™•ì¸
        cursor = self.db_ops.connection.cursor()
        cursor.execute("SELECT current_timestamp - query_start FROM pg_stat_activity WHERE pid = pg_backend_pid()")
        result = cursor.fetchone()
        
        if result and result[0]:
            # 30ë¶„ ì´ìƒ ì‹¤í–‰ ì¤‘ì¸ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ê²½ê³ 
            if result[0].total_seconds() > 1800:
                logger.warning("ì„¸ì…˜ì— ì¥ì‹œê°„ ì‹¤í–‰ ì¤‘ì¸ ì¿¼ë¦¬ ê°ì§€")
                return False
        
        cursor.close()
        return True
        
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ìƒíƒœ ì ê²€ ì‹¤íŒ¨: {e}")
        return False
```

#### **2.2 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ ë©”ì„œë“œ**
```python
def _check_memory_usage(self):
    """
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬
    """
    import psutil
    
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    
    # ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì„¤ì •
    WARNING_THRESHOLD = 100  # 100MB
    CRITICAL_THRESHOLD = 200  # 200MB
    
    if memory_mb > CRITICAL_THRESHOLD:
        logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìœ„í—˜: {memory_mb:.1f}MB")
        self._force_memory_cleanup()
    elif memory_mb > WARNING_THRESHOLD:
        logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB")
        gc.collect()
    
    return memory_mb
```

#### **2.3 ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ë©”ì„œë“œ**
```python
def _force_memory_cleanup(self):
    """
    ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
    """
    import gc
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    gc.collect()
    
    # Python ê°ì²´ ì°¸ì¡° ì •ë¦¬
    for obj in gc.get_objects():
        if hasattr(obj, '__dict__'):
            obj.__dict__.clear()
    
    logger.info("ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
```

### **3ë‹¨ê³„: íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ**

#### **3.1 íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬**
```python
def _process_single_chunk_with_transaction(self, table_name, where_clause, batch_size, offset, order_by_field, writer, max_retries=3):
    """
    íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬
    """
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # íŠ¸ëœì­ì…˜ ì‹œì‘
            with self.db_ops.connection.cursor() as cursor:
                # ì²­í¬ ë°ì´í„° ì¡°íšŒ
                query = self._build_chunk_query(table_name, where_clause, batch_size, offset, order_by_field)
                cursor.execute(query)
                
                rows_processed = 0
                for row in cursor:
                    writer.writerow(row)
                    rows_processed += 1
                
                # íŠ¸ëœì­ì…˜ ì»¤ë°‹
                self.db_ops.connection.commit()
                
                return {
                    'rows_processed': rows_processed,
                    'success': True,
                    'error': None
                }
                
        except Exception as e:
            # íŠ¸ëœì­ì…˜ ë¡¤ë°±
            self.db_ops.connection.rollback()
            retry_count += 1
            
            if retry_count >= max_retries:
                logger.error(f"ì²­í¬ ì²˜ë¦¬ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}")
                return {
                    'rows_processed': 0,
                    'success': False,
                    'error': str(e)
                }
            
            logger.warning(f"ì²­í¬ ì²˜ë¦¬ ì¬ì‹œë„ {retry_count}/{max_retries}: {e}")
            time.sleep(2 ** retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„
```

#### **3.2 ì„¸ë¶„í™”ëœ ì˜¤ë¥˜ ë¶„ë¥˜ ë° ì²˜ë¦¬**
```python
def _handle_chunk_error(self, error, offset, batch_size):
    """
    ì²­í¬ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
    """
    error_type = type(error).__name__
    
    if isinstance(error, (psycopg2.OperationalError, psycopg2.InterfaceError)):
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ - ì¬ì‹œë„ ê°€ëŠ¥
        logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ì¬ì‹œë„ ì˜ˆì •")
        return 'retry'
        
    elif isinstance(error, (psycopg2.DataError, psycopg2.IntegrityError)):
        # ë°ì´í„° ì˜¤ë¥˜ - ì¬ì‹œë„ ë¶ˆê°€ëŠ¥
        logger.error(f"ë°ì´í„° ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ê±´ë„ˆë›°ê¸°")
        return 'skip'
        
    elif isinstance(error, psycopg2.InternalError):
        # ë‚´ë¶€ ì˜¤ë¥˜ - ì¬ì‹œë„ ê°€ëŠ¥
        logger.warning(f"ë‚´ë¶€ ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}), ì¬ì‹œë„ ì˜ˆì •")
        return 'retry'
        
    else:
        # ê¸°íƒ€ ì˜¤ë¥˜ - ë¡œê¹… í›„ ì¬ì‹œë„
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ì¸í•œ ì²­í¬ ì‹¤íŒ¨ (offset {offset}): {error}")
        return 'retry'
```

### **4ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ë° ë³µêµ¬ ì‹œìŠ¤í…œ**

#### **4.1 ì§„í–‰ ìƒí™© ì €ì¥**
```python
def _save_checkpoint(self, table_name, offset, total_exported, csv_path):
    """
    ì²­í¬ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì €ì¥
    """
    checkpoint_data = {
        'table_name': table_name,
        'offset': offset,
        'total_exported': total_exported,
        'csv_path': csv_path,
        'timestamp': pd.Timestamp.now().isoformat(),
        'status': 'in_progress',
        'checksum': self._calculate_csv_checksum(csv_path)
    }
    
    checkpoint_file = f"{csv_path}.checkpoint"
    with open(checkpoint_file, 'w') as f:
        json.dump(checkpoint_data, f, indent=2)
    
    logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: offset {offset}, ì²˜ë¦¬ëœ í–‰ {total_exported}")
```

#### **4.2 ë³µêµ¬ ê¸°ëŠ¥**
```python
def _resume_from_checkpoint(self, table_name, csv_path):
    """
    ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬
    """
    checkpoint_file = f"{csv_path}.checkpoint"
    
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r') as f:
                checkpoint_data = json.load(f)
            
            if checkpoint_data['status'] == 'in_progress':
                # ì²´í¬ì„¬ ê²€ì¦
                if self._verify_checkpoint_integrity(checkpoint_data, csv_path):
                    logger.info(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬: {checkpoint_data['offset']}ë¶€í„° ì‹œì‘")
                    return checkpoint_data['offset'], checkpoint_data['total_exported']
                else:
                    logger.warning("ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨, ì²˜ìŒë¶€í„° ì‹œì‘")
                    return 0, 0
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì½ê¸° ì‹¤íŒ¨: {e}")
            return 0, 0
    
    return 0, 0

def _verify_checkpoint_integrity(self, checkpoint_data, csv_path):
    """
    ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦
    """
    try:
        if not os.path.exists(csv_path):
            return False
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(csv_path)
        if file_size == 0:
            return False
        
        # ì²´í¬ì„¬ ê²€ì¦
        current_checksum = self._calculate_csv_checksum(csv_path)
        if current_checksum != checkpoint_data.get('checksum'):
            logger.warning("ì²´í¬ì„¬ ë¶ˆì¼ì¹˜, ì²´í¬í¬ì¸íŠ¸ ë¬´íš¨í™”")
            return False
        
        return True
        
    except Exception as e:
        logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def _calculate_csv_checksum(self, csv_path):
    """
    CSV íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°
    """
    import hashlib
    
    try:
        with open(csv_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception:
        return None

def _cleanup_checkpoint(self, csv_path):
    """
    ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì •ë¦¬
    """
    checkpoint_file = f"{csv_path}.checkpoint"
    if os.path.exists(checkpoint_file):
        try:
            # ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
            with open(checkpoint_file, 'r') as f:
                checkpoint_data = json.load(f)
            
            checkpoint_data['status'] = 'completed'
            checkpoint_data['completed_at'] = pd.Timestamp.now().isoformat()
            
            with open(checkpoint_file, 'w') as f:
                json.dump(checkpoint_data, f, indent=2)
            
            # ì¼ì • ì‹œê°„ í›„ ì‚­ì œ (ì„ íƒì‚¬í•­)
            # os.remove(checkpoint_file)
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
```

### **5ë‹¨ê³„: ì„±ëŠ¥ ìµœì í™”**

#### **5.1 ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •**
```python
def _optimize_batch_size_dynamically(self, initial_batch_size, memory_usage, processing_time, session_health):
    """
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì²˜ë¦¬ ì‹œê°„, ì„¸ì…˜ ìƒíƒœì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
    """
    if not session_health:
        # ì„¸ì…˜ ìƒíƒœ ë¶ˆëŸ‰ ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        new_batch_size = max(initial_batch_size // 4, 100)
        logger.info(f"ì„¸ì…˜ ìƒíƒœ ë¶ˆëŸ‰, ë°°ì¹˜ í¬ê¸° ì¡°ì •: {initial_batch_size} â†’ {new_batch_size}")
        return new_batch_size
    
    elif memory_usage > 150:  # 150MB ì´ˆê³¼
        new_batch_size = max(initial_batch_size // 2, 100)
        logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ, ë°°ì¹˜ í¬ê¸° ì¡°ì •: {initial_batch_size} â†’ {new_batch_size}")
        return new_batch_size
    
    elif processing_time > 30:  # 30ì´ˆ ì´ˆê³¼
        new_batch_size = max(initial_batch_size // 2, 100)
        logger.info(f"ì²˜ë¦¬ ì‹œê°„ ê¸¸ìŒ, ë°°ì¹˜ í¬ê¸° ì¡°ì •: {initial_batch_size} â†’ {new_batch_size}")
        return new_batch_size
    
    elif memory_usage < 50 and processing_time < 10 and session_health:  # ì—¬ìœ ë¡œìš´ ìƒí™©
        new_batch_size = min(initial_batch_size * 2, 2000)
        logger.info(f"ì—¬ìœ ë¡œìš´ ìƒí™©, ë°°ì¹˜ í¬ê¸° ì¦ê°€: {initial_batch_size} â†’ {new_batch_size}")
        return new_batch_size
    
    return initial_batch_size
```

#### **5.2 ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (ì„ íƒì )**
```python
def _export_to_csv_parallel(self, table_name, csv_path, where_clause, batch_size, order_by_field, num_workers=2):
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì„±ëŠ¥ í–¥ìƒ (ì„ íƒì )
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    # ì²­í¬ ë²”ìœ„ ê³„ì‚°
    total_count = self.db_ops.get_table_row_count(table_name, where_clause)
    chunk_ranges = self._calculate_chunk_ranges(total_count, batch_size, num_workers)
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = []
        
        for chunk_start, chunk_end in chunk_ranges:
            future = executor.submit(
                self._process_chunk_range,
                table_name, where_clause, chunk_start, chunk_end, order_by_field
            )
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘ ë° CSV ë³‘í•©
        all_chunks = []
        for future in as_completed(futures):
            chunk_data = future.result()
            all_chunks.append(chunk_data)
        
        # ì²­í¬ë³„ë¡œ CSVì— ìˆœì°¨ì ìœ¼ë¡œ ì“°ê¸°
        self._merge_chunks_to_csv(all_chunks, csv_path)
```

## ğŸ“Š êµ¬í˜„ ìš°ì„ ìˆœìœ„

### **Phase 1: í•µì‹¬ ì•ˆì „ì„± ê¸°ëŠ¥ (1-2ì£¼)**
- [ ] ì„¸ì…˜ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬
- [ ] ì²´í¬í¬ì¸íŠ¸ ë° ë³µêµ¬ ì‹œìŠ¤í…œ

### **Phase 2: ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” (1-2ì£¼)**
- [ ] ì„¸ë¶„í™”ëœ ì˜¤ë¥˜ ë¶„ë¥˜ ë° ì²˜ë¦¬
- [ ] ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- [ ] ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦

### **Phase 3: ì„±ëŠ¥ ìµœì í™” (1ì£¼)**
- [ ] ì„¸ì…˜ í’€ë§ ìµœì í™”
- [ ] ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### **ì„¤ê³„ ë° ê³„íš**
- [ ] í˜„ì¬ ì½”ë“œ ë¶„ì„ ë° ì˜í–¥ë„ í‰ê°€
- [ ] ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì„¤ê³„ ê²€í† 
- [ ] í…ŒìŠ¤íŠ¸ ê³„íš ìˆ˜ë¦½
- [ ] ë¡¤ë°± ê³„íš ìˆ˜ë¦½

### **ê°œë°œ í™˜ê²½ ì¤€ë¹„**
- [ ] ê°œë°œ ë¸Œëœì¹˜ ìƒì„±
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (ëŒ€ìš©ëŸ‰ í…Œì´ë¸”)
- [ ] ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë„êµ¬ ì„¤ì •
- [ ] ì„±ëŠ¥ ì¸¡ì • ë„êµ¬ ì„¤ì •
- [ ] **ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ë„êµ¬ ì„¤ì •**

### **í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„**
- [ ] `_export_to_csv_chunked` ë©”ì„œë“œ êµ¬í˜„
- [ ] **ì„¸ì…˜ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ ë©”ì„œë“œ êµ¬í˜„**
- [ ] **íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬ êµ¬í˜„**
- [ ] **ì²´í¬í¬ì¸íŠ¸ ë° ë³µêµ¬ ì‹œìŠ¤í…œ êµ¬í˜„**
- [ ] ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë©”ì„œë“œ êµ¬í˜„
- [ ] ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ê¸°ì¡´ ë©”ì„œë“œì™€ì˜ í˜¸í™˜ì„± í™•ì¸

### **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦**
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] **ì„¸ì…˜ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸**
- [ ] **íŠ¸ëœì­ì…˜ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸**
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
- [ ] ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
- [ ] **ì˜¤ë¥˜ ìƒí™© ë³µêµ¬ í…ŒìŠ¤íŠ¸**
- [ ] ì—ëŸ¬ ìƒí™© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸

### **ë°°í¬ ë° ëª¨ë‹ˆí„°ë§**
- [ ] ìŠ¤í…Œì´ì§• í™˜ê²½ í…ŒìŠ¤íŠ¸
- [ ] í”„ë¡œë•ì…˜ ë°°í¬
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] **ì„¸ì…˜ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì„¤ì •**
- [ ] ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘

## âš ï¸ ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘ ë°©ì•ˆ

### **1. ë°ì´í„° ë¬´ê²°ì„± ìœ„í—˜**
- **ìœ„í—˜**: ì²­í¬ ì²˜ë¦¬ ì¤‘ ì¼ë¶€ ì‹¤íŒ¨ ì‹œ ë°ì´í„° ì†ì‹¤
- **ëŒ€ì‘**: **íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²˜ë¦¬, ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ, ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì‹œë„**

### **2. ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ìœ„í—˜**
- **ìœ„í—˜**: ê¸´ ë£¨í”„ ë™ì•ˆ ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
- **ëŒ€ì‘**: **ì„¸ì…˜ ìƒíƒœ ëª¨ë‹ˆí„°ë§, ì£¼ê¸°ì  ì„¸ì…˜ ê°±ì‹ , ìë™ ì¬ì—°ê²°**

### **3. ì„±ëŠ¥ ì €í•˜ ìœ„í—˜**
- **ìœ„í—˜**: ì²­í¬ë³„ íŒŒì¼ I/Oë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜
- **ëŒ€ì‘**: ë°°ì¹˜ í¬ê¸° ìµœì í™”, ë²„í¼ë§, ë³‘ë ¬ ì²˜ë¦¬ ê²€í† 

### **4. í˜¸í™˜ì„± ìœ„í—˜**
- **ìœ„í—˜**: ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ë¬¸ì œ
- **ëŒ€ì‘**: ì ì§„ì  ì „í™˜, í”Œë˜ê·¸ ê¸°ë°˜ ë™ì‘, ì² ì €í•œ í…ŒìŠ¤íŠ¸

## ğŸ¯ ì„±ê³µ ì§€í‘œ

### **ì •ëŸ‰ì  ì§€í‘œ**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 500MB â†’ 50MB ì´í•˜ (90% ê°ì†Œ)
- PostgreSQL ì¶©ëŒ: 95% ê°ì†Œ
- ì²˜ë¦¬ ì•ˆì •ì„±: 99% ì´ìƒ ì„±ê³µë¥ 
- **ì„¸ì…˜ ì•ˆì •ì„±: 99% ì´ìƒ ì—°ê²° ìœ ì§€**
- **ë°ì´í„° ë¬´ê²°ì„±: 100% ë³´ì¥**

### **ì •ì„±ì  ì§€í‘œ**
- ì½”ë“œ ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
- **ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ ëŠ¥ë ¥ ëŒ€í­ í–¥ìƒ**
- **ì„¸ì…˜ ê´€ë¦¬ ì•ˆì •ì„± í–¥ìƒ**
- í™•ì¥ì„± ë° ì•ˆì •ì„± í–¥ìƒ

## ğŸ“ ê²°ë¡ 

ì´ ê³„íšì„œëŠ” `data_copy_engine.py`ë¥¼ **ì„¸ì…˜ ê´€ë¦¬ì™€ íŠ¸ëœì­ì…˜ ê¸°ë°˜ì˜ ì•ˆì „í•œ ì²­í¬ ë°©ì‹**ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ë¬¸ì œì™€ ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. 

**í•µì‹¬ì€ "ë©”ëª¨ë¦¬ì— ëˆ„ì í•˜ì§€ ì•Šê³ , ì„¸ì…˜ì„ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ë©°, íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ê³ , ì²´í¬í¬ì¸íŠ¸ë¡œ ë³µêµ¬ ê°€ëŠ¥í•˜ê²Œ" í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.**

**ì£¼ìš” ê°œì„ ì‚¬í•­:**
1. **ì„¸ì…˜ ê´€ë¦¬**: ì£¼ê¸°ì  ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ê°±ì‹ 
2. **íŠ¸ëœì­ì…˜ ê¸°ë°˜**: ê° ì²­í¬ë¥¼ ë…ë¦½ì ì¸ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì²˜ë¦¬
3. **ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ**: ì§„í–‰ ìƒí™© ì €ì¥ ë° ë³µêµ¬ ê¸°ëŠ¥
4. **ì„¸ë¶„í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬**: ì˜¤ë¥˜ ìœ í˜•ë³„ ì ì ˆí•œ ëŒ€ì‘ ë°©ì•ˆ

ë‹¨ê³„ë³„ êµ¬í˜„ì„ í†µí•´ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ ì „í™˜ì„ ì§„í–‰í•˜ë©°, ê° ë‹¨ê³„ë§ˆë‹¤ ì² ì €í•œ í…ŒìŠ¤íŠ¸ì™€ ê²€ì¦ì„ ê±°ì³ ì•ˆì •ì„±ì„ í™•ë³´í•  ê²ƒì…ë‹ˆë‹¤.

**ì˜ˆìƒ ì™„ë£Œ ê¸°ê°„: 4-7ì£¼**
**ì˜ˆìƒ íš¨ê³¼: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 90% ê°ì†Œ, ì•ˆì •ì„± ëŒ€í­ í–¥ìƒ, ë°ì´í„° ë¬´ê²°ì„± 100% ë³´ì¥** ğŸš€